<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Feature-Store on Sem Sinchenko</title>
    <link>http://localhost:1313/ssinchenko/tags/feature-store/</link>
    <description>Recent content in Feature-Store on Sem Sinchenko</description>
    <image>
      <title>Sem Sinchenko</title>
      <url>http://localhost:1313/ssinchenko/images/avatar-favicon.png</url>
      <link>http://localhost:1313/ssinchenko/images/avatar-favicon.png</link>
    </image>
    <generator>Hugo -- 0.127.0</generator>
    <language>en</language>
    <lastBuildDate>Sun, 14 Apr 2024 13:42:36 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/ssinchenko/tags/feature-store/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Effective asOfJoin in PySpark for Feature Store</title>
      <link>http://localhost:1313/ssinchenko/post/fs_asof_problem_pyspark/</link>
      <pubDate>Sun, 14 Apr 2024 13:42:36 +0200</pubDate>
      <guid>http://localhost:1313/ssinchenko/post/fs_asof_problem_pyspark/</guid>
      <description>Leveraging Time-Based Feature Stores for Efficient Data Science Workflows In our previous post, we briefly touched upon the concept of ML feature stores and their significance in streamlining machine learning workflows. Today, we&amp;rsquo;ll again explore a specific type of feature store known as a time-based feature store, which plays a crucial role in handling temporal data and enabling efficient feature retrieval for data science tasks. In this post we&amp;rsquo;ll how a feature-lookup problem may be effectively solved in PySpark using domain knowledge and understanding how Apache Spark works with partitions and columnar data formats.</description>
    </item>
    <item>
      <title>Computing ML Feature Store in PySpark</title>
      <link>http://localhost:1313/ssinchenko/post/effective_feature_store_pyspark/</link>
      <pubDate>Sun, 07 Apr 2024 16:01:25 +0200</pubDate>
      <guid>http://localhost:1313/ssinchenko/post/effective_feature_store_pyspark/</guid>
      <description>In this blog post, I will share my experience in building an ML Feature Store using PySpark. I will demonstrate how one can utilize case-when expressions to generate multiple aggregations with minimal data shuffling across the cluster. This approach is significantly more efficient than the naive method of using a combination of groupBy and pivot for generating aggregations (or features in ML terms).</description>
    </item>
  </channel>
</rss>
