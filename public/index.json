[{"content":" Introduction: data governance in DataLakes Unlike traditional OLTP or OLAP databases, the data lake and it\u0026#39;s successor, the lakehouse, is not actually a database. The concept of the data lake, as I understand it, is that we have\nA distributed file system with relatively low storage costs and strong guarantees of fault tolerance; Files in this distributed FS, which can be anything: text, logs, bits, tables, etc; A query engine or framework that allows us to run SQL queries against these files; A metastore (like Hive-compatible), which is really just an OLTP system that contains a mapping of table names and metadata to file locations. As a result, we have quite cheap cold storage that is very good for storing anything, and that can look like an OLAP system to the analysts working with it.\nState of the art I love Databricks for their open source focus. But sometimes their marketing can be very aggressive and confusing. For example, saying that Unitycatalog is the industry\u0026#39;s first unified governance solution sounds kind of weird when Apache Ranger has been with us for a decade.\nApache Ranger is a framework to enable, monitor and manage comprehensive data security across your data platform. It is an open-source authorization solution that provides access control and audit capabilities for big data platforms through centralized security administration.\nApache Ranger has the following goals:\nCentralized security administration to manage all security related tasks in a central UI or using REST APIs. Fine grained authorization to do a specific action and/or operation with Hadoop component/tool and managed through a central administration tool Standardize authorization method across all Hadoop components. Enhanced support for different authorization methods - Role based access control, attribute based access control etc. Centralize auditing of user access and administrative actions (security related) within all the components of Hadoop. Of course, Ranger is very old, very complex, very hard to use, and as a person who has worked with it, it is very painful to work with. But I would say that Unitycatalog from Databricks is more like a modern alternative to Ranger, but not the \u0026#34;industry first\u0026#34;.\nDiving into the code But let\u0026#39;s put aside Databricks\u0026#39; aggressive marketing. Matei Zaharia open-sourced Unitycatalog in a live session at the recent Data+Ai Summit, and it was cool! So let\u0026#39;s dive into the code and see what it can do for us compared to the state of the art.\nPersisten storage Let\u0026#39;s start with one of the most important parts: persistent storage. Apache Ranger supports several backend RDBMSs for storing policies and accesses (MySQL, PostgreSQL, etc.). During configuration, you\u0026#39;ll need to provide an existing database, and Ranger will create all its tables as part of the initialization process. Databricks Unitycatalog has both an advantage and a disadvantage. The upside is that Unity comes with an included (as a dependency) H2DB. The disadvantage is that Unity currently only supports H2DB. And this H2DB file is created in /etc/, so you cannot easily separate the policy server from the policy store. Migrating and backing up the DB is also difficult.\nserver/src/main/java/io/unitycatalog/server/persist/HibernateUtil.java:\nconfiguration.setProperty(\u0026#34;hibernate.connection.url\u0026#34;, \u0026#34;jdbc:h2:file:./etc/db/h2db;DB_CLOSE_DELAY=-1\u0026#34;); configuration.setProperty(\u0026#34;hibernate.hbm2ddl.auto\u0026#34;, \u0026#34;update\u0026#34;); LOGGER.debug(\u0026#34;Hibernate configuration set for production\u0026#34;); I hope the persistent part will be changed soon and the current implementation with H2 is just for demonstration purposes. Because without the ability to choose a real RDBMS to store policies in, the solution is far from being ready for production, more like a playground from my point of view.\nML/AI Governance The most promising and interesting part of Unitycatalog for me is the extension of governance to ML/AI objects. This is definitely a huge advantage over Apache Ranger, which does not offer anything like this. It is also a huge step forward because MLFlow, for example, has very limited authentication and permission management capabilities. There is no integration with something like Active Directory out of the box. And I was looking for Unitycatalog with the hope that it will finally be a unified solution that covers not only data in the data lake but also ML models/experiments. Unfortunately, the current OSS version does not offer anything related to ML…\nI hope that Databricks will expose additional APIs related to ML/AI governance in future releases. Without that, it is hard to call the Unitycatalog truly unified.\nFunctions Governance But the Functions API within the Unitycatalog looks like something really new and fresh. I don\u0026#39;t know of anything competitive in the existing big data governance frameworks.\nIt provides two main functionalities:\nRegister SQL function Register external function that can be in any language. server/src/main/java/io/unitycatalog/server/model/CreateFunction.java\npublic enum RoutineBodyEnum { SQL(\u0026#34;SQL\u0026#34;), EXTERNAL(\u0026#34;EXTERNAL\u0026#34;); } Such a function may also have dependencies. A dependency is either a table or another function:\npublic class Dependency { public static final String JSON_PROPERTY_TABLE = \u0026#34;table\u0026#34;; private TableDependency table; public static final String JSON_PROPERTY_FUNCTION = \u0026#34;function\u0026#34;; private FunctionDependency function; } The part of the documentation related to the Functions API is a little unclear, but from the code it looks like functions are bound to catalogs. But it is important that function names are unique, because names are keys, there is nothing like adding a hash or generating an internal ID. At least I have not found anything like that.\nAnyway, the functionality looks really cool! I can imagine a lot of uses for it. For example, you could register something like audit functions, or delta-vacuum functions, or some SQL routines. Finally, a nice tool to unify access management for table functions!\nA huge advantage of Unitycatalog over Apache Ranger!\nTables Governance Table management is a functionality offered by almost all of Unitycatalog\u0026#39;s competitors, including Apache Ranger. So let\u0026#39;s see what the Databricks solution brings to the table.\nBoth external and managed tables are supported, so it is cool!\nserver/src/main/java/io/unitycatalog/server/model/TableType.java\npublic enum TableType { MANAGED(\u0026#34;MANAGED\u0026#34;), EXTERNAL(\u0026#34;EXTERNAL\u0026#34;); } In the marketing promo, Databricks claims that Unitycatalog is a unified tool that can be used with more than just delta tables. But in fact it cannot. This is because you cannot create an external table and map it to existing data in Iceberg/Hudi formats in your datalake:\nserver/src/main/java/io/unitycatalog/server/model/DataSourceFormat.java\npublic enum DataSourceFormat { DELTA(\u0026#34;DELTA\u0026#34;), CSV(\u0026#34;CSV\u0026#34;), JSON(\u0026#34;JSON\u0026#34;), AVRO(\u0026#34;AVRO\u0026#34;), PARQUET(\u0026#34;PARQUET\u0026#34;), ORC(\u0026#34;ORC\u0026#34;), TEXT(\u0026#34;TEXT\u0026#34;); } So, at the moment only Delta is supported. Apache Iceberg or Apache Hudi are supported only by extending metadata to delta-compatible one. It may be achieve via Apache XTable (incubating) or via Delta UniForm, of course. But still disappointing a little. For the comparison, Apache Ranger works with a Hive Metastore where you can register any of Delta, Iceberg and Hudi.\nWhat really puzzles me is why Uniticatalog does not have a separate field for partitioning information in io.uniticatalog.server.model.TableInfo. Maybe for a DWH-like workload, hive-style partitioning is kind of outdated these days. But in my experience of a data engineer working in ML team, for batch ML-like/MLOps processing, old-school functional data engineering is still a perfect fit. I\u0026#39;m disappointed that Unitycatalog does not support information about partitioning. Especially considering that it was announced for ML/AI governance.\nI\u0026#39;m actually wondering now, does Unicatalog actually support hive-style partitioning?\nVolumes Governance Technically, Apache Ranger supports the management of unstructured data in the same way as tables. This can be achieved in Ranger by registering a policy on a path. Unitycatalog provides a more user friendly API in my opinion. This API is called \u0026#34;Volumes\u0026#34;. The support of both managed and external volumes is claimed:\nserver/src/main/java/io/unitycatalog/server/model/VolumeType.java\npublic enum VolumeType { MANAGED(\u0026#34;MANAGED\u0026#34;), EXTERNAL(\u0026#34;EXTERNAL\u0026#34;); } But at the moment you can actually creates only external one…\nserver/src/main/java/io/unitycatalog/server/persist/VolumeRepository.java\nif (VolumeType.MANAGED.equals(createVolumeRequest.getVolumeType())) { throw new BaseException(ErrorCode.INVALID_ARGUMENT, \u0026#34;Managed volume creation is not supported\u0026#34;); } Volumes are defined by path or storage_location. Also Unitycatalog generates unique IDs for each volume, so it looks like that names may be not unique:\nvolumeInfo.setVolumeId(UUID.randomUUID().toString()); What is missing at the moment? One of the most important part of any governance system for me is an audit capabilities. For example, Apache Ranger provides an audit APIs that allows you to check who accessed what. I did not find anything like this in the Unitycatalog at the moment. I hope such an API will be provided soon.\nConclusion At the time of this writing, Unitycatalog looks more like a proof of concept or MVP than a production-ready solution. There are no audit capabilities, no external RDBMS persistent storage support. All ML/AI governance features are currently missing. Big questions were raised about the lack of support for hive-style partitioning.\nNevertheless, it looks like a big step forward. I have been waiting for a modern alternative to Apache Ranger for a long time. And the mentioned extension of governance to AI/ML looks very promising.\nI\u0026#39;m looking forward to future releases of Unity! Thanks to Databricks for following the open source path!\n","permalink":"http://localhost:1313/ssinchenko/post/uniticatalog-first-look/","summary":"Databricks recently open-sourced Unitycatlog, a unified data catalog that aims to provide a single source of truth for data discovery, governance, and access control across multiple systems. In this blog post, we take a first look at Unitycatlog and dive into the source code to explain which features from the announcement are actually present. We explore how Unitycatlog addresses the challenges of managing data in a complex data landscape and discuss its potential impact on simplifying data governance and improving data accessibility for organizations.","title":"Unitycatalog: the first look"},{"content":"Leveraging Time-Based Feature Stores for Efficient Data Science Workflows In our previous post, we briefly touched upon the concept of ML feature stores and their significance in streamlining machine learning workflows. Today, we\u0026rsquo;ll again explore a specific type of feature store known as a time-based feature store, which plays a crucial role in handling temporal data and enabling efficient feature retrieval for data science tasks. In this post we\u0026rsquo;ll how a feature-lookup problem may be effectively solved in PySpark using domain knowledge and understanding how Apache Spark works with partitions and columnar data formats.\nPrecomputed features stored can significantly speed up the Data Science development process and simplify maintenance, lineage, and data quality tracking. By having a centralized repository of precomputed features, data scientists can easily access and reuse these features across multiple projects, eliminating the need to repeatedly compute them from scratch. This approach saves valuable time and computational resources, allowing data scientists to focus on model development and experimentation rather than feature engineering. Moreover, a feature store acts as a single source of truth for all features, ensuring consistency and reducing the risk of mismatch between different models or projects. It also simplifies lineage tracking, as the origin and transformations of each feature are well-documented and easily traceable. Additionally, data quality issues can be identified and addressed at the feature level, ensuring that all models consuming those features benefit from the improvements. By centralizing feature management, organizations can avoid the need to implement feature computation, lineage, and data quality tracking for each model separately, leading to a more efficient and maintainable Data Science workflow.\nUnderstanding Time-Based Feature Stores A time-based feature store is essentially a table that contains precomputed features for each customer at various timepoints over a recent period. This table is typically partitioned by a timecol column, which represents the date on which the features are up-to-date. Each partition within the table includes data in the form of a customer ID (custid) mapped to a set of features.\nFor example, consider a scenario where we have a feature store table with one thousand features computed for each customer ID and corresponding date (represented by the timecol partition column). The table structure would look something like this:\ntimecol (partition column) | custid | feature_1 | feature_2 | ... | feature_1000 Typical Use Case: Leveraging Feature Stores for Data Science Tasks A common use case for leveraging a feature store arises when a data scientist has data in the form of customer_id -\u0026gt; (observation_date, observation, *additional_columns). The observation_date represents the date wheni, for example, a marketing communication was sent to a customer, and the observation represents the customer\u0026rsquo;s reaction to the marketing communication, such as whether the customer accepted the offer within a week after the communication or not. In terms of ML Models observation is like y variable.\nThe task at hand is to create a predictive machine learning model that can estimate the probability of success of a marketing communication for a given customer. To accomplish this, we need to create a dataset in the form of customer_id -\u0026gt; (observation_date, observation, *additional_columns, features), where the features are retrieved from the feature store.\nIt is crucial to avoid leakage of features from the future and ensure that only the latest available features before the observation_date are used. This requirement leads to the \u0026ldquo;asOfJoin\u0026rdquo; problem, which involves joining the customer data with the feature store table based on the customer ID and the observation date.\nThe asOfJoin Problem The asOfJoin problem revolves around finding the most recent set of features for each customer that were computed before a given observation date. It requires joining the customer data with the feature store table while considering the temporal aspect of the data.\nConceptually, the asOfJoin operation can be described as follows:\nFor each customer ID in the customer data: Find the latest available features in the feature store table where the timecol is less than or equal to the observation date asOfJoin problem in DS workflow The asOfJoin problem is a common challenge in data science workflows, particularly when dealing with time-series data or scenarios where features need to be retrieved based on specific timepoints. In the upcoming sections, we\u0026rsquo;ll dive deeper into the asOfJoin problem, explore various approaches to solve it efficiently, and discuss how important for an engineer do not use generic algorithms blindly but put a domain knowledge into the solution code instead.\nData Generation and test setup To simulate the described above workflow and asOfJoin let\u0026rsquo;s create few feature tables of different size and an observation table.\nData Generation Code from uuid import uuid1 from datetime import datetime, timedelta from random import random, randint from pyspark.sql import SparkSession, functions as F, types as T, DataFrame, Row spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() def generate_fs_schema(n: int) -\u0026gt; T.StructType: fields = [T.StructField(\u0026#34;custid\u0026#34;, T.StringType())] for i in range(n): fields.append(T.StructField(f\u0026#34;feature_col_{i}\u0026#34;, T.DoubleType())) return T.StructType(fields=fields) OBSERVATIONS_SCHEMA = T.StructType( [ T.StructField(\u0026#34;custid\u0026#34;, T.StringType()), T.StructField(\u0026#34;timecol\u0026#34;, T.StringType()), T.StructField(\u0026#34;observation\u0026#34;, T.IntegerType()), T.StructField(\u0026#34;additionalColA\u0026#34;, T.StringType()), T.StructField(\u0026#34;additionalColB\u0026#34;, T.StringType()), T.StructField(\u0026#34;additionalColC\u0026#34;, T.StringType()), T.StructField(\u0026#34;additionalColD\u0026#34;, T.StringType()), T.StructField(\u0026#34;additionalColE\u0026#34;, T.StringType()), T.StructField(\u0026#34;additionalColF\u0026#34;, T.StringType()), T.StructField(\u0026#34;additionalColG\u0026#34;, T.StringType()), T.StructField(\u0026#34;additionalColH\u0026#34;, T.StringType()), T.StructField(\u0026#34;additionalColI\u0026#34;, T.StringType()), ] ) N = 100_000 CUSTOMER_IDS_ALL = [uuid1() for _ in range(N)] CUSTOMER_IDS_OBSERVATIONS = [uid for uid in CUSTOMER_IDS_ALL if random() \u0026lt;= .25] DATES_ALL = [d.to_pydatetime() for d in pd.date_range(start=\u0026#34;2022-01-01\u0026#34;, end=\u0026#34;2024-01-01\u0026#34;, freq=\u0026#34;ME\u0026#34;)] def generate_observations() -\u0026gt; DataFrame: rows = [] for cid in CUSTOMER_IDS_OBSERVATIONS: rows.append( Row( custid=cid, timecol=datetime.strftime(DATES_ALL[randint(1, len(DATES_ALL) - 1)] - timedelta(days=randint(0, 20)), \u0026#34;%Y-%m-%d\u0026#34;), observation=randint(0, 1), additionalColA=random(), additionalColB=random(), additionalColC=random(), additionalColD=random(), additionalColE=random(), additionalColF=random(), additionalColG=random(), additionalColH=random(), additionalColI=random(), ) ) return spark.createDataFrame(rows, schema=OBSERVATIONS_SCHEMA) def generate_fs_partition(schema: T.StructType, date_col: str) -\u0026gt; DataFrame: rows = [] for cid in CUSTOMER_IDS_OBSERVATIONS: res = {} for col in schema.fields: if col.name == \u0026#34;custid\u0026#34;: val = cid elif col.name == \u0026#34;observation\u0026#34;: val = randint(0, 1) else: val = random() res[col.name] = val rows.append(Row(**res)) return spark.createDataFrame(rows, schema) The code starts by importing necessary libraries and creating a SparkSession.\nThe generate_fs_schema function is defined next, which takes an integer n as input and generates a StructType schema for a feature store dataset; The schema includes a \u0026ldquo;custid\u0026rdquo; field of string type and n additional fields named \u0026ldquo;feature_col_i\u0026rdquo; of double type; The OBSERVATIONS_SCHEMA is defined as a StructType representing the schema for an observations dataset (or train dataset in terms of ML workflow). It includes fields such as \u0026ldquo;custid\u0026rdquo;, \u0026ldquo;timecol\u0026rdquo;, \u0026ldquo;observation\u0026rdquo;, and additional columns from \u0026ldquo;additionalColA\u0026rdquo; to \u0026ldquo;additionalColI\u0026rdquo; that represents some additional features or informatin, for example a channel of merketing communication; The code sets the value of N to 100,000, which represents the number of unique customer IDs to generate. The CUSTOMER_IDS_ALL list is created by generating N unique customer IDs using the uuid1 function. The CUSTOMER_IDS_OBSERVATIONS list is created by filtering the CUSTOMER_IDS_ALL list based on a random probability of 0.25; The DATES_ALL list is created by generating a sequence of dates from \u0026ldquo;2022-01-01\u0026rdquo; to \u0026ldquo;2024-01-01\u0026rdquo; with a monthly frequency using the pd.date_range function. For simplicity we will simulate the case when Feature Store Table has a monthly basis. In other words, we will have all the features for each customer once per month. The generate_observations function is defined, which generates a DataFrame representing the observations dataset. It iterates over the CUSTOMER_IDS_OBSERVATIONS list and creates a row for each customer ID with random values for the observation and additional columns. For avoiding a trivial case when observation dates match exactly to end of month we make also a random shift by 0-20 days back; The generate_fs_partition function is defined, which generates a DataFrame representing a partition of the feature store dataset. It takes a StructType schema and a date column name as input. It iterates over the CUSTOMER_IDS_OBSERVATIONS list and creates a row for each customer ID with random values for the feature columns. The following code generates for us three observation datasets and three Feature Store tables of different size:\nOBS = generate_observations() OBS.write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;data/OBSERVATIONS\u0026#34;) OBS.sample(0.5).write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;data/OBSERVATIONS_SMALL\u0026#34;) OBS.sample(0.25).write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;data/OBSERVATIONS_TINY\u0026#34;) SCHEMA_10 = generate_fs_schema(10) SCHEMA_50 = generate_fs_schema(50) SCHEMA_150 = generate_fs_schema(150) for date in DATES_ALL: date_str = datetime.strftime(date, \u0026#34;%Y-%m-%d\u0026#34;) generate_fs_partition(SCHEMA_10, date_str).write.mode(\u0026#34;overwrite\u0026#34;).parquet(f\u0026#34;data/FS_TABLE_10/timecol={date_str}\u0026#34;) generate_fs_partition(SCHEMA_50, date_str).write.mode(\u0026#34;overwrite\u0026#34;).parquet(f\u0026#34;data/FS_TABLE_50/timecol={date_str}\u0026#34;) generate_fs_partition(SCHEMA_150, date_str).write.mode(\u0026#34;overwrite\u0026#34;).parquet(f\u0026#34;data/FS_TABLE_150/timecol={date_str}\u0026#34;) Checking the generated data We have three observation datasets:\nTINY: 1.1 Mb; SMALL: 2.2 Mb; REGULAR: 4.2 Mb; And we have three feature store tables:\n10 features: 65 Mb; 50 features: 251 Mb; 150 features: 717 Mb; All three feature tables are partitioned by timecol:\n\u0026gt; dust --depth 1 FS_TABLE_150/ 0B ┌── _SUCCESS │ 0% 4.0K ├── ._SUCCESS.crc │ 0% 29M ├── timecol=2022-01-31 │ 4% 29M ├── timecol=2022-02-28 │ 4% 29M ├── timecol=2022-03-31 │ 4% 29M ├── timecol=2022-04-30 │ 4% 29M ├── timecol=2022-05-31 │ 4% 29M ├── timecol=2022-06-30 │ 4% 29M ├── timecol=2022-07-31 │ 4% 29M ├── timecol=2022-08-31 │ 4% 29M ├── timecol=2022-09-30 │ 4% 29M ├── timecol=2022-10-31 │ 4% 29M ├── timecol=2022-11-30 │ 4% 29M ├── timecol=2022-12-31 │ 4% 29M ├── timecol=2023-01-31 │ 4% 29M ├── timecol=2023-02-28 │ 4% 29M ├── timecol=2023-03-31 │ 4% 29M ├── timecol=2023-04-30 │ 4% 29M ├── timecol=2023-05-31 │ 4% 29M ├── timecol=2023-06-30 │ 4% 29M ├── timecol=2023-07-31 │ 4% 29M ├── timecol=2023-08-31 │ 4% 29M ├── timecol=2023-09-30 │ 4% 29M ├── timecol=2023-10-31 │ 4% 29M ├── timecol=2023-11-30 │ 4% 29M ├── timecol=2023-12-31 │ 4% 717M ┌─┴ FS_TABLE_150 Why partitioning is important? Apache Spark\u0026rsquo;s optimizer can leverage information about partitioning and min/max values from Parquet file headers to optimize query execution and reduce the amount of data that needs to be read.\nPartitioning:\nWhen data is partitioned based on certain columns, Spark can use this information to prune partitions that are not relevant to the query. If a query has filters on the partitioning columns, Spark can identify which partitions satisfy the filter conditions and skip reading the irrelevant partitions entirely. This partition pruning optimization can significantly reduce the amount of data scanned and improve query performance. Min/Max Values in Parquet File Headers:\nParquet files contain metadata in their headers, including the minimum and maximum values for each column within the file. Spark\u0026rsquo;s optimizer can utilize this information to determine if a file needs to be read based on the query\u0026rsquo;s filter conditions. If the filter condition falls outside the range of min/max values for a column in a Parquet file, Spark can skip reading that file altogether. By avoiding unnecessary file scans, Spark can optimize query execution and reduce the amount of I/O operations. Combining Partitioning and Min/Max Values:\nWhen data is partitioned and stored in Parquet format, Spark can leverage both partitioning information and min/max values for optimization. Spark can first prune irrelevant partitions based on the partitioning scheme and query filters. Within the remaining partitions, Spark can further utilize the min/max values from the Parquet file headers to determine which files need to be read. By combining these optimizations, Spark can significantly reduce the amount of data scanned and improve query performance. NOTE: By leveraging partition pruning, projection pushdowm and predicate pushdown it is possible to allow Spark/PySpark to work in a hard out-of-core mode, when the overall size of data on disks is much much bigger than the amount of available memory!\nBenchmarking preparation spark = ( SparkSession .builder .master(\u0026#34;local[*]\u0026#34;) .config(\u0026#34;spark.sql.autoBroadcastJoinThreshold\u0026#34;, \u0026#34;-1\u0026#34;) .getOrCreate() ) spark.sparkContext.setLogLevel(\u0026#34;ERROR\u0026#34;) observations = spark.read.parquet(\u0026#34;data/OBSERVATIONS/\u0026#34;) observations_tiny = spark.read.parquet(\u0026#34;data/OBSERVATIONS_TINY/\u0026#34;) observations_small = spark.read.parquet(\u0026#34;data/OBSERVATIONS_SMALL/\u0026#34;) features_10 = spark.read.parquet(\u0026#34;data/FS_TABLE_10/\u0026#34;) features_50 = spark.read.parquet(\u0026#34;data/FS_TABLE_50/\u0026#34;) features_150 = spark.read.parquet(\u0026#34;data/FS_TABLE_150/\u0026#34;) NOTE: We explicitly disabled broadcast joins here juyst because I\u0026rsquo;m using an old Dell laptop with 16G of memory and an old i3-8145U. In my case an observation data that my laptop can process is so small that it will be implicitly broadcasted in almost any asOgJoin implementation. But on real-world problems when observation data contains tipically 100k - 1M of rows with a lot of additional columns, so auto-broadcasting won\u0026rsquo;t help anyway.\nasOfJoin techniques There are few alvailable generic asOfJoin implementations in PySpark. We will focus mostly on two of them:\nMultiple Join and Aggregate Union-based Multiple Join and Aggragate This algorithm is implemnted directly in Apache Spark and can be used from PySpark by invoking pyspark.pandas.merge_asof. Let\u0026rsquo;s see how it works. There is a cool docstring that explains the idea in the Apache Spark Source Code:\n/** * Replaces logical [[AsOfJoin]] operator using a combination of Join and Aggregate operator. ... ... **/ object RewriteAsOfJoin extends Rule[LogicalPlan] It transform the following pseudo-query:\nSELECT * FROM left ASOF JOIN right ON (condition, as_of on(left.t, right.t), tolerance) to the following query:\nSELECT left.*, __right__.* FROM ( SELECT left.*, ( SELECT MIN_BY(STRUCT(right.*), left.t - right.t) AS __nearest_right__ FROM right WHERE condition AND left.t \u0026gt;= right.t AND right.t \u0026gt;= left.t - tolerance ) as __right__ FROM left ) WHERE __right__ IS NOT NULL Multiple Join and Aggregate Join on the Feature Lookup problem In the case of Features Lookup problem we can use Pandas on Spark (previously known as Koalas):\nfrom pyspark import pandas as koalas def asofjoin_koalas(obs: DataFrame, fs: DataFrame) -\u0026gt; DataFrame: obs = obs.withColumn(\u0026#34;timecol\u0026#34;, F.to_date(F.col(\u0026#34;timecol\u0026#34;), format=\u0026#34;yyyy-MM-dd\u0026#34;)) res = koalas.merge_asof(left=obs.to_pandas_on_spark(), right=fs.to_pandas_on_spark(), on=[\u0026#34;timecol\u0026#34;], by=\u0026#34;custid\u0026#34;) return res.spark.frame() Unfortunately, because of the full-read into memory of Feature Table the complexity of the overall task won\u0026rsquo;t depend of the size of observation tabele. Obviously any try to run that code will tend to OOM on a local setup and most probably to the endless disk spill on a real-worlds cluster:\n%%time asofjoin_koalas(observations_tiny, features_10).write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;tmp/temp_test\u0026#34;) java.lang.OutOfMemoryError: Java heap space It fails even on a tiny data, but it is actually because of the size of FS table.\nUnion based approach Union-based approach is based on the idea of union two data and apply a LAST(col, ignorenulls=true) OVER WINDOW PARTIION BY join-key ORDER BY time_fs WHERE time_fs \u0026lt;= time_obs. This approach is used, for example, in Databricks Labs DBL Tempo project.\nNOTE: Due to a hard license limitation of DBL Tempo project from Databricks Labs that is destributed under Databricks commerical license I cannot use it my benchmark.\nThe problem of Union approach Unfortunately we will face the same probelm with union like with koalas: Union not only required a full table read but also required a full table shuffle. It will always tend to a shaffle of all the features from the disk to the memory and to huge disk spill.\nUsing a domain knowledge As one may see, generic approaches to asOfJoin problem looks like non working in a case of Features Lookup. What can we do here? We can do what each engineer should do: use domain knowledge. Let\u0026rsquo;s see what can we use:\nFeatures Table is much bigger that observations Features Table is optimized for direct join by timecol (partition prunning and pre-partitioning) and custid (min-max in header of parquet files) Proposed algorithm Let\u0026rsquo;s try the following algorithm:\nTake only keys (id -\u0026gt; time) from observations Take only keys (id -\u0026gt; time) from fs table Use left join by id: We know that observation is a small table We are taking only tow columns We are able to make explicit broadcast due to these facts For each id from observation get the latest time from FS by groupBy + max Take the table from p.4, that has a dimension N_observation x 2 (small table, two columns) Join that table to FeatureStore table: Left join: one key is partition, another key is unique in partition (we can leverage pushdown/prunning) Right table is small and has only two columns We can use explicit broadcast of the right table here Join resulted table with observations: simple inner join, no duplicates/nulls def asofjoin_manual(obs: DataFrame, fs: DataFrame) -\u0026gt; DataFrame: only_ids_left = obs.select(\u0026#34;custid\u0026#34;, \u0026#34;timecol\u0026#34;) only_ids_right = fs.select(F.col(\u0026#34;custid\u0026#34;), F.col(\u0026#34;timecol\u0026#34;).alias(\u0026#34;timecol_right\u0026#34;)) cond = (only_ids_left.custid == only_ids_right.custid) \u0026amp; (only_ids_left.timecol \u0026lt;= only_ids_right.timecol_right) cross = ( only_ids_right .join( F.broadcast(only_ids_left), on=cond, how=\u0026#34;left\u0026#34; ) .drop(only_ids_right.custid) ) final_ids_fs = ( cross .groupBy(F.col(\u0026#34;custid\u0026#34;), F.col(\u0026#34;timecol\u0026#34;).alias(\u0026#34;timecol_obs\u0026#34;)) .agg(F.max(\u0026#34;timecol_right\u0026#34;).alias(\u0026#34;timecol\u0026#34;)) ) selected_features = ( fs.join( F.broadcast(final_ids_fs), on=[\u0026#34;custid\u0026#34;, \u0026#34;timecol\u0026#34;], how=\u0026#34;left\u0026#34; ) .withColumn(\u0026#34;timecol_fs\u0026#34;, F.col(\u0026#34;timecol\u0026#34;)) .withColumn(\u0026#34;timecol\u0026#34;, F.col(\u0026#34;timecol_obs\u0026#34;)) ) return selected_features.join(observations, on=[\u0026#34;custid\u0026#34;, \u0026#34;timecol\u0026#34;], how=\u0026#34;inner\u0026#34;) Tests SMALL FEATURE STORE\n%%time asofjoin_manual(observations, features_10).write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;tmp/temp_test\u0026#34;) Result: success, 3.5 sec\nMEDIUM FEATURE STORE\n%%time asofjoin_manual(observations, features_50).write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;tmp/temp_test\u0026#34;) Result: sucess, 4.64 sec\nBIG FEATURE STORE\n%%time asofjoin_manual(observations, features_150).write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;tmp/temp_test\u0026#34;) Result: sucess, 8.95 sec\nAnalysis As one may see, our appraoch is working with any size of Featore Store because in our algorithm the complexity depends mostly of the size of observations that is a relative small piece of data. And also we can use all the benefits of partitioning structure and uniqeness of the Id column within partitions. Additional sorting the data before writeing (for example, in DeltaLake it may be achieved by Z ORDER) will provide additional benefits!\nConclusion Domain knowledge is crucial for Data Engineers when writing logic because it enables them to make informed decisions and optimize their algorithms for specific tasks. By understanding the size of tables, partitioning schemes, and other domain-specific information, Data Engineers can tailor their algorithms to be more effective and efficient for the given use case. Generic data algorithms, while designed to be applicable in a wide range of scenarios, often sacrifice effectiveness in favor of generality. This is because there is no \u0026ldquo;free lunch\u0026rdquo; in algorithm design, meaning that an algorithm that performs well on all possible inputs is unlikely to exist. Instead, by leveraging their domain knowledge, Data Engineers can create custom algorithms that are specifically designed to handle the unique characteristics and constraints of their data. This approach leads to improved performance, scalability, and resource utilization, ultimately resulting in more effective and efficient data processing pipelines.\n","permalink":"http://localhost:1313/ssinchenko/post/fs_asof_problem_pyspark/","summary":"Leveraging Time-Based Feature Stores for Efficient Data Science Workflows In our previous post, we briefly touched upon the concept of ML feature stores and their significance in streamlining machine learning workflows. Today, we\u0026rsquo;ll again explore a specific type of feature store known as a time-based feature store, which plays a crucial role in handling temporal data and enabling efficient feature retrieval for data science tasks. In this post we\u0026rsquo;ll how a feature-lookup problem may be effectively solved in PySpark using domain knowledge and understanding how Apache Spark works with partitions and columnar data formats.","title":"Effective asOfJoin in PySpark for Feature Store"},{"content":"Introduction In the rapidly evolving world of machine learning (ML), data scientists and ML engineers face a common challenge: efficiently managing and reusing features across multiple models. Feature engineering, a crucial step in the ML development process, can be time-consuming and repetitive, leading to delays in model deployment and reduced productivity. This is where the concept of an ML Feature Store comes into play.\nAn ML Feature Store is a centralized repository that allows teams to store, manage, and access features used in ML models. It acts as a single source of truth for feature data, enabling different ML models to reuse the same features without the need for :wredundant feature engineering efforts. By providing a unified interface for storing and retrieving features, a Feature Store streamlines the ML development process and significantly reduces the time-to-market for new models.\nOne of the key benefits of a Feature Store is its ability to promote feature reuse across multiple models. Instead of each model having its own siloed feature set, a Feature Store allows features to be shared and reused by different models. This not only saves time and effort in feature engineering but also ensures consistency and maintainability across the ML ecosystem.\nML Feature Store top level architecture Feature Store based on customers behaviour In the realm of customer interaction-based features, these features are typically aggregates of various interactions, such as the number of logins, clicks, or average spending from a credit card, calculated over different time periods and grouped by specific categories.\nFor instance, a Feature Store can be used to create features like the amount of logins in the mobile category for the last three months or the average spending in the grocery category from a credit card with the type \u0026ldquo;Credit Card.\u0026rdquo; These features provide valuable insights into customer behavior and preferences, which can be leveraged by machine learning models to make informed predictions and drive personalized experiences.\nIn the following sections, we will delve deeper into the specific use case of creating features based on customer interactions using PySpark.\nTechnical introduction One might say that creating such a feature store requires working with window functions because of the time component, but time intervals can easily be encoded as group columns, so the task is just to calculate different aggregates (sum, count, average, etc.) over different group combinations. For example, if we have a timeline column, say login_date, and we need to calculate aggregates for the last week, month, and six months, it is easy to create additional columns with flags to avoid window functions:\nfrom pyspark.sql import functions as F data_with_flags = ( data .withColumn( \u0026#34;last_week_flag\u0026#34;, (F.datediff(F.current_date(), F.col(\u0026#34;login_date\u0026#34;)) \u0026lt;= F.lit(7)) ) .withColumn( \u0026#34;last_month_flag\u0026#34;, (F.datediff(F.current_date(), F.col(\u0026#34;login_date\u0026#34;)) \u0026lt;= F.lit(30.5)) ) .withColumn( \u0026#34;last_six_month_flag\u0026#34;, (F.datediff(F.current_date(), F.col(\u0026#34;login_date\u0026#34;)) \u0026lt;= F.lit(30.5 * 6)) ) ) So, the task of calculation of a single batch of the Feture Store table can be transformed into the generic task of aggregations over multiple groups. Let\u0026rsquo;s use H2O benchmark dataset as an example:\nfrom pyspark.sql import SparkSession spark = ( SparkSession.builder .master(\u0026#34;local[*]\u0026#34;) .getOrCreate() ) schema = T.StructType.fromJson( {\u0026#39;type\u0026#39;: \u0026#39;struct\u0026#39;, \u0026#39;fields\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;id1\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}, {\u0026#39;name\u0026#39;: \u0026#39;id2\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}, {\u0026#39;name\u0026#39;: \u0026#39;id3\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}, {\u0026#39;name\u0026#39;: \u0026#39;id4\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}, {\u0026#39;name\u0026#39;: \u0026#39;id5\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}, {\u0026#39;name\u0026#39;: \u0026#39;id6\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}, {\u0026#39;name\u0026#39;: \u0026#39;v1\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}, {\u0026#39;name\u0026#39;: \u0026#39;v2\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}, {\u0026#39;name\u0026#39;: \u0026#39;v3\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;double\u0026#39;, \u0026#39;nullable\u0026#39;: True, \u0026#39;metadata\u0026#39;: {}}]} ) data = ( spark.read .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) .schema(schema) .csv(\u0026#34;data/G1_1e6_1e6_10_0.csv\u0026#34;) ) data.show() Result:\n+-----+-----+------------+---+---+-----+---+---+--------+ | id1| id2| id3|id4|id5| id6| v1| v2| v3| +-----+-----+------------+---+---+-----+---+---+--------+ |id008|id006|id0000098213| 6| 1| 9802| 1| 9|34.51913| |id007|id003|id0000022646| 8| 1|20228| 1| 4|64.95154| |id008|id004|id0000083470| 1| 6| 2125| 1| 8|43.23634| |id009|id006|id0000051855| 5| 8|41429| 1| 12|27.36826| |id005|id007|id0000074629| 10| 1|71480| 2| 15|65.04233| |id001|id009|id0000045958| 6| 2|30060| 5| 8|16.57359| |id009|id008|id0000060869| 10| 10|95489| 4| 8|60.78273| |id010|id010|id0000015471| 3| 2|53762| 5| 11|40.72817| |id008|id009|id0000032117| 10| 7|37774| 4| 2| 7.48368| |id006|id001|id0000064092| 4| 5|64203| 1| 15|79.79128| |id001|id001|id0000041819| 3| 3|91110| 2| 11|34.33383| |id005|id004|id0000097955| 3| 6|95321| 5| 7|32.20545| |id009|id005|id0000004865| 6| 10|54982| 3| 8| 3.07528| |id008|id009|id0000060610| 8| 10|31843| 1| 8|37.05268| |id009|id009|id0000008902| 8| 9| 9394| 4| 13|23.04208| |id006|id004|id0000044586| 6| 6| 5279| 2| 6|49.30788| |id007|id007|id0000015887| 10| 1| 2987| 2| 1|66.90033| |id007|id003|id0000039177| 2| 3|85798| 4| 2|31.13281| |id002|id004|id0000066644| 9| 1|57709| 1| 12|53.35556| |id006|id003|id0000064335| 7| 5|38365| 2| 7|59.54201| +-----+-----+------------+---+---+-----+---+---+--------+ The entire data set has 1,000,000 rows. Here we have the column id3 which is a string and has ~100,000 unique values. This column represents a unique key, for example a customer id. The columns id1, id2, id4, id5 have 10 unique but independent values and will represent our grouping keys, like time interval flag and different categories.\nTo reproduce my experiment, you can use the farsante library, which contains effective H2O dataset generators written in Rust. See the documentation for details. I used the rust cli api of farsante and the following commands to genrate this dataset:\ncargo build --release cd target/release ./farsante --n 1000000 --k 10 GroupBy + Pivot approach Our goal is to take our initial data with 9 columns and 1,000,000 rows and create a feature table with 121 columns and 100,000 rows. In this case, 121 columns contains\nid column (\u0026quot;id3\u0026quot;) count(\u0026quot;*\u0026quot;), sum(\u0026quot;v2\u0026quot;), avg(\u0026quot;v3\u0026quot;) for all unique values of column id1 (3 * 10 = 30 columns) count(\u0026quot;*\u0026quot;), sum(\u0026quot;v2\u0026quot;), avg(\u0026quot;v3\u0026quot;) for all unique values of col id2 (3 * 10 = 30 cols) count(\u0026quot;*\u0026quot;), sum(\u0026quot;v2\u0026quot;), avg(\u0026quot;v3\u0026quot;) for all unique values of col id4 (3 * 10 = 30 cols) count(\u0026quot;*\u0026quot;), sum(\u0026quot;v2\u0026quot;), avg(\u0026quot;v3\u0026quot;) for all unique values of col id5 (3 * 10 = 30 cols) 120 + 1 columns in total. We will only touch on the simplest case, without touching on the topic of combinations of group values. But as you will see, our code is easily extendable to this case. We need to create a structure with columns that contain values of group keys in their names. The most obvious way to do this is to simply use groupBy + pivot in PySpark. But since we are talking about a production-like pipeline, the output schema of our table should not depend on the input data, so we need to fix the values of the group keys before computing anything. In our example, we can infer these values, but in production, of course, it is strongly recommended to fix these values at the level of pipeline configuration files. Otherwise, you could easily shoot yourself in the foot one day when a new group key comes along and breaks your table schema.\ngroups = { \u0026#34;id1\u0026#34;: [d[0] for d in data.select(F.col(\u0026#34;id1\u0026#34;)).distinct().collect()], \u0026#34;id2\u0026#34;: [d[0] for d in data.select(F.col(\u0026#34;id2\u0026#34;)).distinct().collect()], \u0026#34;id4\u0026#34;: [d[0] for d in data.select(F.col(\u0026#34;id4\u0026#34;)).distinct().collect()], \u0026#34;id5\u0026#34;: [d[0] for d in data.select(F.col(\u0026#34;id5\u0026#34;)).distinct().collect()], } PRIMARY_KEY = \u0026#34;id3\u0026#34; By using this structure it is easy to write the naive groupBy-pivot version:\nfrom functools import reduce def naive_fs(data, groups): pre_result = [] for grp_col in groups.keys(): pre_result.append( data .groupBy(PRIMARY_KEY) .pivot(grp_col, groups[grp_col]) .agg( F.count(\u0026#34;v1\u0026#34;).alias(f\u0026#34;_valof_{grp_col}_count_v1\u0026#34;), F.sum(\u0026#34;v2\u0026#34;).alias(f\u0026#34;valof_{grp_col}_sum_v2\u0026#34;), F.mean(\u0026#34;v3\u0026#34;).alias(f\u0026#34;valof_{grp_col}_avg_v3\u0026#34;), ) ) return reduce(lambda a, b: a.join(b, on=[PRIMARY_KEY], how=\u0026#34;left\u0026#34;), pre_result) Let\u0026rsquo;s see how fast it produce our tiny dataset with one million of rows:\n%%time naive_fs(data, groups).write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;tmp/naive_output\u0026#34;) Results (each run means a full restart of SparkSession to avoid getting confusing results due to disk hashing or AQE optimizations):\nRun N1: CPU times: user 44.5 ms, sys: 9.43 ms, total: 53.9 ms Wall time: 21.8 s Run N2: CPU times: user 41.1 ms, sys: 11.3 ms, total: 52.4 ms Wall time: 20.2 s Run N3: CPU times: user 49.3 ms, sys: 7.53 ms, total: 56.8 ms Wall time: 21.8 s Spark Plan analysis Opening a Spark plan shows me that in this case Spark actually does three sort-merge joins. Trying the same code but with 10 million rows dataset will get stuck and fail with Java heap space (on distributed cluster it will transform into huge data spill and most likely fail due to disk space or even it may brake the whole cluster if it is not protected from disk overflow).\nCase-when approach Another approach is to use the CASE-WHEN approach. For example, to compute the count(*) over some group, we can avoid using groupBy at all, just because the count of rows related to some value val of a group key is just a sum over a CASE-WHEN expression like this: F.sum(F.when(F.col(\u0026quot;grp_key\u0026quot;) == F.lit(val), F.lit(1)).otherwise(F.lit(0))). In this case, we use case-when to return 1 for all rows related to the group and zero otherwise. The sum over such a result is obviously equal to the number of rows related to the value of the group. To calculate sum we can use the value of the sum column for rows related to the group and zero otherwise. To calculate the average, we need to replace unrelated rows with null, because the built-in Spark averaging function ignores null. You can check the documentation to understand how to calculate other types of aggregates.\nLet\u0026rsquo;s write the code, that generate our aggragations for H2O dataset:\ndef case_when_fs(data, groups): cols_list = [] for grp_col, values in groups.items(): for val in values: cond = F.col(grp_col) == F.lit(val) cols_list.append( F.sum(F.when(cond, F.lit(1)).otherwise(F.lit(0))).alias(f\u0026#34;{val}_valof_{grp_col}_count_v1\u0026#34;) ) cols_list.append( F.sum(F.when(cond, F.col(\u0026#34;v2\u0026#34;)).otherwise(F.lit(0))).alias(f\u0026#34;{val}_valof_{grp_col}_sum_v2\u0026#34;) ) cols_list.append( F.mean(F.when(cond, F.col(\u0026#34;v2\u0026#34;)).otherwise(F.lit(None))).alias(f\u0026#34;{val}_valof_{grp_col}_avg_v3\u0026#34;) ) return data.groupby(PRIMARY_KEY).agg(*cols_list) We will run the same test with write for that approach:\n%%time case_when_fs(data, groups).write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;tmp/case_when_output\u0026#34;) Results:\nRun N1: CPU times: user 157 ms, sys: 40.5 ms, total: 198 ms Wall time: 13.9 s Run N2: CPU times: user 176 ms, sys: 35.6 ms, total: 211 ms Wall time: 14.3 s Run N3: CPU times: user 192 ms, sys: 41.3 ms, total: 233 ms Wall time: 16.9 s Spark Plan analysis In this case, there are no sort-merge-join operations in the plan. The calculation is almost x1.5 faster. Also, this code will work in the case of 10 million rows without errors (out-of-core case) without disk spill and Java heap space errors. Also, this approach give you more prciese control over the variable names, groups combinations, etc.\nConclusion The described above case is quite specific, but still very offen. And it is a nice example, how engineers can use domain knowledge about keys distribution, required output, etc. to write less generic but more effective code!\n","permalink":"http://localhost:1313/ssinchenko/post/effective_feature_store_pyspark/","summary":"In this blog post, I will share my experience in building an ML Feature Store using PySpark. I will demonstrate how one can utilize case-when expressions to generate multiple aggregations with minimal data shuffling across the cluster. This approach is significantly more efficient than the naive method of using a combination of groupBy and pivot for generating aggregations (or features in ML terms).","title":"Computing ML Feature Store in PySpark"},{"content":"Preface I would like to thank Martin Grund. He gave me a lot of useful advice during my work on this project!\nIntroduction Recently, I published a post about potential compatibility issues between SparkConnect and 3d-party libraries that depend on py4j. After that post, people from Databricks contacted me and suggested we work together on this issue. With their help, I tried to create a step-by-step guide for newbies on how to extend SparkConnect and how to potentially migrate from py4j logic to writing protocol plugins.\nWhat is going on? As you may know, Apache Spark 4.0 is coming. And it looks like the SparkConnect protocol will be the main way of communication between the driver and the user\u0026rsquo;s code. So we need to be ready for it. But there are still a lot of 3d party libraries based on Java/Scala core and PySpark bindings via py4j. There is also a gap in the Apache Spark documentation on how to extend the protocol, what are the best practices, and how to migrate your project to a new way of working.\nWhat is Spark Connect For anyone new to the topic, SparkConnect is a new modern way to make Apache Spark a little more \u0026ldquo;modular\u0026rdquo;. Instead of creating a Driver directly in the user\u0026rsquo;s code, we create a Driver in a remote SparkConnect server. All communication between the client and such a server is done via the gRPC protocol. This opens a lot of interesting possibilities, such as\nEasy creation of new language APIs, for example the recently introduced Go Spark Client; Better user isolation when working on shared clusters (like Databricks Shared Clusters); Reduced dependency hell because users do not have to check all Spark internal dependencies and only need a thin client; Spark Connect Architecture For a top-level overview, you can check out an Overview in Spark Documentation. For a deeper dive into the topic I recommend this nice blog post by Bartosz Konieczny or the video of Martin Grund\u0026rsquo;s presentation:\nHow the post is organized The first section will briefly describe how to get and configure everything you need to test a SparkConnect application. Like getting Spark itself, building it from source, fixing some bugs, etc. We will also touch on all the prerequisites needed to build an application: Java, Maven, Python, protobuf, buf, etc; After that, we will briefly touch on the basic structures provided by Spark for extending the protocol: CommandPlugin, RelationPlugin, ExtensionPlugin; Next, we will define the JVM library that we want to wrap in the SparkConnect extension; After that, we will write all the necessary protocol extensions; The next step is to create a Python client; Finally, the end2end testing of the application; At the end I will try to say a few words about what is good in a new SparkConnect and what is missing from my point of view. Setup In this project I will use Java instead of Scala. The reason is quite simple: in my understanding, any Scala dev can read Java code. But it does not always work well in the other direction just because of Scala magic and implicity. Of course, the same code will look more elegant in Scala, but since this is a blog post, readability should be the first priority. I will also use Maven as a build system, simply because Apache Spark itself uses this build tool.\nBefore we start, you will need the following\nJava 17: Archive or install it through your system package manager or use SDKMan; Python 3.10: Archive or install it via the system package manager or use PyEnv; Maven: Download page or install it via the system package manager; Protobuf: Release Page, but it is better to install it via the system package manager; Buf: GitHub repo. Build the latest Spark The main reason to work directly with the latest Spark snapshot is that while the Spark Connect general implementation is readily available since Spark 3.4 for Python and Spark 3.5 for Scala, there is still a lot of work in progress to reduce sharp edges and improve documentation.\nWe need to work with a latest available spark-4.0.0-SNAPSHOT version. At first you need to clone it locally via git clone git@github.com:apache/spark.git. After that just go inside and call mvn ./build/mvn -Pconnect clean package. The next step is to generate corresponding PySpark library: cd python; python3.10 setup.py sdist. It will generate something like pyspark-4.0.0.dev0.tar.gz in dist folder. It is a ready to use distribution that may be installed into user\u0026rsquo;s environment.\nA JVM library example Our goal will be to try to wrap some existing Spark-Java logic that manipulates by instances of DataFrame class and also by own classes. Of course, at first, we need to define such a library first!\nCommand Logic The simplest of all the possible cases is just a command. In the world of Spark Connect, a command is a simple way to invoke server-side logic that might incur a side-effect and does not necessarily return data. On the JVM-side it should be represented by public void command(A argA, B argB, ...). For example, it may be the case when we need to initialize library classes by something like public void init(). Another case is when we want to write something to the FileSystem. Let\u0026rsquo;s try to mimic such a case:\npublic class CommandLikeLogic { public static void commandA(Long paramA, Long paramB, String paramC) throws IOException { var spark = SparkSession.active(); var path = new Path(paramC); var fs = path.getFileSystem(spark.hadoopConf()); var outputStream = fs.create(path); outputStream.writeUTF(String.format(\u0026#34;FieldA: %d\\nFieldB: %d\u0026#34;, paramA, paramB)); outputStream.flush(); outputStream.close(); } } It is a very dummy class with just a single method commandA(Long paramA, Long paramB, String paramC). It do the following:\nGet a current active SparkSession object; Get a right implementation of org.apache.hadoop.fs.FileSystem based on the configuration from the SparkSession; Write values of paramA and paramB into a file in path defined by paramC As one may see there is no magic. But working with underlying org.apache.hadoop.fs.FileSystem is not possible from PySpark and it is very common case to write such a logic in JVM-languages!\nDataFrame Logic Another case is when JVM-object should create and return DataFrame (Dataset\u0026lt;Row\u0026gt;) object. It is the most common case for any kind of 3d-party library. From my experience developers typically work-around such a logic via py4j:\nDataFrame in PySpark has a private attribute _jdf that represents py4j.java_gateway.JavaObject corresponds to Dataset\u0026lt;Row\u0026gt; in JVM; DataFrame in PySaprk has a constructor that takes py4j.java_gateway.JavaObject and SparkSession and returns pyspark.sql.DataFrame; With SparkConnect this workaround doesn\u0026rsquo;t work anymore, so let\u0026rsquo;s see how we may do it in a new and right way. Let\u0026rsquo;s try to wrap the following simple class:\npublic class DataFrameLogic { public static Dataset\u0026lt;Row\u0026gt; createDummyDataFrame() { var schema = new StructType( new StructField[] { DataTypes.createStructField(\u0026#34;col1\u0026#34;, DataTypes.LongType, true), DataTypes.createStructField(\u0026#34;col2\u0026#34;, DataTypes.StringType, true), DataTypes.createStructField(\u0026#34;col3\u0026#34;, DataTypes.BooleanType, true) }); var spark = SparkSession.active(); var rows = new ArrayList\u0026lt;Row\u0026gt;(); var gen = new Random(); for (int i = 0; i \u0026lt;= 10; i++) { rows.add( RowFactory.create( gen.nextLong(), String.format(\u0026#34;%d-%d\u0026#34;, gen.nextInt(), gen.nextLong()), gen.nextBoolean())); } return spark.createDataFrame(rows, schema); } } As one may see it is a really dummy object. And, of course, you can do exactly the same in pure PySpark (doesn\u0026rsquo;t matter, with SparkConnect or not). But let\u0026rsquo;s still try to wrap it just to understand how it works. In this case this simple class should be enough for our purposes. In the world of Spark Connect, this DataFrame logic is represented by Relation messages. These messages are declarative transformations that can be arbitrarily nested and are very similar to the standard Spark SQL operations like project, filter, groupBy.\nManipulation of JVM objects The most complex and tricky thing is when we need not only call void commands or create/modify DataFrame objects, but create instances of regular JVM classes and calls methods of them. To mimic such a case let\u0026rsquo;s again create a dummy Java class with couple of getter/setter methods, constructor and, for example, custom toString() implementation:\npublic class ObjectManipulationLogic { private String strParameter; private Long longParameter; public ObjectManipulationLogic(String strParameter, Long longParameter) { this.strParameter = strParameter; this.longParameter = longParameter; } public String getStrParameter() { return strParameter; } public void setStrParameter(String strParameter) { this.strParameter = strParameter; } public Long getLongParameter() { return longParameter; } public void setLongParameter(Long longParameter) { this.longParameter = longParameter; } @Override public String toString() { return \u0026#34;ObjectManipulationLogic{\u0026#34; + \u0026#34;strParameter=\u0026#39;\u0026#34; + strParameter + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, longParameter=\u0026#34; + longParameter + \u0026#39;}\u0026#39;; } } Our class has a public constructor just for simplicity. In reality, of course, it could be more complex with factory methods. But it should be enough for our demo case.\nOur class has two fields, corresponding getters/setters, and a custom `toString\u0026rsquo;. In reality, of course, no one will wrap complex JVM logic in SparkConnect. But as you will see in the next sections, there is no fundamental difference in which method is called. So such a class is complex enough for our demo purposes.\nDefining protobuf messages At first, to start any kind of SparkConnect extending we need to define contracts in the form of protobuf messages that are interpreted by Spark. We decided to use the following three use-cases:\nCalling a command; Creating a DataFrame; Manipulating JVM object. syntax = \u0026#39;proto3\u0026#39;; option java_multiple_files = true; option java_package = \u0026#34;com.ssinchenko.example.proto\u0026#34;; message CallCommandLikeLogic { int64 paramA = 1; int64 paramB = 2; string paramC = 3; } message CallDataFrameLogic {} message CallObjectManipulationLogic { int32 objectId = 1; bool newObject = 2; bool deleteObject = 3; string methodName = 4; repeated string args = 5; } Let\u0026rsquo;s go step by step.\nThe first rows tels us to use proto3 (protobuf version 3) syntax, which package is it, which java package is it, etc. There is no special magic, so, just use it as an example for your Spark proto code.\nmessage CallCommandLikeLogic { int64 paramA = 1; int64 paramB = 2; string paramC = 3; } This message will be used to call a CommandLikeLogic Java class. As you remember, the only static method commandA takes exactly three arguments (Long paramA, Long paramB, String paramC). In protobuf syntax it transforms into int64 paramA, int64 paramB and string paramC. Numbers in proto-syntax means the id of argument in the message, you can read more about it in the protobuf documentation.\nThe message for CallDataFrameLogic is even simpler: it is just an empty one, but we still need it for checking the appropriate command.\nThe last message is quite tricky. To manipulate objects in JVM via gRPC we need at least the following:\nAn ID of the object just to know which one should we touch in JVM; A flag that we need to create a new object instead of manipulating existing one; A flag that we need to delete an existing object; A name of the method we want to call (in the case when we do not want to delete or create an object); A list of strings-represented arguments; And it gain us the structure of the CallObjectManipulationLogic message:\nmessage CallObjectManipulationLogic { int32 objectId = 1; bool newObject = 2; bool deleteObject = 3; string methodName = 4; repeated string args = 5; } Because in proto3 syntax any argument is optional we do not need to mark fields as optional or required. So, all the checking logic is moving to runtime.\nGenerating Classes and Methods from proto-code Now we need to generate corresponding Java classes and methods based on the proto3 code. Of course you can use something like protoc -I=... -java_out=... .... But it is not the simplest way. To make it simpler there is a very cool tool named buf (link to the buf repo in GitHub). I already mentioned this tool in previous sections. With buf generation for all the languages may be done via one command: buf generate. But at first we need to define buf.work.yaml and buf.gen.yaml that should contain the information about which plugins we want to use.\nbuf.work.yaml:\nversion: v1 directories: - protobuf buf.gen.yaml:\nversion: v1 plugins: - plugin: buf.build/protocolbuffers/python:v25.3 out: python/spark_connect_example/proto - plugin: buf.build/grpc/python:v1.62.0 out: python/spark_connect_example/proto - plugin: buf.build/community/nipunn1313-mypy:v3.5.0 out: python/spark_connect_example/proto - plugin: buf.build/protocolbuffers/java:v25.3 out: java NOTE: You may find a list of available buf plugins here.\nThe first one defines the protobuf files directory, the second one defines what should be generated and also where this files should be places. Let\u0026rsquo;s again see on the structure of the project to better understand what\u0026rsquo;s going on:\n|- src |-- main |--- java |---- com/ssinchenko/example |----- lib |----- proto # \u0026lt;- generated from proto Java classes will be here |----- server |--- protobuf # \u0026lt;- proto3 code is here |--- python |---- spark_connect_example |----- proto # \u0026lt;- generated from proto Python classes will be here |---- pyproject.toml |--- buf.gen.yaml |--- buf.work.yaml |- pom.xml So, long story short, all that you need from now to generate (or regenerate) all the classes/py-files is to call buf generate from the src/main directory.\nCommandPlugin As I mention already, the case of void command is the simplest one. Let\u0026rsquo;s see how implementation of the CommandPlugin will look for the case of our CommandLikeLogic class:\npublic class CommandLikeLogicPlugin implements CommandPlugin { @Override public boolean process(byte[] command, SparkConnectPlanner planner) { Any commandProto; try { commandProto = Any.parseFrom(command); } catch (InvalidProtocolBufferException e) { throw new RuntimeException(e); } if (commandProto.is(CallCommandLikeLogic.class)) { try { var message = commandProto.unpack(CallCommandLikeLogic.class); CommandLikeLogic.commandA(message.getParamA(), message.getParamB(), message.getParamC()); } catch (IOException e) { return false; } } return true; } } NOTE: In this case CallCommandLikeLogic is exactly the generated by buf class. It contains more than 500 lines of Java code, so I cannot put it here. But you may generate your own to see how it looks like!\nLet\u0026rsquo;s in details on the syntax.\nimplements CommandPlugin is just about which interface should we implement; we always gets byte[] as an input; you may check it the spark source code; Any in this case not just any class, but proto.Any: link to rest API documentation; planner is a spark internal tool that contains a lot of useful things, including SparkSession itself: var = planner.sessionHolder().session();; command.is(...) verifies that the embedded message types is of the class we expect; command.unpack(...) unpack the command into an actual generated Class with methods; finally we just call our method from out JVM \u0026ldquo;library\u0026rdquo; like class. RelationPlugin A more tricky case is when we need to return DataFrame. For that case we need to implement RelationPlugin. It requires to override a method transform:\npublic class DataFrameLogicPlugin implements RelationPlugin { @Override public Optional\u0026lt;LogicalPlan\u0026gt; transform(byte[] relation, SparkConnectPlanner planner) { Any relationProto; try { relationProto = Any.parseFrom(relation); } catch (InvalidProtocolBufferException e) { throw new RuntimeException(e); } if (relationProto.is(CallDataFrameLogic.class)) { return Optional.of(DataFrameLogic.createDummyDataFrame().logicalPlan()); } return Optional.empty(); } } As one may see it is quite similar to command but instead of boolean we need to return an Optional\u0026lt;LogicalPlan\u0026gt;. On the JVM side you can easily get this plan by just calling df.logicalPlan() and that\u0026rsquo;s it.\nNOTE: You only need to return Optional.empty if the command is not for this handler! In all other cases you must either throw an exception or return an actual LogicalPlan!\nObjects Manipulation The most advanced case is when we want to manipulate an object on the JVM side, as it can be done before with py4j. It is a little tricky because we can only use LogicalPlan for interaction between JVM and client! To implement this thing I will use the following:\nI will use HashMap\u0026lt;Integer, Object\u0026gt; to map all objects to integer IDs; To generate an ID of the object I will use System.identifyHashCode(obj); All communication with the client will be done via LogicalPlan, even error messages can be transported this way; Inside I will parse an input message and call create, delete object or call methods; To simplify things, I based my code on the assumption that there may be only 100 objects, or that arguments passed by the client are always correct. And of course I have not touched the topic of ID collision. But you can extend this code to achieve all of the above. My goal was just to create a kind of proof of concept, nothing more!\npublic class ObjectManipulationLogicPlugin implements RelationPlugin { /** This is a map, that stores link to all the objects. */ private static final HashMap\u0026lt;Integer, ObjectManipulationLogic\u0026gt; idsMapping = new HashMap\u0026lt;\u0026gt;(100); public static ObjectManipulationLogic getObj(Integer id) { return idsMapping.get(id); } public static Integer addObj(ObjectManipulationLogic obj) { var id = System.identityHashCode(obj); idsMapping.put(id, obj); return id; } public static void update(Integer id, ObjectManipulationLogic obj) { idsMapping.put(id, obj); } public static void dropObj(Integer id) { idsMapping.remove(id); } private Dataset\u0026lt;Row\u0026gt; getSuccessDF(SparkSession spark) { return spark.createDataFrame( List.of(RowFactory.create(\u0026#34;success\u0026#34;)), new StructType( new StructField[] { DataTypes.createStructField(\u0026#34;status\u0026#34;, DataTypes.StringType, false) })); } @Override public Optional\u0026lt;LogicalPlan\u0026gt; transform(byte[] relation, SparkConnectPlanner planner) { // To make the code simpler I\u0026#39;m not checking type of passed from Python arguments! // So, the overall logic is build on the assumption, that it is impossible to get // from python an invalid string or invalid long. // // It makes sense, because it is x10 simpler to do it on the Python side Any relationProto; try { relationProto = Any.parseFrom(relation); } catch (InvalidProtocolBufferException e) { throw new RuntimeException(e); } if (relationProto.is(CallObjectManipulationLogic.class)) { var spark = planner.sessionHolder().session(); try { // We are parsing the message var message = relationProto.unpack(CallObjectManipulationLogic.class); if (message.getNewObject()) { // If we need to create a new object we are doing the following: // 1. Get args // 2. Create an instance // 3. Add an id of the instance to the Map // 4. Return the id to Python var args = message.getArgsList(); var paramA = args.get(0); var paramB = Long.parseLong(args.get(1)); var instance = new ObjectManipulationLogic(paramA, paramB); var id = ObjectManipulationLogicPlugin.addObj(instance); var df = spark.createDataFrame( List.of(RowFactory.create(id)), new StructType( new StructField[] { DataTypes.createStructField(\u0026#34;id\u0026#34;, DataTypes.IntegerType, false) })); return Optional.of(df.logicalPlan()); } else if (message.getDeleteObject()) { // If we need to drop the object we just delete it from the Map // After that GC will do it\u0026#39;s work. var id = message.getObjectId(); ObjectManipulationLogicPlugin.dropObj(id); return Optional.empty(); } else { // All other cases is when we need to call a method var methodName = message.getMethodName(); var args = message.getArgsList(); var id = message.getObjectId(); var instance = ObjectManipulationLogicPlugin.getObj(id); // Possible to do the same via Reflection API; // But to achieve explicitly I\u0026#39;m directly check the method name. // We need to know types anyway, to return a DataFrame with a right schema. // So, we are checking all the possible methods and do the following: // 1. If it is setter than just parse args and modify the obj // 2. If it is getter or toString we just wrap the output into DataFrame switch (methodName) { case \u0026#34;getStrParameter\u0026#34; -\u0026gt; { var df = spark.createDataFrame( List.of(RowFactory.create(instance.getStrParameter())), new StructType( new StructField[] { DataTypes.createStructField(\u0026#34;strParameter\u0026#34;, DataTypes.StringType, false) })); return Optional.of(df.logicalPlan()); } case \u0026#34;getLongParameter\u0026#34; -\u0026gt; { var df = spark.createDataFrame( List.of(RowFactory.create(instance.getLongParameter())), new StructType( new StructField[] { DataTypes.createStructField(\u0026#34;longParameter\u0026#34;, DataTypes.LongType, false) })); return Optional.of(df.logicalPlan()); } case \u0026#34;setStrParameter\u0026#34; -\u0026gt; { instance.setStrParameter(args.get(0)); update(id, instance); return Optional.of(getSuccessDF(spark).logicalPlan()); } case \u0026#34;setLongParameter\u0026#34; -\u0026gt; { instance.setLongParameter(Long.parseLong(args.get(0))); update(id, instance); return Optional.of(getSuccessDF(spark).logicalPlan()); } case \u0026#34;toString\u0026#34; -\u0026gt; { var df = spark.createDataFrame( List.of(RowFactory.create(instance.toString())), new StructType( new StructField[] { DataTypes.createStructField( \u0026#34;stringRepresentation\u0026#34;, DataTypes.StringType, false) })); return Optional.of(df.logicalPlan()); } default -\u0026gt; { var df = spark.createDataFrame( List.of( RowFactory.create(String.format(\u0026#34;Invalid method name %s\u0026#34;, methodName))), new StructType( new StructField[] { DataTypes.createStructField(\u0026#34;errorMessage\u0026#34;, DataTypes.StringType, false) })); return Optional.of(df.logicalPlan()); } } } } catch (IOException e) { // In the case of error we are just wrapping the error message to DataFrame var sw = new StringWriter(); var pw = new PrintWriter(sw); e.printStackTrace(pw); var df = spark.createDataFrame( List.of(RowFactory.create(String.format(\u0026#34;IOException %s\u0026#34;, sw))), new StructType( new StructField[] { DataTypes.createStructField(\u0026#34;errorMessage\u0026#34;, DataTypes.StringType, false) })); return Optional.of(df.logicalPlan()); } } // That is the case when the message corresponds to another plugin/extension. return Optional.empty(); } } Build the project To build the project it is enough just to call mvn clean package and it will create an artifcat like this connect-1.0-SNAPSHOT.jar. Of course, in assumption, that you are using the code of my example from GitHub repository!\nSparkConnect Plugins in Python To setup all the dependencies do the following:\nMake SPARK_HOME variable points to the place where gou cloned spark. In my case it is export SPARK_HOME=~/github/spark; In you python project (in my repo it is src/main/python) create virtual environment: python3.10 -m venv .venv; Install pyspark dependencies: source .venv/bin/activate \u0026amp;\u0026amp; pip install -r ${SPARK_HOME}/dev/requirements.txt; Install pyspark itself: source .venv/bin/activate \u0026amp;\u0026amp; pip install ${SPARK_HOME}/python/dist/pyspark-4.0.0.dev0.tar.gz. It should be enough to run my example and to make autocomplete works.\nImplementation in Python If you did not generate needed python-classes that implements protobuf Messages, you can easily do it via buf generate (from src/main).\nThe next step is to implement a gRPC client.\nCallCommand Each message kind should be implemented as a separate Python class that inherit from pyspark.sql.connect.plan.LogicalPlan. For command you need to override the command method:\nclass CallCommandPlan(LogicalPlan): def __init__(self, param_a: int, param_b: int, param_c: str) -\u0026gt; None: super().__init__(None) self._a = param_a self._b = param_b self._c = param_c def command(self, session: SparkConnectClient) -\u0026gt; proto.Command: command = proto.Command() command.extension.Pack(CallCommandLikeLogic(paramA=self._a, paramB=self._b, paramC=self._c)) return command And after that you can create a function that uses this class. Remember that even if it is just a Command, you still need to call an action on the returned DataFrame!\ndef call_command(a: int, b: int, file_name: str, spark: SparkSession) -\u0026gt; None: print(file_name) DataFrame(CallCommandPlan(a, b, file_name), spark).collect() CallDataFrame The next in a row is a dummy function that generates DataFrame:\nclass CallDataFrameLogicPlan(LogicalPlan): def __init__(self) -\u0026gt; None: super().__init__(None) def plan(self, session: SparkConnectClient) -\u0026gt; proto.Relation: plan = self._create_proto_relation() ext = CallDataFrameLogic() plan.extension.Pack(ext) return plan def create_dataframe_extension(spark: SparkSession) -\u0026gt; DataFrame: return DataFrame(CallDataFrameLogicPlan(), spark) Almost like a Command but now you need to override plan method, not command.\nObjects Manipulation The most tricky part. As you remember, we used IDs in JVM side to provide a way of interacting between client and java-object. But before we implement a Python wrapper on top of Java object we need to create a LogicalPlan extension:\nclass CallObjectManipulationPlan(LogicalPlan): def __init__( self, object_id: int = 0, new_object: bool = False, delete_object: bool = False, method_name: str = \u0026#34;\u0026#34;, jargs: list[str] | None = None, ) -\u0026gt; None: if jargs is None: jargs = [] super().__init__(None) self._object_id = object_id self._new_object = new_object self._delete_object = delete_object self._method_name = method_name self._jargs = jargs def plan(self, session: SparkConnectClient) -\u0026gt; proto.Relation: plan = self._create_proto_relation() ext = CallObjectManipulationLogic( objectId=self._object_id, newObject=self._new_object, deleteObject=self._delete_object, methodName=self._method_name, args=self._jargs, ) plan.extension.Pack(ext) return plan This class is very similar to our dummy CallDataFrameLogicPlan but not we have an actual constructor with fields.\nLet\u0026rsquo;s see, how the Python class corresponds to the Java class may be written:\nclass JavaLikeObject: def __init__(self, param_a: str, param_b: int, spark: SparkSession) -\u0026gt; None: query_plan = CallObjectManipulationPlan( new_object=True, jargs=[param_a, str(param_b)] ) df = DataFrame(query_plan, spark) if \u0026#34;errorMessage\u0026#34; in df.columns: err_msg = df.collect()[0].asDict().get(\u0026#34;errorMessage\u0026#34;, \u0026#34;\u0026#34;) raise ValueError(err_msg) obj_id = df.collect()[0].asDict().get(\u0026#34;id\u0026#34;, -1) self._id = obj_id self._spark = spark def get_str_parameter(self) -\u0026gt; str: query_plan = CallObjectManipulationPlan( object_id=self._id, method_name=\u0026#34;getStrParameter\u0026#34; ) return ( DataFrame(query_plan, self._spark) .collect()[0] .asDict() .get(\u0026#34;strParameter\u0026#34;, \u0026#34;\u0026#34;) ) def get_long_parameter(self) -\u0026gt; int: query_plan = CallObjectManipulationPlan( object_id=self._id, method_name=\u0026#34;getLongParameter\u0026#34; ) return ( DataFrame(query_plan, self._spark) .collect()[0] .asDict() .get(\u0026#34;longParameter\u0026#34;, -1) ) def set_str_parameter(self, str_par: str) -\u0026gt; None: query_plan = CallObjectManipulationPlan( object_id=self._id, method_name=\u0026#34;setStrParameter\u0026#34;, jargs=[str_par] ) DataFrame(query_plan, self._spark).collect() def set_long_parameter(self, long_par: int) -\u0026gt; None: query_plan = CallObjectManipulationPlan( object_id=self._id, method_name=\u0026#34;setLongParameter\u0026#34;, jargs=[str(long_par)], ) DataFrame(query_plan, self._spark).collect() def to_string(self) -\u0026gt; str: query_plan = CallObjectManipulationPlan( object_id=self._id, method_name=\u0026#34;toString\u0026#34; ) return ( DataFrame(query_plan, self._spark) .collect()[0] .asDict() .get(\u0026#34;stringRepresentation\u0026#34;, \u0026#34;\u0026#34;) ) def delete(self) -\u0026gt; None: query_plan = CallObjectManipulationPlan( object_id=self._id, delete_object=True ) DataFrame(query_plan, self._spark) Here we just cover each of possible method in Java in the corresponding Python method.\nEnd2end demo Let\u0026rsquo;s check the most interesting part: objects manipulation. To do we need to create a main.py file:\nfrom pyspark.sql.connect.session import SparkSession from spark_connect_example.app_plugin import JavaLikeObject if __name__ == \u0026#34;__main__\u0026#34;: spark = SparkSession.builder.remote(\u0026#34;sc://localhost:15002\u0026#34;).getOrCreate() # Creating an object java_like = JavaLikeObject(param_a=\u0026#34;some\u0026#34;, param_b=100, spark=spark) # Call setter java_like.set_str_parameter(\u0026#34;some\u0026#34;) # Call setter java_like.set_long_parameter(1) # Call getter print(java_like.get_long_parameter()) # Call setter again java_like.set_long_parameter(2) # Check that getter returns a new value print(java_like.get_long_parameter()) # Call toString print(java_like.to_string()) Running SparConnect server Now we need to do some manipulations in the folder where we cloned spark. For me it was ~/github/spark. Or just do something like cd $SPARK_HOME.\nBefore we can actually check this, we need to have the SparkConnect server up and running. For some unknown reason, a recommended way to build Spark with connect support did not add the Java protobuf library to CP. To fix this, I just got it from Maven Central: wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.22.0/protobuf-java-3.22.0.jar.\nWe also need to copy the built connect-1.0-SNAPSHOT.jar into the same folder as spark. Finally, we need to execute a very long command, so it is better to create a short bash script for it:\n./sbin/start-connect-server.sh \\ --wait \\ --verbose \\ --jars connect-1.0-SNAPSHOT.jar,protobuf-java-3.22.0.jar \\ --conf spark.connect.extensions.command.classes=com.ssinchenko.example.server.CommandLikeLogicPlugin \\ --conf spark.connect.extensions.relation.classes=com.ssinchenko.example.server.DataFrameLogicPlugin,com.ssinchenko.example.server.ObjectManipulationLogicPlugin \\ --packages org.apache.spark:spark-connect_2.13:4.0.0-SNAPSHOT Let\u0026rsquo;s see it in details:\n--wait means to wait for the process to exit. Otherwise SparkConnect will run in background; --verbose just add some additional debug information; --jars: we need to pass here our connect-1.0-SNAPSHOT.jar. In my case it was necessary to pass also protobuf-java; --conf spark.connect.extensions.relation.classes=...: we should mention here all the plugins for relations; --conf spark.connect.extensions.relation.classes: the same, but for commands; --packages org.apache.spark:spark-connect_2.13:4.0.0-SNAPSHOT: just tell spark what to run. To run the server, just run the bash script we just made. Another way is to copy this long command into you terminal.\nRunning an application After running the server, just go back to our src/main/python and run the command source .venv/bin/activate \u0026amp;\u0026amp; python main.py. You will see something like:\n1 2 ObjectManipulationLogic{strParameter=\u0026#39;some\u0026#39;, longParameter=2} NOTE: To see detailed logs from python side, use export SPARK_CONNECT_LOG_LEVEL=debug\nAnyway, as you can see, our workaround to call JVM from PySpark-connect thin client works!\n🥳🥳🥳🥳🥳🥳🥳🥳\nDiscussion What was cool? I found that having a built-in way to serialize and deserialize LogicalPlan is very cool! It opens up a lot of possibilities for what you can do with SparkConnect! By packing everything into DataFrame and unpacking it on the client side, you can pass almost anything. I guess you could even serialize code into binary format and pack it into DataFrame. Of course, this might look a little crazy, but no more crazy than the magic of py4j!\nWhat seems to be missing Test environment There is no easy way to test your application against SparkConnect. You always have to get binaries, and that is not cool. Even if you need to work with the 3.5.x version, you always have to download both binaries and PySpark, which means you download all jar files twice. It would be very cool if we had a simpler way.\nError messages Error messages from SparkConnect are still sometimes not informative enough. For example, you might get something like org.apache.spark.sql.connect.common.InvalidPlanInput: No handler found for extension and it is not at all obvious what the reason is and on which message it happened. Is the problem in the Java plugin implementation? Or maybe there is a problem in the Python serialization? Or is it just a missing plugin class in CP? Unfortunately, you have to check all these cases, because the error message says almost nothing.\nAfterword I hope my example will help you in your research and development!\nP.S. If you find this guide useful, it is not necessary to buy me coffee or anything like that. Just go and star the corresponding repository in Git to show me that all my efforts were not in vain!\n","permalink":"http://localhost:1313/ssinchenko/post/extending-spark-connect/","summary":"This blog post presents a very detailed step-by-step guide on how to create a SparkConnect protocol extension in Java and call it from PySpark. It will also cover a topic about how to define all the necessary proto3 messages for it. At the end of this guide you will have a way to interact with Spark JVM from PySpark almost like you can with py4j in a non-connect version.","title":"Extending Spark Connect"},{"content":" Preface There is no rocket science in this blog post, just some examples and information from the official Maven documentation. By the way, when I started working on this topic, it was not easy to find an example on the Internet. Even ChatGPT did not provide me with a 100% working solution in case of a complex Maven project. From this point of view, I think that my post might be useful for someone who wants to dive into the similar topic from scratch.\nIntroduction I always like two topics: Apache Spark and graphs. Recently I had a nice chance to work on an open source project that combines these two topics. It is a GraphAr project, a novel way and format to store network data in data lakes. You can think of it as a Delta Lake for Big Graphs, just because the general idea is quite close: we have a metadata file and store the underlying data (vertices and edges) in parquet (or orc, or csv) files. The project is quite young, so initially only Apache Spark 3.2.2 was supported. I have been looking for a new OSS project to contribute to for a long time, so I committed to extend support to at least two versions: 3.2.2 and 3.3.4.\nInitially the project had the structure like this:\n| graphar |- spark |-- src |--- main |---- scala/com/alibaba/graphar |----- datasources |------ GarDataSource.scala |------ ... |----- commonCode |------ CommonClass.scala |------ AnotherCommonClass.scala |-- pom.xml The main problem: Spark Datasources API An implementation of tools for working with a GraphAr format in Spark contains two main parts:\nAn implementation of helpers and methods for working with metadata An implementation of datasources to allow users to write read.format(\u0026#34;com.alibaba.graphar.datasources.GarDataSource\u0026#34;).save(\u0026#34;...\u0026#34;) If the first one is mostly generic and uses spark @Stable APIs, the second one is quite tricky and calls parquet, orc and csv ReaderFactory / OutputWriter implementations. Because of continued moving internal spark datasources from v1 to v2, the second part is the biggest problem. Even switching from 3.2.x to 3.3.x break everything.\nObvious solution: reflection The first and quite obvious though is, of course, to use Reflection API. It is a relative low-level JVM API that allows you to call classes and methods in a runtime. For example, let\u0026#39;s imagine we have a class that has a static method staticMethod(s: String) =\u0026gt; Int in spark 3.3.x but staticMethod(b: Boolean) =\u0026gt; Int in spark 3.2.\nval myClass = Class.forName(\u0026#34;com.mypackage.MyClass\u0026#34;) val staticMethod = myClass.getMethod(\u0026#34;staticMethod\u0026#34;) spark.version match { case s: String if s.startsWith(\u0026#34;3.3\u0026#34;) =\u0026gt; staticMethod .invoke(\u0026#34;true\u0026#34;).asInstanceOf[Int] case _ =\u0026gt; staticMethod.invoke(true).asInstanceOf[Int] The first problem is that such a code is very hard to read and maintain. In such a case you loose all the capabilities of modern IDEs like Emacs, that shows you inline errors and suggestions. Also, you loose an advantage of compiled language, because if you make a typo in a name of the class, or in a name of the method, you will know it only in runtime. Btw, spark itself uses reflection API to support both Hadoop 2 / Hadoop 3.\nThe second problem is that reflection can help you to resolve simple cases when an API of some library was changed. But reflection cannot help you, for example, if you need to override a Class or an Interface and this class/interface changed from one version of the library to another. That was exactly the case of com.alibaba.graphar.datasources.\nThe right way to do it: Maven Profiles Even being little old-school tool and being not specially supposed for Scala projects, Apache Maven is still very popular and very reliable building system for any JVM project. What is most important is that Maven provide Reactor, a tool for working with multi-projects with complex dependencies.\nSplitting GraphAr to common part and datasources part The first thing I needed to do is to split the monolithic GraphAr Spark project into two parts:\nA common part that contains the code, that uses @Stable API of spark A datasources subproject, that contains overriding of rapidly changing spark internal classes Because it was expected to provide the support of multiple spark versions in the future, I choose the following project structure:\n| graphar |- spark |-- graphar |--- src/main/scala/com/alibaba/graphar/... |--- pom.xml |-- datasources-32 |--- src/main/scala/com/alibaba/graphar/... |--- pom.xml |-- datasources-33 |----src/main/scala/com/alibaba/graphar/... |-- pom.xml Here graphar should contain the common code, and we have two datasources submodule version. One for spark 3.2.x and for spark 3.3.x specific code.\nTop-level pom.xml file The top-level pom.xml is quite simple and defines mostly the profiles. We will use one profile for spark 3.2.x and one for spark 3.3.x:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphar\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${graphar.version}\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;pom\u0026lt;/packaging\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;datasources-32\u0026lt;/id\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;sbt.project.name\u0026gt;graphar\u0026lt;/sbt.project.name\u0026gt; ... \u0026lt;spark.version\u0026gt;3.2.2\u0026lt;/spark.version\u0026gt; ... \u0026lt;graphar.version\u0026gt;0.1.0-SNAPSHOT\u0026lt;/graphar.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;modules\u0026gt; \u0026lt;module\u0026gt;graphar\u0026lt;/module\u0026gt; \u0026lt;module\u0026gt;datasources-32\u0026lt;/module\u0026gt; \u0026lt;/modules\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;datasources-33\u0026lt;/id\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;sbt.project.name\u0026gt;graphar\u0026lt;/sbt.project.name\u0026gt; ... \u0026lt;spark.version\u0026gt;3.3.4\u0026lt;/spark.version\u0026gt; ... \u0026lt;graphar.version\u0026gt;0.1.0-SNAPSHOT\u0026lt;/graphar.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;modules\u0026gt; \u0026lt;module\u0026gt;graphar\u0026lt;/module\u0026gt; \u0026lt;module\u0026gt;datasources-33\u0026lt;/module\u0026gt; \u0026lt;/modules\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; What is important here is that Maven Profiles does not allow you to override dependencies or other complex things. But, it allows you to create or override properties and you can use property, for example, for spark version for further overriding of dependencies!\nTo use Reactor build, the top-level module should always use pom packaging system.\nStarting from this moment you can call any Maven command for a specific profile in the following way:\nmvn clean package -P datasources-32 mvn clean package -P datasources-33 A small note about IDEs integration For a smooth integration with a Language Server (like metals) you need to specify, which profile should be used. You can add a default profile into top-level pom.xml in the following way (in a profile tag):\n\u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; datasources submodule pom.xml It is important to have scala-maven-plugin inside the pom.xml of every submodule, that contains scala code! Otherwise, even if Reactor choose the right compilation order, there will be errors because plugins are not pushed down from parent module to submodules!\nAny submodule in multi-module project should contains own pom.xml, that defines parent project. Cool thing is that inside submodule pom you can refer to properties, defined in the parent pom! Let\u0026#39;s see on a GraphAr submodule pom for datasources:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphar\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${graphar.version}\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphar-datasources\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${graphar.version}\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-core_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-streaming_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-mllib_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-sql_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-hive_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; As one may see, we are defining spark-core and spark-sql dependencies using a parent module properties spark.version and scala.version!\n\u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; here means that the dependency classes should not be included into output JAR file and will be presented in CP in runtime.\nCommons submodule pom.xml In our case, commons are depends on datasources implementation. But, because the package and classes in both datasources-32 and datasources-33 are the same, we do not need to specify it for each profile. It is enough to specify it only once as a dependency:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphar\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${graphar.version}\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphar-commons\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${graphar.version}\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphar-datasources\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${graphar.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-core_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.scalatest\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scalatest_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.scala-lang\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-library\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${scala.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; As one may see in this case Reactor will resolve the following dependency as an inner one:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;graphar-datasources\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${graphar.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; It is important to have scalatest in all the submodules that contain tests!\nHow Reactor build looks like Let\u0026#39;s see on an output of, for example, mvn clean package -P datasources-33:\n[INFO] ------------------------------------------------------------------------ [INFO] Reactor Build Order: [INFO] [INFO] graphar [pom] [INFO] graphar-datasources [jar] [INFO] graphar-commons [jar] [INFO] In this case, Maven realized, that graphar is just a top-level pom-module and that datasources should be compiled first because commons depends on it.\nYou can use GraphAr Spark implementation as a source of inspiration for your own spark-related projects. I hope that you found that post useful!\n","permalink":"http://localhost:1313/ssinchenko/post/multiple-spark-versions-with-maven/","summary":"I recently had the opportunity to work on an open source project that implements a custom Apache Spark data source and associated logic for working with graph data. The code was written to work with Apache Spark 3.2.2. I am committed to extending support to multiple versions of Spark. In this blog post I want to show how the structure of such a project can be organized using Maven profiles.","title":"Supporting multiple Apache Spark versions with Maven"},{"content":" Preface I work for a company that is a customer of the Databricks platform. However, this blog post represents only my personal view and opinion and is not related to any official position of my employer or my employer\u0026#39;s dissatisfaction with the service provided by Databricks.\nFollow-up I had a very productive discussion with Martin Grund and Matthew Powers from Databricks. I can summarize their points in the following list:\nThe main reason for pushing SparkConnect into DBR 14.x is security. Also, calls to the underlying JVM were already restricted in DBR 13.x, so it should not change anything dramatically. Long story short, the libraries listed below would most likely not work on shared clusters even in DBR 13.x; Spark developers are not going to reimplement all the py4j interfaces. Instead of providing a generic way to work with the underlying JVM, they would prefer to support migration of existing libraries to grpc; We will work together to fill the existing gap in documentation on how to extend the SparkConnect protocol; If some of these points are not clear to you, I would recommend going deeper into the details of the blog post.\nIntroduction: Spark-Connect When I saw the announcement of Spark Connect for the first time, I had very controversial feelings. From one side, developers of PySpark applications are finally getting the ability to use the latest minor version of Python and not depend on the long list of PySpark dependencies and JVM. But from the other side, I was thinking about how 3d-party PySpark libraries should work with it?\nA common pattern of 3d-party PySpark packages We all know that Python is not the fastest programming language, but it works very well as a glue for compiled languages like the JVM family. And this is exactly how PySpark worked before Spark Connect: we have a JVM when all the Spark routines are running and we have Py4j acting as a bridge. If you take a look at the SparkSession object, you can see the following private attributes:\nself._sc = sparkContext self._jsc = self._sc._jsc self._jvm = self._sc._jvm So, we are having an access to the underlying JVM and underlying JavaSparkContext object. Another important thing is an __init__ method of pyspark.sql.DataFrame object:\ndef __init__( self, jdf: JavaObject, sql_ctx: Union[\u0026#34;SQLContext\u0026#34;, \u0026#34;SparkSession\u0026#34;], ): It allows developers to take a py4j.java_gateway.JavaObject that represents an org.apache.spark.sql.DataFrame JVM object and create from it a PySpark DataFrame. Back converting is even easier: pyspark.sql.DataFrame has an attribute _jdf that is a reference to the underlying org.apache.spark.sql.DataFrame object:\nself._jdf: JavaObject = jdf One may already understand the obvious pattern of creating a PySpark package:\nWe are writing a core part of the package in Java/Scala with access to all Apache Spark APIs in fast compilable language We are creating a python library that just provide bindings to underlying JAR-package via py4j. For example, recently I was happy to create such a PySpark wrapper for a new huge Graph storage format library: my PR with PySpark bindings to GraphAr scala package.\nWhat was changed with SparkConnect? The answer is quite obvious: because Spark Connect decouples the driver from the user\u0026#39;s code, the user no longer has access to the underlying JVM. Of course, the topic of importance of py4j for 3d-party libs is very specific and many users of PySpark did not even notice the importance of this change. But for me it was the first question.\nI will be honest if I say that I tried to find an alternative way for 3d-parties to interact with the underlying JVM with Spark Connect. But there is almost nothing about it in all the promotional material from Databricks and in the Apache Spark documentation. The only theoretically possible workaround I could find is this tiny Markdown page, hidden very deep in the source code of Apache Spark. Based on this, I can imagine that the Spark Connect protocol was designed to be extensible, but again: there is no documentation on how to do it!\nWhat was changed in Databricks 14.x release and why it is an absolutely breaking change Databricks 14.x introduced the following changes:\nThere is no longer a dependency on the JVM when querying Apache Spark and as a consequence, internal APIs related to the JVM, such as _jsc, _jconf, _jvm, _jsparkSession, _jreader, _jc, _jseq, _jdf, _jmap, and _jcols are no longer supported. Of course, almost no one (including myself) checks the Release Notes of non-LTS releases of Databricks. And finally, at the beginning of February it happens: Databricks Runtime 14.3 LTS was released. Everyone, including me go and check the changes and see the same thing like in 14.0: _jvm, _jsc, _jsparkSession and _jdf are no longer available in Databricks Notebooks.\nSuch a change absolutely destroyed described above pattern of creating PySpark 3d-party packages…\nWhy is it important? Of course, one may say: Ok, they break something, but no-one except you care about it because everything you need is inside Databricks and Apache Spark itself. Ok, lets see which libraries will be broken.\nMicrosoft Synapse ML (ex MMLSpark) Synapse ML is a well know (4.9k stars) spark extension, focused on applying ML/DL on Apache Spark clusters. One may know it as MMLSpark. There core part of the library is written in Scala, but APIs for R, Python, #NET and Java are provided. If one make a look how a Python API is organized under the hood they would see the described above \u0026#34;py4j-pattern\u0026#34;:\nclass DiscreteHyperParam(object): \u0026#34;\u0026#34;\u0026#34; Specifies a discrete list of values. \u0026#34;\u0026#34;\u0026#34; def __init__(self, values, seed=0): ctx = SparkContext.getOrCreate() self.jvm = ctx.getOrCreate()._jvm self.hyperParam = self.jvm.com.microsoft.azure.synapse.ml.automl.HyperParamUtils.getDiscreteHyperParam( values, seed, ) link to the code above\nDue to the popularity of that library they already faced issues from Databricks users: [BUG] Databricks 14.3 LTS usage of internal _jvm variable is no longer supported #2167. And I have zero ideas how they are going to fix it because to make it work with Spark Connect they need to rewrite all the logic in pure Python/PySpark.\nAmazon Deequ/PyDeequ PyDeequ is a popular (625 stars) Data Quality library that is native to Apache Spark because its core is written in Scala. Again, if one make a look on how is it implemented under the hood they will see \u0026#34;py4j-pattern\u0026#34; again:\nclass _AnalyzerObject: \u0026#34;\u0026#34;\u0026#34; Analyzer base object to pass and accumulate the analyzers of the run with respect to the JVM \u0026#34;\u0026#34;\u0026#34; def _set_jvm(self, jvm): self._jvm = jvm return self @property def _deequAnalyzers(self): if self._jvm: return self._jvm.com.amazon.deequ.analyzers raise AttributeError( \u0026#34;JVM not set, please run _set_jvm() method first.\u0026#34; ) # TODO: Test that this exception gets raised link to the code above\nSpark-NLP Spark-NLP is one of the most popular (3.6k stars) way to run LLMs on Apache Spark clusters. Let\u0026#39;s again go the source code and see how it works. Oops, looks like we found using of _jdf / _jvm again:\nclass RecursiveEstimator(JavaEstimator, ABC): def _fit_java(self, dataset, pipeline=None): self._transfer_params_to_java() if pipeline: return self._java_obj.recursiveFit(dataset._jdf, pipeline._to_java()) else: return self._java_obj.fit(dataset._jdf) link to the code above\nSpark-extensions spark-extensions is relative popular (155 stars) and actively maintained library, that contains a lot of small helpers and extensions of Apache Spark/PySpark. Under the hood its PySpark part is based on the \u0026#34;py4j-pattern\u0026#34; (yes, again):\nfunc = sc._jvm.uk.co.gresearch.spark.__getattr__(\u0026#34;package$\u0026#34;).__getattr__(\u0026#34;MODULE$\u0026#34;).dotNetTicksToTimestamp link to the code above\nH2O Sparkling Water Sparkling Water is an official way to run H2O models on Apache Spark cluster. Repository has 955 stars and is actively maintained. Under the hood one may again find \u0026#34;py4j-pattern\u0026#34; that is based on _jvm / _jdf:\nclass H2OTargetEncoderModel(H2OTargetEncoderMOJOParams, JavaModel, JavaMLWritable): def transform(self, dataset): callerFrame = inspect.stack()[1] inTrainingMode = (callerFrame[3] == \u0026#39;_fit\u0026#39;) \u0026amp; callerFrame[1].endswith(\u0026#39;pyspark/ml/pipeline.py\u0026#39;) if inTrainingMode: return self.transformTrainingDataset(dataset) else: return super(H2OTargetEncoderModel, self).transform(dataset) def transformTrainingDataset(self, dataset): self._transfer_params_to_java() return DataFrame(self._java_obj.transformTrainingDataset(dataset._jdf), dataset.sql_ctx) link to the code above\nPayPal gimel gimel is a quite popular (239 stars) framework that is built on top of Apache Spark. In the documentation they directly recommend to use \u0026#34;py4j-pattern\u0026#34;:\n# import DataFrame and SparkSession from pyspark.sql import DataFrame, SparkSession, SQLContext # fetch reference to the class in JVM ScalaDataSet = sc._jvm.com.paypal.gimel.DataSet # fetch reference to java SparkSession jspark = spark._jsparkSession # initiate dataset dataset = ScalaDataSet.apply(jspark) # Read Data | kafka semantics abstracted for user df = dataset.read(\u0026#34;kafka_dataset\u0026#34;) # Apply transformations (business logic | abstracted for Gimel) transformed_df = df(...transformations...) # Write Data | Elastic semantics abstracted for user dataset.write(\u0026#34;elastic_dataset\u0026#34;,df) link to the code above\nHNSWlib-spark HNSWlib is a quite popular (240 stars) and modern JVM library for an Approximate Nearest Neighbors Search. hnswlib-spark is an Apache Spark/PySpark wrapper on top of the main library. And under the hood PySpark part is partially based on a \u0026#34;py4j-pattern\u0026#34; by using SparkContext constructor:\ndef __init__(self): spark_conf = SparkConf() spark_conf.setAppName(spark_nlp_config.app_name) spark_conf.setMaster(spark_nlp_config.master) spark_conf.set(\u0026#34;spark.driver.memory\u0026#34;, memory) spark_conf.set(\u0026#34;spark.serializer\u0026#34;, spark_nlp_config.serializer) spark_conf.set(\u0026#34;spark.kryo.registrator\u0026#34;, spark_nlp_config.registrator) spark_conf.set(\u0026#34;spark.jars.packages\u0026#34;, spark_nlp_config.maven_spark) spark_conf.set(\u0026#34;spark.hnswlib.settings.index.cache_folder\u0026#34;, cache_folder) # Make the py4j JVM stdout and stderr available without buffering popen_kwargs = { \u0026#39;stdout\u0026#39;: subprocess.PIPE, \u0026#39;stderr\u0026#39;: subprocess.PIPE, \u0026#39;bufsize\u0026#39;: 0 } # Launch the gateway with our custom settings self.gateway = launch_gateway(conf=spark_conf, popen_kwargs=popen_kwargs) self.process = self.gateway.proc # Use the gateway we launched spark_context = SparkContext(gateway=self.gateway) self.spark_session = SparkSession(spark_context) self.out_thread = threading.Thread(target=self.output_reader) self.error_thread = threading.Thread(target=self.error_reader) self.std_background_listeners() link to the code above\nThe Archives Unleashed Toolkit AUT is a tool and a library to analyze Web Archives on Apache Spark clusters. Its PySpark part uses the same \u0026#34;py4j-pattern\u0026#34;:\nclass WebArchive: def __init__(self, sc, sqlContext, path): self.sc = sc self.sqlContext = sqlContext self.loader = sc._jvm.io.archivesunleashed.df.DataFrameLoader(sc._jsc.sc()) self.path = path link to the code above\nApache Linkis Linkis is a top-level Apache project (3.2k stars). It\u0026#39;s PySpark part is heavily based on the same \u0026#34;py4j-pattern\u0026#34;:\njsc = intp.getJavaSparkContext() jconf = intp.getSparkConf() conf = SparkConf(_jvm = gateway.jvm, _jconf = jconf) sc = SparkContext(jsc=jsc, gateway=gateway, conf=conf) sqlc = HiveContext(sc, intp.sqlContext()) sqlContext = sqlc spark = SparkSession(sc, intp.getSparkSession()) link to the code above\nSpark-dgraph-connector spark-dgraph-connector is an another project from G-Research. It\u0026#39;s PySpark part uses the same \u0026#34;py4j-pattern\u0026#34;:\nclass DgraphReader: def __init__(self, reader: DataFrameReader): super().__init__() self._jvm = reader._spark._jvm self._spark = reader._spark self._reader = self._jvm.uk.co.gresearch.spark.dgraph.connector.DgraphReader(reader._jreader) link to the code above\nGraphAr And finally a project where I\u0026#39;m a contributor and maintainer of PySpark part: GraphAr. GraphAr is a novel way to store huge Graph data in DataLake or LakeHouse solutions. The whole PySpark part is based on \u0026#34;py4j-pattern\u0026#34;. Mostly because Synapse ML and PyDeequ were main sources of inspiration for me when I worked on the implementation…\nAn endless amount of in-house solutions and libraries I\u0026#39;m more than sure that many companies using Databricks have their own in-house helpers, libraries, etc. And I\u0026#39;m more than sure that a lot of these in-house projects rely on the same \u0026#34;py4j-pattern\u0026#34;.\nDiscussion The main question for me here is why is Databricks pushing Spark Connect so hard? I have always seen Databricks as a company founded by computer science rock stars and open source enthusiasts. I hope that the new policy of breaking 3d party libs in Databricks runtime and notebooks is just an incident and there will be an explanation soon. And I really hope that with such an action Databricks is not trying to force people to use only the built-in proprietary tools of the platform (like the recently announced data quality solution instead of PyDeequ). We all love Databricks because it is based on open source tools and because the company is so open to collaboration and integration.\nI love the whole idea of Spark Connect. Many benefits of using it are obvious:\nRelax dependencies and requirements on user code; The ability to expose the Spark API to more programming languages (Golang, Rust, etc.); An ability to simplify integration with IDEs (JetBrains, VSCode, Vim, Emacs, etc.); A lot of other benefits… The only problem is the speed with which Spark Connect is pushed by Databricks. In my opinion, in this case, Databricks should not just say something like \u0026#34;Guys, you used private stuff, there was no guarantee that it would work, so it is your and only your problem\u0026#34; to all 3d party project developers.\n","permalink":"http://localhost:1313/ssinchenko/post/how-databricks-14x-breaks-3dparty-compatibility/","summary":"In this post, I want to discuss the groundbreaking changes in the latest LTS release of the Databricks runtime. This release introduced Spark Connect as the default way to work with shared clusters. I will give a brief introduction to the topic of internal JVM calls and Spark Connect, provide examples of 3d-party OSS projects broken in 14.3, and try to understand the reasons for such a move by Databricks.","title":"How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility"},{"content":" PySpark column-level lineage Introduction In this post, I will show you how to use information from the spark plan to track data lineage at the column level. Let\u0026#39;s say we have the following DataFrame object:\nfrom pyspark.sql import SparkSession, functions as F spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() dat = spark.read.csv(\u0026#34;/home/sem/github/farsante/h2o-data-rust/J1_1e8_1e5_5.csv\u0026#34;, header=True) dat.printSchema() Result:\nroot |-- id1: string (nullable = true) |-- id2: string (nullable = true) |-- id4: string (nullable = true) |-- id5: string (nullable = true) |-- v2: string (nullable = true) Let\u0026#39;s create some transformations on top of our dat object:\ndat_new = ( dat.withColumn(\u0026#34;id1_renamed\u0026#34;, F.col(\u0026#34;id1\u0026#34;)) .withColumn(\u0026#34;id1_and_id2\u0026#34;, F.concat_ws(\u0026#34;_\u0026#34;, F.col(\u0026#34;id1\u0026#34;), F.col(\u0026#34;id2\u0026#34;))) .withColumn(\u0026#34;num1\u0026#34;, F.lit(1)) .withColumn(\u0026#34;num2\u0026#34;, F.lit(2)) .filter(F.rand() \u0026lt;= F.lit(0.5)) .select(\u0026#34;id1_renamed\u0026#34;, \u0026#34;id1_and_id2\u0026#34;, \u0026#34;id1\u0026#34;, \u0026#34;num1\u0026#34;, \u0026#34;num2\u0026#34;) .withColumn(\u0026#34;hash_id\u0026#34;, F.hash(\u0026#34;id1_and_id2\u0026#34;)) .join(dat.select(\u0026#34;id1\u0026#34;, \u0026#34;id4\u0026#34;), on=[\u0026#34;id1\u0026#34;], how=\u0026#34;left\u0026#34;) .withColumn(\u0026#34;hash_of_two_ids\u0026#34;, F.concat_ws(\u0026#34;_\u0026#34;, \u0026#34;id4\u0026#34;, \u0026#34;hash_id\u0026#34;)) .groupBy(\u0026#34;id1_renamed\u0026#34;) .agg(F.count_distinct(\u0026#34;hash_of_two_ids\u0026#34;).alias(\u0026#34;cnt_ids\u0026#34;), F.sum(F.col(\u0026#34;num1\u0026#34;) + F.col(\u0026#34;num2\u0026#34;)).alias(\u0026#34;sum_col\u0026#34;)) ) Even with such a small transformation, it is not at all obvious which column is coming from where. Tracking transformations is the goal of Data Lineage. There are several types of data lineage:\nAt the data source level, when we want to track all data sources in the process of our transformations; At the column level, when we want to track how which column was transformed during the process. In this post I will focus on the second one, but the first one can be achieved in a similar way. But to implement it, we need to understand a little bit how Apache Spark works, how lazy computations work, and what the Directed Acyclic Graph of computations is.\nA short introduction to spark computations model and Catalyst You can get a deeper dive by reading an original paper about Spark SQL and Catalyst: Spark SQL: Relational Data Processing in Spark. I will just give a top-level overview.\nWhen you apply a transformation, like withColumn(\u0026#34;num1\u0026#34;, F.lit(1)), Spark only adds a step to the computation graph, but does not add an actual column to the PySpark DataFrame you are working with. So at any moment, DataFrame is not a \u0026#34;real\u0026#34; data, but just a directed graph of computation steps. PySpark provides a way to get a string representation of the plan, to work with a plan as with a real graph data structure you need to use the Scala/Java API of Apache Spark. When you perform an action, like df.count() or df.write, Spark will get your computation graph and make an execution. This is a very simplified view, because in reality there are many different intermediate steps:\nTransforming the parsed logical plan into an analyzed logical plan by resolving sources and column references; Optimizing the logical plan by applying optimization rules (such as moving filter expressions to the beginning of the plan, or moving select expressions to the source column level); Generate different versions of physical plans based on the same optimized logical plan; Apply cost-based selection of the best physical plan; perform code generation based on the selected physical plan; Execute the code. For anyone who wants to better understand how spark works with plans and how optimizations can be applied, I highly recommend the book How query engines work by Andy Grow, creator of the Apache Arrow Datafusion.\nGetting a string-representation of plan in PySpark But for now, we just need the parsed logical plan, so let\u0026#39;s make a simple Python function that returns it:\nimport contextlib from pyspark.sql import DataFrame def get_logical_plan(df: DataFrame) -\u0026gt; str: with contextlib.redirect_stdout(StringIO()) as stdout: df.explain(extended=True) plan_lines = stdout.getvalue().split(\u0026#34;\\n\u0026#34;) start_line = plan_lines.index(\u0026#34;== Analyzed Logical Plan ==\u0026#34;) + 2 end_line = plan_lines.index(\u0026#34;== Optimized Logical Plan ==\u0026#34;) return \u0026#34;\\n\u0026#34;.join(plan_lines[start_line:end_line]) It may look overly complicated, but there is no other way to get a string representation of the analyzed logical plan from PySpark. df.explain returns nothing, instead it prints all plans (analyzed logical, optimized logical, physical) to standard output. That\u0026#39;s why we need to use contextlib.redirect_stdout. You can check what the whole output of df.explain looks like. It is broken up by lines like == Analyzed Logical Plan == and similar. Also, the analyzed logical plan always starts from the schema of the DataFrame, so we need to add another line.\nLet\u0026#39;s see what the plan looks like for our dat_new DataFrame that we created:\nget_logical_plan(dat_new) Aggregate [id1_renamed#2430], [id1_renamed#2430, count(distinct hash_of_two_ids#2491) AS cnt_ids#2508L, sum((num1#2445 + num2#2454)) AS sum_col#2510L] +- Project [id1#1321, id1_renamed#2430, id1_and_id2#2437, num1#2445, num2#2454, hash_id#2469, id4#2480, concat_ws(_, id4#2480, cast(hash_id#2469 as string)) AS hash_of_two_ids#2491] +- Project [id1#1321, id1_renamed#2430, id1_and_id2#2437, num1#2445, num2#2454, hash_id#2469, id4#2480] +- Join LeftOuter, (id1#1321 = id1#2478) :- Project [id1_renamed#2430, id1_and_id2#2437, id1#1321, num1#2445, num2#2454, hash(id1_and_id2#2437, 42) AS hash_id#2469] : +- Project [id1_renamed#2430, id1_and_id2#2437, id1#1321, num1#2445, num2#2454] : +- Filter (rand(-7677477572161899967) \u0026lt;= 0.5) : +- Project [id1#1321, id2#1322, id4#1323, id5#1324, v2#1325, id1_renamed#2430, id1_and_id2#2437, num1#2445, 2 AS num2#2454] : +- Project [id1#1321, id2#1322, id4#1323, id5#1324, v2#1325, id1_renamed#2430, id1_and_id2#2437, 1 AS num1#2445] : +- Project [id1#1321, id2#1322, id4#1323, id5#1324, v2#1325, id1_renamed#2430, concat_ws(_, id1#1321, id2#1322) AS id1_and_id2#2437] : +- Project [id1#1321, id2#1322, id4#1323, id5#1324, v2#1325, id1#1321 AS id1_renamed#2430] : +- Relation [id1#1321,id2#1322,id4#1323,id5#1324,v2#1325] csv +- Project [id1#2478, id4#2480] +- Relation [id1#2478,id2#2479,id4#2480,id5#2481,v2#2482] csv As you can see, the analyzed logical plan contains all calculation steps from the last one to the first one (Relation ... csv). An important thing is that PySpark adds unique IDs to each column, so the final names in the plan are not real column names, but something like name#unique_id. This will help us a lot when we will create our column lineage parser, because it simplifies all things: you do not need to think about collisions or renaming, because PySpark has already solved all these problems!\nParsing plan to get column-lineage As you can see, there is a limited list of possible operations:\nRelation: mapping of columns to files or tables; Project~: any column operation, such as withColumn, withColumnRenamed, select, etc; Filter~: any filter operation; Join~: various types of join operations; Aggregate~: aggregate operations; There are also some additional cases like Union, but the union operation makes things very complex, so let\u0026#39;s decide to avoid it. Just because if a plan contains Union it is very hard to parse it, because a column can appear in any side of a union-like operation…\nDefining an output data-structure and user API First, we need to define what our column lineage will look like and what the data structure representing the lineage will be. By design, the data lineage is a directed acyclic graph (or tree). One of the simplest ways to represent a graph-like structure is simply to use a list of edges (called an adjacency list). Nodes of our graph will contain not only ids, but also some additional information, like the description of the computation step. Let\u0026#39;s store the attributes in a dict-like structure. And the API should be very simple: just a function that takes a DataFrame object and a column name. For simplicity, it might also be good to store the list of all nodes in the graph. Let\u0026#39;s define the structure and a function signature:\nfrom dataclasses import dataclass @dataclass class ColumnLineageGraph: \u0026#34;\u0026#34;\u0026#34;Structure to represent columnar data lineage.\u0026#34;\u0026#34;\u0026#34; nodes: list[int] # list of hash values that represent nodes edges: list[list[int]] # list of edges in the form of list of pairs node_attrs: dict[int, str] # labels of nodes (expressions) def get_column_lineage(df: DataFrame, columns: str) -\u0026gt; ColumnLineageGraph: raise NotImplementedError() Creating recursive parsing function We will be using a lot of regular expressions and we need to import them first:\nimport re Transforming from graph-nodes to column names It doesn\u0026#39;t really matter that our logical plan is a list of strings. By design and idea, it is the tree structure, and the best way to traverse the tree is, of course, a recursion. Let\u0026#39;s create an inner recursive function to traverse the plan:\ndef _node2column(node: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Inner function. Transform the node from plan to column name. Like: col_11#1234L -\u0026gt; col_11. \u0026#34;\u0026#34;\u0026#34; match_ = re.match(r\u0026#34;([\\w\\d]+)#[\\w\\d]+\u0026#34;, node) if match_: return match_.groups()[0] We also need a way to get a node ID from the column name. To do this, let\u0026#39;s add another simple function:\ndef _get_aliases(col: str, line: str) -\u0026gt; tuple[list[str], str]: \u0026#34;\u0026#34;\u0026#34;Inner function. Returns all the aliases from the expr and expr itself.\u0026#34;\u0026#34;\u0026#34; alias_exp = _extract_alias_expressions(col, line) # Regexp to extract columns: each column has a pattern like col_name#1234 return (re.findall(r\u0026#34;[\\w\\d]+#[\\w\\d]+\u0026#34;, alias_exp), alias_exp) Parsing ALIAS expressions One of the most complicated cases in a Spark plan is an alias. You may be faced with the following options:\nLiteral expressions, like 1 AS col#1234; Just an alias, like col1#1234 AS col2#1235; An alias to the expression, like (col1#1234 + col2#1235) AS col3#1236. And the last one can contain an unlimited number of nested expressions. It is almost impossible to parse such a case via regular expressions, looks like we need to balance parentheses, as in Leetcode easy task. I will use a counter based approach, where we have a counter of unbalanced parentheses and we reach the end of the expression when the counter is zero.\ndef _extract_alias_expressions(col: str, line: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Inner function. Extract expression before ... AS col from the line.\u0026#34;\u0026#34;\u0026#34; num_close_parentheses = 0 # our counter idx = line.index(f\u0026#34; AS {col}\u0026#34;) # the end of the alias expression we need to parse alias_expr = [] # buffer to store what we are parsing if line[idx - 1] != \u0026#34;)\u0026#34;: \u0026#34;\u0026#34;\u0026#34;It is possible that there is no expression. It is the case when we just make a rename of the column. In the plan it will look like `col#123 AS col#321`; \u0026#34;\u0026#34;\u0026#34; for j in range(idx - 1, 0, -1): alias_expr.append(line[j]) if line[j - 1] == \u0026#34;[\u0026#34;: break if line[j - 1] == \u0026#34; \u0026#34;: break return \u0026#34;\u0026#34;.join(alias_expr)[::-1] \u0026#34;\u0026#34;\u0026#34;In all other cases there will be `(` at the end of the expr before AS. Our goal is to go symbol by symbol back until we balance all the parentheses. \u0026#34;\u0026#34;\u0026#34; for i in range(idx - 1, 0, -1): alias_expr.append(line[i]) if line[i] == \u0026#34;)\u0026#34;: # Add parenthesis num_close_parentheses += 1 if line[i] == \u0026#34;(\u0026#34;: if num_close_parentheses == 1: # Parentheses are balanced break # Remove parenthesis num_close_parentheses -= 1 \u0026#34;\u0026#34;\u0026#34;After balancing parentheses we need to parse leading expression. It is always here because we checked single alias case separately.\u0026#34;\u0026#34;\u0026#34; for j in range(i, 0, -1): alias_expr.append(line[j]) if line[j - 1] == \u0026#34;[\u0026#34;: break if line[j - 1] == \u0026#34; \u0026#34;: break return \u0026#34;\u0026#34;.join(alias_expr[::-1]) It may look like magic, so let\u0026#39;s check how it works on examples from our real plan representation:\n_extract_alias_expressions( \u0026#34;id1_and_id2#2437\u0026#34;, \u0026#34;Project [id1#1321, id2#1322, id4#1323, id5#1324, v2#1325, id1_renamed#2430, concat_ws(_, id1#1321, id2#1322) AS id1_and_id2#2437]\u0026#34; ) And the result is:\n\u0026#39;concat_ws((_, id1#1321, id2#1322)\u0026#39; Looks like it works! Finally some of the knowledge from the Leetcode tasks was put into practice!\nParsing aggregation-like expressions In most cases we do not need additional columns from the row of the plan, except for one that we are working with. The only exception is aggregation: it might be good to store information about aggregation keys in our final node attributes. Let\u0026#39;s add a simple function to do this:\ndef _add_aggr_or_not(expr: str, line: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;If the expr is aggregation we should add agg keys to the beginning.\u0026#34;\u0026#34;\u0026#34; # We are checking for aggregation pattern match_ = re.match(r\u0026#34;^[\\s\\+\\-:]*Aggregate\\s\\[([\\w\\d#,\\s]+)\\].*$\u0026#34;, line) if match_: agg_expr = match_.groups()[0] return ( \u0026#34;GroupBy: \u0026#34; + re.sub(r\u0026#34;([\\w\\d]+)#([\\w\\d]+)\u0026#34;, r\u0026#34;\\1\u0026#34;, agg_expr) + f\u0026#34;\\n{expr}\u0026#34; ) # If not just return an original expr return expr Building a final recursive parser Now we have everything we need. So let\u0026#39;s go through the logical plan line by line, adding nodes and attributes to our graph structure:\ndef _get_graph(lines: list[str], node: str): nodes = [] edges = [] node_attrs = {} for i, l in enumerate(lines): # noqa: E741 \u0026#34;\u0026#34;\u0026#34;Iteration over lines of logical plan.\u0026#34;\u0026#34;\u0026#34; # We should use hash of line + node as a key in the graph. # It is not enough to use only hash of line because the same line # may be related to multiple nodes! # A good example is reading the CSV that is represented by one line! h = hash(l + node) # If the current node is not root we need to store hash of previous node. prev_h = None if not nodes else nodes[-1] if node not in l: continue if f\u0026#34;AS {node}\u0026#34; in l: \u0026#34;\u0026#34;\u0026#34;It is a hard case, when a node is an alias to some expression.\u0026#34;\u0026#34;\u0026#34; aliases, expr = _get_aliases(node, l) # For visualization we need to transform from nodes to columns expr = re.sub(r\u0026#34;([\\w\\d]+)#([\\w\\d]+)\u0026#34;, r\u0026#34;\\1\u0026#34;, expr) # Append a new node nodes.append(h) # Append expr as an attribute of the node node_attrs[h] = _add_aggr_or_not(f\u0026#34;{expr} AS {_node2column(node)}\u0026#34;, l) if len(aliases) == 1: # It is the case of simple alis # Like col1#123 AS col2#321 # In this case we just replace an old node by new one. if prev_h: edges.append([h, prev_h]) node = aliases[0] else: # It is a case of complex expression. # Here we recursively go through all the nodes from expr. if prev_h: edges.append([h, prev_h]) for aa in aliases: # Get graph from sub-column sub_nodes, sub_edges, sub_attrs = _get_graph(lines[i:], aa) # Add everything to the current graph nodes.extend(sub_nodes) edges.extend(sub_edges) node_attrs = {**node_attrs, **sub_attrs} # Add connection between top subnode and node edges.append([sub_nodes[0], h]) return (nodes, edges, node_attrs) else: # Continue of the simple alias or expr case # In the future that may be more cases, that is the reason of nested if instead of elif if \u0026#34;Relation\u0026#34; in l: nodes.append(h) if prev_h: edges.append([h, prev_h]) # It is a pattern, related to data-sources (like CSV) match_ = re.match(r\u0026#34;[\\s\\+\\-:]*Relation\\s\\[.*\\]\\s(\\w+)\u0026#34;, l) if match_: s_ = \u0026#34;Read from {}: {}\u0026#34; # Add data-source as a node node_attrs[h] = s_.format(match_.groups()[0], _node2column(node)) else: # We need it to avoid empty graphs and related runtime exceptions print(l) node_attrs[h] = f\u0026#34;Relation to: {_node2column(node)}\u0026#34; elif \u0026#34;Join\u0026#34; in l: nodes.append(h) if prev_h: edges.append([h, prev_h]) match_ = re.match(r\u0026#34;[\\s\\+\\-:]*Join\\s(\\w+),\\s\\((.*)\\)\u0026#34;, l) if match_: join_type = match_.groups()[0] join_expr = match_.groups()[1] join_expr_clr = re.sub(r\u0026#34;([\\w\\d]+)#([\\w\\d]+)\u0026#34;, r\u0026#34;\\1\u0026#34;, join_expr) node_attrs[h] = f\u0026#34;{join_type}: {join_expr_clr}\u0026#34; else: continue if not nodes: # Just the case of empty return. We need to avoid it. # I\u0026#39;m not sure that line is reachable. nodes.append(h) node_attrs[h] = f\u0026#34;Select: {_node2column(node)}\u0026#34; return (nodes, edges, node_attrs) All together Now we are ready to put all the pieces together into a single function:\ndef get_column_lineage(df: DataFrame, column: str) -\u0026gt; ColumnLineageGraph: \u0026#34;\u0026#34;\u0026#34;Get data lineage on the level of the given column. Currently Union operation is not supported! API is unstable, no guarantee that custom spark operations or connectors won\u0026#39;t break it! :param df: DataFrame :param column: column :returns: Struct with nodes, edges and attributes \u0026#34;\u0026#34;\u0026#34; lines = get_plan_from_df(df, PlanType.ANALYZED_LOGICAL_PLAN).split(\u0026#34;\\n\u0026#34;) # Top line should contain plan-id of our column. We need it. # Regular pattern of node is column#12345L or [\\w\\d]+#[\\w\\d]+ match_ = re.match(r\u0026#34;.*(\u0026#34; + column + r\u0026#34;#[\\w\\d]+).*\u0026#34;, lines[0]) if match_: node = match_.groups()[0] else: err = f\u0026#34;There is no column {column} in the final schema of DF!\u0026#34; raise KeyError(err) nodes, edges, attrs = _get_graph(lines, node) return ColumnLineageGraph(nodes, edges, attrs) Testing and drawing our implementation Let\u0026#39;s see how our function works:\nget_column_lineage(dat_new, \u0026#34;cnt_ids\u0026#34;) Will produce the following:\nColumnLineageGraph(nodes=[-3047688324833821294, 8934572903754805890, -22248459158511064, -3092611391038289840, 1490298382268190732, -6431655222193019101, -1002279244933706460], edges=[[8934572903754805890, -3047688324833821294], [-22248459158511064, 8934572903754805890], [1490298382268190732, -3092611391038289840], [-6431655222193019101, 1490298382268190732], [-1002279244933706460, 1490298382268190732], [-3092611391038289840, 8934572903754805890]], node_attrs={-3047688324833821294: \u0026#39;GroupBy: id1_renamed\\ncount((distinct hash_of_two_ids) AS cnt_ids\u0026#39;, 8934572903754805890: \u0026#39;concat_ws((_, id4, cast(hash_id as string)) AS hash_of_two_ids\u0026#39;, -22248459158511064: \u0026#39;Read from csv: id4\u0026#39;, -3092611391038289840: \u0026#39;hash((id1_and_id2, 42) AS hash_id\u0026#39;, 1490298382268190732: \u0026#39;concat_ws((_, id1, id2) AS id1_and_id2\u0026#39;, -6431655222193019101: \u0026#39;Read from csv: id1\u0026#39;, -1002279244933706460: \u0026#39;Read from csv: id2\u0026#39;}) Looks like it works, at least in our simple case.\nDrawing the graph To draw the graph as a tree, let\u0026#39;s use the Python library NetworkX. And GraphViz as the drawing engine. You need to install the following packages to use it:\nnetworkx pygraphviz matplotlib def plot_column_lineage_graph( df: DataFrame, column: str, ) -\u0026gt; \u0026#34;matplotlib.pyplot.Figure\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Plot the column lineage graph as matplotlib figure. :param df: DataFrame :param column: column :returns: matplotlib.pyplot.Figure \u0026#34;\u0026#34;\u0026#34; try: import networkx as nx from networkx.drawing.nx_agraph import graphviz_layout except ModuleNotFoundError as e: err = \u0026#34;NetworkX is not installed. Try `pip install networkx`. \u0026#34; err += ( \u0026#34;You may use `get_column_lineage` instead, that doesn\u0026#39;t require NetworkX.\u0026#34; ) raise ModuleNotFoundError(err) from e try: import matplotlib.pyplot as plt except ModuleNotFoundError as e: err = \u0026#34;You need matplotlib installed to draw the Graph\u0026#34; raise ModuleNotFoundError(err) from e import importlib if not importlib.util.find_spec(\u0026#34;pygraphviz\u0026#34;): err = \u0026#34;You need to have pygraphviz installed to draw the Graph\u0026#34; raise ModuleNotFoundError(err) lineage = get_column_lineage(df, column) g = nx.DiGraph() g.add_nodes_from(lineage.nodes) g.add_edges_from(lineage.edges) pos = graphviz_layout(g, prog=\u0026#34;twopi\u0026#34;) pos_attrs = {} for node, coords in pos.items(): pos_attrs[node] = (coords[0], coords[1] + 10) nx.draw(g, pos=pos) nx.draw_networkx_labels(g, labels=lineage.node_attrs, pos=pos_attrs, clip_on=False) return plt.gcf() If we run it, we get the following:\nimport matplotlib.pyplot as plt col = \u0026#34;cnt_ids\u0026#34; f = plot_column_lineage_graph(dat_new, col) f.show() Looks exactly as what we need!\nAfterwards This functionality is mostly for educational purposes, to better understand how Spark Plan is organized. Another possible use case is if you need some simple inline Python code for this task. For real production data lineage on top of Spark, I recommend using a Spline Project!\n","permalink":"http://localhost:1313/ssinchenko/post/pyspark-column-lineage/","summary":"In this post, I will show you how to use information from the spark plan to track data lineage at the column level. This approach will also works with recently introduced SparkConnect.","title":"PySpark column lineage"},{"content":" How to estimate a PySpark DataFrame size? Sometimes it is an important question, how much memory does our DataFrame use? And there is no easy answer if you are working with PySpark. You can try to collect the data sample and run local memory profiler. You can estimate the size of the data in the source (for example, in parquet file). But we will go another way and try to analyze the logical plan of Spark from PySpark. In case when we are working with Scala Spark API we are able to work with resolved or unresolved logical plans and physical plan via a special API. But from PySpark API only string representation is available and we will work with it.\nGetting Logical Plan There are two ways to get the logical plan: the first one is via SQL command EXPLAIN and the second one is via df.explain. We will use the second one just to skip the process of creating a temporary view from a DataFrame. df.explain does not provide and API to get a string representation of the plan, it is sent to standard output instead. We need to redirect standard output to get the plan:\nimport contextlib import io with contextlib.redirect_stdout(io.StringIO()) as stdout: df.explain(mode=\u0026#34;cost\u0026#34;) logical_plan = stdout.getvalue().split(\u0026#34;\\n\u0026#34;) Here we used the argument mode=\u0026#34;extended\u0026#34;. Based on documentation, this argument means:\ncost: Print a logical plan and statistics if they are available.\nSo, we will get all the available statistics, include the estimated size of the DataFrame that we are looking for.\nLet\u0026#39;s see what the plan looks like. But first we need some data frames of different sizes for our experiments. I was working on the project to rewrite h2o db benchamrk generation from R to Rust (it is now part of the farsante repository) and had some CSV benchmark datasets of different sizes, from a few Kb to a few Gb. I can use them for our experiments:\nfrom pyspark.sql import SparkSession spark = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).getOrCreate() medium_data = spark.read.csv(\u0026#34;/home/sem/github/farsante/h2o-data-rust/G1_1e8_1e8_10_5.csv\u0026#34;) small_data = spark.read.csv(\u0026#34;/home/sem/github/farsante/h2o-data-rust/J1_1e8_1e5_5.csv\u0026#34;) tiny_data = spark.read.csv(\u0026#34;/home/sem/github/farsante/h2o-data-rust/J1_1e8_1e2_5.csv\u0026#34;) And the output logical plan looks like this:\nmedium_data.explain(mode=\u0026#34;cost\u0026#34;) == Optimized Logical Plan == Relation [_c0#17,_c1#18,_c.....,_c7#24,_c8#25] csv, Statistics(sizeInBytes=4.5 GiB) == Physical Plan == FileScan csv [_c0#17,_c1#18,_c......,_c7#24,_c8#25] Batched: false, DataFilters: [], Format: CSV, Loc... As you can see, the information we are looking for is in the first (or top) row. And it will always be there, because the logical plan is \u0026#34;reversed\u0026#34; and goes from the last operation to the first one, line by line. So the top line of the logical plan will always be a line representing the current state of our DataFrame.\nParsing the top line of the logical plan As you already understand, we are looking for the number from this line: Statistics(sizeInBytes=4.5 GiB). Let\u0026#39;s use built-in Python regexps to extract this information:\nimport re pattern = r\u0026#34;^.*sizeInBytes=([0-9]+\\.[0-9]+)\\s(B|KiB|MiB|GiB|TiB|EiB).*$\u0026#34; Let\u0026#39;s see on the pattern. It says:\n^.*: any amount of symbols in the beginning of the row. sizeInBytes=([0-9]+\\.[0-9]+): the number if the form of 1234.1234 exactly after the word sizeInBytes and equal sign. We create a regex-group from this number. \\s(B|KiB|MiB|GiB|TiB|EiB|): our second group which follows the first one after exactly one space symbol. .*$: any amount of symbols in the end of the row. with contextlib.redirect_stdout(io.StringIO()) as stdout: medium_data.explain(mode=\u0026#34;cost\u0026#34;) plan = stdout.getvalue() top_line = plan.split(\u0026#34;\\n\u0026#34;)[1] re.match(pattern, top_line).groups() Result:\n(\u0026#39;4.5\u0026#39;, \u0026#39;GiB\u0026#39;) Looks like it works!\nCorner case: what happens if Spark doesn\u0026#39;t know the size? Before we finalize our code in the Python function, let\u0026#39;s check what happens if Spark doesn\u0026#39;t know the size of the data. This is the common case for DataFrame objects that are created from memory, not from disk.\ndata = [(i, f\u0026#34;id{i}\u0026#34;, f\u0026#34;id2{i}\u0026#34;, f\u0026#34;id3{i}\u0026#34;) for i in range(1_100_000)] sdf = spark.createDataFrame( data, schema=\u0026#34;struct\u0026lt;c1:int,c2:string,c3:string,c4:string\u0026gt;\u0026#34; ).withColumn(\u0026#34;new_col\u0026#34;, F.col(\u0026#34;c1\u0026#34;) * 4) with contextlib.redirect_stdout(io.StringIO()) as stdout: sdf.explain(mode=\u0026#34;cost\u0026#34;) plan = stdout.getvalue() top_line = plan.split(\u0026#34;\\n\u0026#34;)[1] re.match(pattern, top_line).groups() Result:\n(\u0026#39;8.4\u0026#39;, \u0026#39;EiB\u0026#39;) This is not what we expected, is it? An EiB is something like \\(\\simeq 10^6\\) TiB… The answer is simple: if spark cannot estimate the size, it simply returns the maximum available value (Scala Long.MaxValue). You might say this is a bug, but after reading this discussion I understood that there is no easy way to work around it on the side of Spark. So let\u0026#39;s just catch this case on the Python side. Unfortunately, our final code with a workaround won\u0026#39;t work if your data is really EiB in size, but I can\u0026#39;t imagine such an amount in a single Spark Job.\nFinalized code import contextlib import io from pyspark.sql import DataFrame def _bytes2mb(bb: float) -\u0026gt; float: return bb / 1024 / 1024 def estimate_size_of_df(df: DataFrame, size_in_mb: bool = False) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;Estimate the size in Bytes of the given DataFrame. If the size cannot be estimated return -1.0. It is possible if we failed to parse plan or, most probably, it is the case when statistics is unavailable. There is a problem that currently in the case of missing statistics spark return 8 (or 12) EiB. If your data size is really measured in EiB this function cannot help you. See https://github.com/apache/spark/pull/31817 for details. Size is returned in Bytes! This function works only in PySpark 3.0.0 or higher! :param df: DataFrame :param size_in_mb: Convert output to Mb instead of B :returns: size in bytes (or Mb if size_in_mb) \u0026#34;\u0026#34;\u0026#34; with contextlib.redirect_stdout(io.StringIO()) as stdout: # mode argument was added in 3.0.0 df.explain(mode=\u0026#34;cost\u0026#34;) # Get top line of Optimized Logical Plan # The output of df.explain(mode=\u0026#34;cost\u0026#34;) starts from the following line: # == Optimized Logical Plan == # The next line after this should contain something like: # Statistics(sizeInBytes=3.0 MiB) (untis may be different) top_line = stdout.getvalue().split(\u0026#34;\\n\u0026#34;)[1] # We need a pattern to parse the real size and untis pattern = r\u0026#34;^.*sizeInBytes=([0-9]+\\.[0-9]+)\\s(B|KiB|MiB|GiB|TiB|EiB).*$\u0026#34; _match = re.search(pattern, top_line) if _match: size = float(_match.groups()[0]) units = _match.groups()[1] else: return -1 if units == \u0026#34;KiB\u0026#34;: size *= 1024 if units == \u0026#34;MiB\u0026#34;: size *= 1024 * 1024 if units == \u0026#34;GiB\u0026#34;: size *= 1024 * 1024 * 1024 if units == \u0026#34;TiB\u0026#34;: size *= 1024 * 1024 * 1024 * 1024 if units == \u0026#34;EiB\u0026#34;: # Most probably it is the case when Statistics is unavailable # In this case spark just returns max possible value # See https://github.com/apache/spark/pull/31817 for details size = -1 if size \u0026lt; 0: return size if size_in_mb: return _bytes2mb(size) # size in Mb return size # size in bytes Testing print(estimate_size_of_df(medium_data, size_in_mb=False)) print(estimate_size_of_df(medium_data, size_in_mb=True)) print(estimate_size_of_df(small_data, size_in_mb=True)) print(estimate_size_of_df(tiny_data, size_in_mb=False)) Result:\n4831838208.0 4608.0 3.0 1691.0 ","permalink":"http://localhost:1313/ssinchenko/post/estimation-spark-df-size/","summary":"Sometimes it is an important question, how much memory does our DataFrame use? And there is no easy answer if you are working with PySpark. You can try to collect the data sample and run local memory profiler. You can estimate the size of the data in the source (for example, in parquet file). But we will go another way and try to analyze the logical plan of Spark from PySpark. In case when we are working with Scala Spark API we are able to work with resolved or unresolved logical plans and physical plan via a special API. But from PySpark API only string representation is available and we will work with it.","title":"How to estimate a PySpark DF size?"},{"content":" Cycling Eastern Serbia I would like to tell you about my bicycle trip through Eastern Serbia. This part of the world is beautiful, but there is a big problem with lack of information in English. So I will try to fill this gap.\nThe route I will describe starts in Belgrade, goes along the Danube River, through Djerdap National Park to the border with Serbia, and returns to Belgrade through Kucaj-Beljanica National Park.\nSummary Total length: about 700 km; Total elevation: about 5000 m; Highest climbing category: 2; Highest point: 960 m above the sea; Estimated days required: 6-7; Overall level: medium; Recommended lowest gear: 1:1 Recommended tires: 35+ mm or MTB; Link to the routeplanner; Link to Kamoot; GPX file; Total price, including hotels and food in restaurants: 385 Eur⋆; ⋆Autumn, 2023\nAll the pictures were made by myself and are placed under Creative Common License, so you are free to reuse any of these pictures.\nThe First Day Belgrade - Ram\n119 km 200 m elevation 6 hours The first day of my route was the way from Belgrade to the city of Ram - a small town on the Danube River. This day I cycled about 120 km on the EuroVel No. 6 mostly. It is the easiest and, I think, the most boring part of the whole route. You only have to go about 100 km away from Belgrade to see the beauty. The route is flat and mostly goes through unpaved roads where you cannot face a car.\nIf you start the way early in the morning like I did you will be able to see a sunrise on the Danube from the bridge:\nAfter that you will turn off the busy road onto a dirt road which is the part of EuroVel. The next 20-30 km will be like this. Quiet and relaxing.\nThe first small town on the way is Starcevo, where you can rest, drink coffee, refill your water packs and supplies:\nThe biggest city on your way is the city of Kovin, about 40 km far away from Belgrade. After that you will turn away from asphalt road to unpaved part of the EuroVel. This way will go almost until the city of Bela Crkva:\nThe road is quiet but can be a little challenging as the surface is partly sand. This part of the route is one of the reasons why I recommend having at least 35 mm tires.\nThe end of the day will be in the towns of Bela Crkva and Banatska Palanka. Make sure you check the ferry schedule! If you have some time before the ferry, I strongly recommend you to visit Bela Crkva camp and nearby lakes:\nAfter that the last 10 km are the way to the ferry crossing. The ferry station is right next to the restaurant, so you can eat and rest here. The prices are not so high and the view from the restaurant is nice. I caught the beginning of the sunset on the beach, waiting for the ferry:\nAnd the end of the day for me was the very beautiful ferry ride from one side of the Danube to the other. I was able to see a beautiful sunset and a perfect view of the ancient RamFortress:\nThere are only a few hotels near the ferry station. I choose Rajski Horizonti and can recommend it because there is a place inside the hotel where you can leave the bike. The restaurant is also nearby, in the same building. Try the fish soup made from the local fish - it is really delicious, cheap and the portion is huge (one portion is about two plates).\nThe Second Day Ram - Donji Milanovac\n95 km 460 m elevation 5 hours 15 minutes In the morning of the second day, I faced the light rain. It was cold and wet, but I had hope that the forecast was true and the weather would get better.\nI had a coffee break in the restaurant near the Silver Lake (Srebrno jezero in Serbian) - a well organized resort with cafes, shops, beaches and hotels. I spent about an hour waiting for the weather to improve before I left, but it did not. Finally at about 12 pm I decided to continue my way even under the rain.\nFinally, after another 20 km of cycling, I could see the beginning of the Djerdap Canyon and the Golubac Fortress in the distance. As far as I know, the fortress was built to control the entrance to Djerdap Canyon. To be honest, I was so happy to finally see mountains that any signs of fatigue immediately flew away!\nThe Golubac fortress itself is a very brutal building that is worth seeing! I heard that it was renovated not so long ago. Anyway, it is in good condition now.\nBut the whole road along the Djerdap Canyon is also very beautiful. I cycled very slowly just because I wanted to see as much beauty as possible!\nYou may see tunnels on some photos, but do not be afraid of them. There are only two relatively long tunnels in Djerdap Canyon (about 350 meters), but they are closer to the end of the canyon and there is a special button for cyclists. By pressing the button, you activate a special sign at the entrance of the tunnel, which means something like \u0026#34;Attention! Cyclists in the tunnel, speed 30 km/h\u0026#34;. As it is part of EuroVel, there are also signs saying \u0026#34;Attention! Cyclists\u0026#34; everywhere. So do not worry so much about the fact that you are cycling with cars and just relax and see the beauty around you!\nThe day ended with crossing a bridge with a beautiful view and a final climb.\nIt was the first serious climb on the way, something between 3d and 4th category by Strava Categorization, but nothing impossible. Only about 200 meters of elevation gain and the average grade is not that high. You have to get used to such climbs, they will follow you the next days. After the climb there is a downhill almost to the final destination of the day - the town of Donji Milanovac.\nDonji Milanovac is the town on the mountain and I strongly recommend you to book a room in the town center. If not, you risk to follow my mistake when I booked a room in place on the top. To reach it I climbed about 150 meters of altitude with crazy grade 14-16%… I ate that day in the restaurant Lepenska Ribica and this place is definitely worth my recommendation! Very nice Pljeckavica is here, one of the best I have tasted in Serbia! There is a supermarket nearby where you can buy protein snacks for the next day if you need them. The souvenir shop is across the street from the supermarket.\nThe Third Day Donji Milanovac - Brza Palanka\n90 km 500 m elevation 5 hours 10 minutes The next day I benefited from the apartments on the top of the mountain part of Donji Milanovac, because I was able to photograph the town exactly as on the freezer magnets you can buy in the souvenir shop:\nWhen I planned this route, I forgot one important thing about the weather: the Serbian side of the Djerdap Canyon is in the shadow of the mountains until about noon:\nIt was the coldest morning of the whole trip. I almost prayed for the climb to warm up a bit. And my prayers were answered: there are some climbs, one of them in the 3d category. There are also few tunnels, but as I mentioned before, there is no need to worry about them.\nThe most famous place of the Djerdap Canyon is the face of Decebalus placed on the rock on the Romanian side of the Danube. Decebalus was an ancient king who fought successfully against the Roman Empire. Today he is a national hero of Romania. The monument is really impressive! That was the first point of my trip where I finally understood that everything that happened or would happen was worth the moment I saw Decebalus!\nBut it doesn\u0026#39;t mean, that other parts of Djerdap are boring. Of course not! There are some photos I made during the raod:\nBut nothing is endless, and after the last climb I left the Djerdap and found myself in the small town Tekija, where I stopped for a coffee and something to eat. There is a big restaurant Panorama, from the second floor you have a nice view:\n20 km from Tekija is the Djerdap hydroelectric power plant and the bridge to Romania. Be prepared for a very long traffic jam formed by the tracks that turn the two-lane road into a single lane. Do not miss the Diana Fortress about 200 meters from the border crossing. It is an ancient remnant of the fort that was built in the end of the 100 year A.D. And you are free to go inside and have a walk, no need to buy a ticket:\nThe last major checkpoint on the way is the town of Kladovo, a medium-sized town. There is a fortress at the entrance of the city and you won\u0026#39;t miss it, just look at the signs along the road. But oiverall city is very cozy and nice too.\nI ended the day in Brza Palanka, a small town famous for it\u0026#39;s beach. I stopped at a campsite Mirocka Voda, they have not only places for tents but also small houses and a shower. I cannot say that the place is very comfortable, but there is a bed under the roof, a shower and a restaurant if you are able to have dinner. What else do we need on a trip?\nDo not drink a lot of beer in the restaurant, because the next day will be long and tougher than this one! Better to have a good rest and maintain your bike.\nThe Fourth Day Brza Palanka - Zajecar\n135 km 1300 m elevation 8 hours 30 minutes The fourth day was supposed to be easy and relaxing, but I decided to set myself a challenge. About 10 km away from the campsite I saw a sign \u0026#34;Vratna Stone Gates, Nuatural Monument\u0026#34; on the road. Quick Googling gave me a lot of beautiful photos and the knowledge that this thing is really rare not only in Serbia but in the whole world. There are only few such places on the planet and I made a decision to change my route. Vratna Stone Gates are natural stone bridges, a really unique thing. They are located near the Vratna monastery, about 15 km away from the main road. So, visiting Vratna cost me 33 additional km and about 600 meters of altitude. Do not follow my mistake and try to leave Brza Palanka as early as possible, otherwise you risk to finish the road to Zajecar in the dark like me.\nThe Vratna monastery itself is a very cozy orthodox monastery surrounded by rocks:\nTo see the stone gates, you have to leave your bike in the monastery yard, but the monks are okay with it, as far as I understood. The way to the gates is a hiking trail of about 1 km, marked with red circles, so you won\u0026#39;t get lost in the forest. And finally you will see this miracle of nature:\nOf course it is worth to visit this place, but after returning to the road I found myself 85 km away from the destination and the time was already about 12 pm. So I spent the rest of the day trying to reach the town of Zajecar before it got dark. That is the reason why I do not have many photos of this day, except the one I took in Vratna.\nThe next big town on the way was Negotin, where I had lunch. Unfortunately I did not have time to look around, I just visited the main square and rode on.\nThe road to Negotin is very quiet, but has a lot of climbs, one of them in the 3d category and others closer to the 4th. There are not many cars and the road goes through small villages and fields.\nYou have to follow the signs of EuroVel that will be on the way and that will tell you the remaining distance. After the last big climb you will be able to see the silhouette of Kucaj mountain in the distance. This is your goal for tomorrow, but for today you just need to finish the climb and after that there will be a long nice downhill almost to the town of Zajecar.\nI made a stop in Zajecar in the apartments near the theater. It is called Teatar Apartment, there is an option to put your bike in the room. There is also a bathroom and a washing machine, so you can finally clean your clothes. You should also try the Zajecarsko beer here, just because this town is the home of the brand. I can recommend this pub, they give you not only nice beer, but also the food is very delicious!\nThis was one of the hardest days for me in terms of total altitude and length. My mistake was to leave Brza Palanka so late that I had to ride the last descent in the dark. Do not make the same mistake!\nThe Fifth Day Zajecar - Lisine\n90 km 1400 m elevation 8 hours The day began by visiting a Felix Romuliana, or Gamzigrad. It is the very interesting place, remains of the Roman imperial palace of the III century A.D. It is placed about 10 km far from the Zajecar city. You can buy a ticket, in 2023 the price was 500 dinars (∼ 4.5 Eur).\nThe next point will be the small village Gamzigradska Banja:\nAfter a short climb you are on the road. It is quite busy, but you only have to ride about 15 km here. There is a tunnel on the way and it is quite long, about 500 meters. I strongly advise you not to go inside just because of the traffic. It is safer to go around, there is a dirt road on top, I marked this place in GPX.\nNext you will follow signs to the city of Sumrakovac and next to fields. This part of the way is very relaxing:\nYour next middlepoint is the town of Zlot:\nMake sure you have rested enough here and fill your water packs and snacks. The next part of the day will be a very long climb on the gravel road and the same downhill. This route has lack of civilization, you won\u0026#39;t be able to fill water packs except from natural springs. And there won\u0026#39;t be any places where you can eat or buy food until the destionation. Also almost the whole way there will be a problem with cell phone connection. Long story short, you are going into the wilderness!\nI suggest to save for the last part of the way about 5 hours at least. If you will have enough time in reserve by the moment, you may visit Lazar\u0026#39;s Cave (or Lazareva Pecina in Serbian). It is about 500 meters away from your road.\nAfter that you start the hardest climb of the whole route: the road along Lazar\u0026#39;s Canyon. It is a second category climb, but since the road is hard gravel, I would say it is closer to the first category by the amount of time and effort needed to pass it. The only positive thing here is that there are no cars. When I passed it I saw only one car for 5 hours. It starts with a part of about 4 km with a very high gradient up to 14-15% and after that a long part of about 7 km with an average gradient of about 6-7%.\nOn the top of the first part of the way you will see Lazar\u0026#39;s Canyon from the best possible viewpoint. Believe me, all your pain and suffering will be worth the only moment when you will be 650 meters above the sea, looking at this fantastic wonder of nature…\nIt was then that I finally understood that I was off schedule. The sun was setting, 300 meters of climbing and a long downhill awaited me. The road is so hard that I could not go faster than 10-13 km/h even on the downhill. In addition, I realized that there are no cars, no people and no mobile phone connection. It was a really scary moment for me and I didn\u0026#39;t take many pictures for the rest of the day.\nI will say it again: do not underestimate this part of the route. Before the trip, I trained specifically for second category climbs to understand the time and effort required. But after four days and more than 400 km, there is a lot of cumulative fatigue. Also, as I mentioned before, the road is so hard that in some places with high gradients, my 35mm tires just slipped on small stones of the road. So on some parts I was forced to push the bike instead of riding. Bikepacking turned into bikehiking…\nWhen the sun finally went down, I just started the 20km downhill. It was really scary. Going down on hard gravel in the forest with a gradient of -14-16%, a lot of big stones or fallen trees on the way, small streams somewhere crossing the way and absolutely no people or any signs of civilization. I made it in about two hours. After that I decided that I had risked enough on this trip, I had so much cumulative fatigue that I was making one mistake after another in planning and I needed to change something. I decided to rest in Lisine for a whole day and then go to Belgrade by the shortest route.\nI made a stop in Izvor Lisine, a hotel and conamed restaurant. I can recommend this place, good balance of price and quality and also it is only 250 meters away from the main waterfall.\nThe Sixs Day 0 km 0 elevation 0 hours Lisine is a very nice place. An island of civilization in the middle of Kucaj-Beljanica National Park. It is named like the very big waterfall Lisine (or Veliki Buk that may be translated like Great Beech):\nIf you go to the waterfall early in the morning, you may see \u0026#34;locals\u0026#34;: small black squirells looking around the waterfall for food left by the tourists. I\u0026#39;m sorry for the quality of the photos, it\u0026#39;s not an easy task to photograph the fast moving squirell through the manual focus lens. Especially if it is your first time photographing animals!\nI spent the first half of the day walking around, taking pictures of the environment. And the environment is really beautiful! There are a lot of waterfalls, natural springs coming right out of the rocks, and nice views of the surrounding mountains. Also, I found a lot of signs to the hiking trails, some of them long and hard, but some not so, like 3-5 km. Because of fatigue and the goal of the rest I decided to skip them this time, but you can make a try!\nIt may sound crazy coming from the person who decided to do a solo bike trip, but in that moment it was so cool just to sit with the understanding that you don\u0026#39;t have to ride anywhere that day!\nI also had time to finally do some maintenance on my bike after the hard gravel roads. Here\u0026#39;s what my cassette looked like, covered in sand and dust:\nI met the sunset well rested and in a good mood. I made a decision that I will try to cover 175 km to Belgrade without stopping. But of course I also prepared a backup plan that I could spend one night in Pozarevac city which is about 90 km far from me.\nThe last Day 175 km 1130 m elevation 9 hours 50 minutes In the morning I said goodbye to the mountains and started my way. Due to the necessity to cover 175 km I did not make many stops to take photos. But one place in Kucaj-Beljanica I want to show you. It is a very beautiful monument in the form of a stone flower, placed in the middle of nowhere. Do not miss it!\nThe road is mostly downhill, there is only one 4th category climb until you reach the town of Decpotovac, where I had coffee and cake. Unfortunately I didn\u0026#39;t have time to look around the town, I just saw an orthodox church and some Yugoslavian modernist architecture on the way.\nAt this point of the way mountains will be left behind and the road is going be boring a little. Endless fields, nothing special to be honest. The only place I found interesting is the town of Svilajnac.\nThe weather was getting worse and worse, strong wind from the front and the forecast promised rain in the Belgrade area after noon. Nearer to Pozarevac I started to feel fatigue, but I had understanding that I do not need to go tomorrow and decided to reach the goal. From Pozarevac you just have to follow the signs of another EuroVel route in the direction of Smederevo. Smederevo is an interesting place with an old fortress. But I didn\u0026#39;t have enough time for it and I already visited this town during one of my vacation bike trips.\nThe route from Smederevo to Belgrade includes three climbs, two of which are categorical (4th category). Keep it in mind and maybe make a stop in the city if you feel tired by this moment. Also Smderevski Put, as the road is called, is quite busy road, so be sure that your rear lights have enough power left. Belgrade hit me with a hard rain and I spent about the whole hour for the last 8 km of the way in the city. But it was a nice challenge as the cherry on the top, I think I closed the season worthy!\nConclusion I cannot say that this route is very difficult. There is nothing special on the way and only one really wild place. You will have enough food and water along the way. At the same time, such a trip is a nice way to get to know Serbia, to see small villages and big cities of this country. And of course, mountains! Djerdap canyon is fantastic, Lisine is beautiful and I have no words to describe my feelings when I stayed on the viewpoint of Lazar\u0026#39;s canyon! So, I hope my post can help you with planning and good luck on your way!\nAfterwords Prices and information are current as of Fall 2023. If you have any kind of update, please contact me via email ssinchenko@pm.me, or if you have a GitHub account, you can open an issue in the repository where my blog is hosted.\n","permalink":"http://localhost:1313/ssinchenko/post/cycling-eastern-serbia/","summary":"I would like to tell you about my bicycle trip through Eastern Serbia. This part of the world is beautiful, but there is a big problem with lack of information in English. So I will try to fill this gap. The route I will describe starts in Belgrade, goes along the Danube River, through Djerdap National Park to the border with Serbia, and returns to Belgrade through Kucaj-Beljanica National Park.","title":"Cycling Eastern Serbia"},{"content":" The problem Recently I decided to switch from Ubuntu to NixOS. Do not ask me why, it was just for fun mostly. One of the main ideas behind NixOS is to separation of dependencies: each new package is installed into separate sandbox with own scope of dependencies. By design it should make system significantly more stable but sometimes there are problems. One of such problems I faced with pyenv – a tool for simplifying python versions management.\nAdding pyenv to your NixOS configuration The simplest way to install pyenv in NixOS is to add the following lines to your /etc/nixos/configuration.nix:\n{ config, pkgs, ... }: let unstableTarball = fetchTarball \u0026#34;https://github.com/NixOS/nixpkgs/archive/nixos-unstable.tar.gz\u0026#34;; in { ... nixpkgs.config = { allowUnfree = true; packageOverrides = pkgs: with pkgs; { unstable = import unstableTarball { config = config.nixpkgs.config; }; }; }; environment.systemPackages = with pkgs; [ pkgs.gcc pkgs.gnumake pkgs.zlib pkgs.libffi pkgs.readline pkgs.bzip2 pkgs.openssl pkgs.ncurses unstable.pyenv ]; } Here we do the following:\nAdd a link to unstable channel of nix packages; Defina an additional source and name it unstable Add zlib, libffi, readline, bzip2, openssl, ncurses because we need them to build Python Add unstable.pyenv to install the latest available version of Pyenv Trying pyenv and getting an error… After sudo nixos-rebuild switch you will be able to try pyenv install python3.10 but most probably you immediatly gen an error like this one:\nzipimport.ZipImportError: can\u0026#39;t decompress data; zlib not available It may looks strange because we installed zlib but the root of this problem is in the main benefit of NixOS: zlib .h and .so files are installed into container and gcc cannot find them when trying to build Python.\nMagic that works for me Maybe it is not the right way but it worked for me. We need to spicefy additional libraries and search path for headers via environment variables. Because NixOS install libraries in paths that contains hashes there is no unified and constant place like /usr/lib that may be hardcoded. Because of this I added the following lines into my configuration.nix:\nenvironment.sessionVariables = { PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34;; # pyenv flags to be able to install Python CPPFLAGS=\u0026#34;-I${pkgs.zlib.dev}/include -I${pkgs.libffi.dev}/include -I${pkgs.readline.dev}/include -I${pkgs.bzip2.dev}/include -I${pkgs.openssl.dev}/include\u0026#34;; CXXFLAGS=\u0026#34;-I${pkgs.zlib.dev}/include -I${pkgs.libffi.dev}/include -I${pkgs.readline.dev}/include -I${pkgs.bzip2.dev}/include -I${pkgs.openssl.dev}/include\u0026#34;; CFLAGS=\u0026#34;-I${pkgs.openssl.dev}/include\u0026#34;; LDFLAGS=\u0026#34;-L${pkgs.zlib.out}/lib -L${pkgs.libffi.out}/lib -L${pkgs.readline.out}/lib -L${pkgs.bzip2.out}/lib -L${pkgs.openssl.out}/lib\u0026#34;; CONFIGURE_OPTS=\u0026#34;-with-openssl=${pkgs.openssl.dev}\u0026#34;; PYENV_VIRTUALENV_DISABLE_PROMPT=\u0026#34;1\u0026#34;; }; In this case we are spicifying all the paths and also directly point to OpenSSL installation. After that pyenv install start working correctly. I spent a lot of time trying to find a right way to specify this variables, so I hope my short post may safe this time for someone else!\n","permalink":"http://localhost:1313/ssinchenko/post/using-pyenv-with-nixos/","summary":"The problem Recently I decided to switch from Ubuntu to NixOS. Do not ask me why, it was just for fun mostly. One of the main ideas behind NixOS is to separation of dependencies: each new package is installed into separate sandbox with own scope of dependencies. By design it should make system significantly more stable but sometimes there are problems. One of such problems I faced with pyenv – a tool for simplifying python versions management.","title":"Using Pyenv with NixOS"},{"content":" Generating Python docstrings with GPT and Emacs Motivation There is an open source library in which I\u0026#39;m a maintainer. And recently I committed to creating docstrings for all the public functions and methods. I heard that recent Large Language Models (LLM) are good enough in the annotation of texts and documenting of code so I decided to try to use one of OpenAI models to solve this problem. In this post I will use Emacs plugins and extensions to generate docstrings but most advises about which prompt is better to use are generic and may be used with different code editors and IDE\u0026#39;s.\nSetup Eamcs and GPT If you are not a user of Emacs editor you can skip this block completely.\nSimilar plugins may be found for almost all the editors:\nVSCode Vim PyCharm Sublime Text Creating OpenAI account First, you must create an OpenAI account. You can do it on their website. You also need an API Token which you can generate in Personal -\u0026gt; Manage account -\u0026gt; API keys -\u0026gt; +Create new secret token.\nConfigure Eamcs There is already an Emacs OpenAI organization that provide a low-level library openai and also a high-level interface codegpt. To provide API keys to the library add the following lines to your Emacs config file:\n(require \u0026#39;openai) (setq openai-key \u0026#34;sk-*************************************\u0026#34;) (setq openai-user \u0026#34;user\u0026#34;) (setq openai--show-log t) So all that you need to do is to install these libraries.\nChoose a right prompt It may be important that by default codegpt library uses temperature=1.0. I tried also to use temperature=0.0 but it changed nothing.\nBy default codegpt library provide the function for generating documentation to any code:\n;;;###autoload (defun codegpt-doc (start end) \u0026#34;Automatically write documentation for your code. This command is interactive region only, the START and END are boundaries of that region in buffer.\u0026#34; (interactive \u0026#34;r\u0026#34;) (codegpt--internal \u0026#34;Please write the documentation for the following function.\u0026#34; start end)) But this function is designed to generate documentation for any language. The problem with Python is it has three popular formats for generating docstrings: Google, NumPy, and Sphinx (or ReStructuredText). And the library for which I was trying to generate docstrings used sphinx format. So I created my own function to do it:\n;; CodeGPT custom prompts (defun codegpt-pysphinx (start end) (interactive \u0026#34;r\u0026#34;) (codegpt--internal \u0026#34;Please write the reStructuredText documentation for the following Python function in reStructuredText format.\u0026#34; start end)) It is highly recommended to add as much as possible mentions of desired docstrings format. As you will see later even doubled directives to provide reStructuredText style docstrings are not enough and sometimes GPT returns something strange…\nDocstring for a simple function Let\u0026#39;s try our approach on a very simple one-line function using text-davinchi-003 LLM:\nPlease write the reStructuredText documentation for the following Python function as a docstring in sphinx format. def column_to_list(df: DataFrame, col_name: str) -\u0026gt; List[Any]: return [x[col_name] for x in df.select(col_name).collect()] \u0026#34;\u0026#34;\u0026#34; column_to_list This functions takes a DataFrame and a column name as argument and returns a list of elements corresponding to the values in the column. :param df: A DataFrame. :type df: `DataFrame` :param col_name: Name of the column to be processed. :type col_name: `str` :return: List of elements. :rtype: `List[Any]` .. note:: This is an example of reStructuredText (reST) syntax. \u0026#34;\u0026#34;\u0026#34; Here we have a good docstring but with a couple of problems:\nadditional line on the top; a very-very strange note which is also not supported by MKDocs generator; typo: functionS instead of function. But it can be easily done by just copying only required part of the text and fixing typos. The most important is that LLM recognize the context right and generate an exact description what this function is really do.\nDocstring for a big function For big complex functions, it works even worse. The first couple of tries returns me a picture like this one:\nExample of return in the wrong format So even two explicit directives to use reStructuredText and sphinx are not enough sometimes.\nAfter some retries with absolutely the same query I finally got what I desired:\n\u0026#34;\u0026#34;\u0026#34; :param df: A :class:`~pyspark.sql.DataFrame` object to validate the schema against :param required_schema: A :class:`~pyspark.sql.types.StructType` expected schema to validate :type required_schema: StructType :param ignore_nullable: A boolean to determine if the nullable property should be ignored when validating :type ignore_nullable: bool, optional :returns: None Validate a DataFrame against required schema structure. This function will copy both the current DataFrame schema and the required schema structure and compare if the DataFrame schema contains all the expected fields of the required schema structure. If one of the fields is not found, a DataFrameMissingStructFieldError will be raised. If ignore_nullable is set to True, the function will ignor the nullable property of the two schemas for validation. \u0026#34;\u0026#34;\u0026#34; There are some problems like a wrong order of parameters and description blocks or using of :class: tag which is not supported by MKDocs. But all of these can be easily fixed.\nSome funny things Sometimes LLM started to dream out of the blue. For example once it returns me docstrings with notes about changes in interface. The most interesting thing is that such a changelog looked very believable, despite being fictional from start to finish!\nA fictional changelog example, generated by LLM Conclusion LLM looks very promising in such a common task like as generating documentation for the existing code. But even simple docs requires a careful control from the human side. Anyway I will use it in my projects because it is significantly better than write documentation from scratch by yourself. I do not think that LLV models can replace software developers in the nearest feature but I think that they will change the whole process of development and will become as much an integral part of development as, for example, IDEs have become.\nP.S. I spend about 0.20 USD on LLM calls and about 30 minutes to setup all the configuration and generate all the docstrings. I\u0026#39;m absolutely sure that manual generation of docs would require significantly bigger amount of time!\n","permalink":"http://localhost:1313/ssinchenko/post/generating-docs-with-gpt/","summary":"Generating Python docstrings with GPT and Emacs Motivation There is an open source library in which I\u0026#39;m a maintainer. And recently I committed to creating docstrings for all the public functions and methods. I heard that recent Large Language Models (LLM) are good enough in the annotation of texts and documenting of code so I decided to try to use one of OpenAI models to solve this problem. In this post I will use Emacs plugins and extensions to generate docstrings but most advises about which prompt is better to use are generic and may be used with different code editors and IDE\u0026#39;s.","title":"Generating docstrings with GPT"},{"content":" Working with File System from PySpark Motivation Any of us is working with File System in our work. Almost every pipeline or application has some kind of file-based configuration. Typically json or yaml files are used. Also for data pipelines, it is sometimes important to be able to write results or state them in a human-readable format. Or serialize some artifacts, like matplotlib plot, into bytes and write them to the disk.\nBut PySpark applications are running in cluster mode, especially in a production environment. And all that we have is some distributed file system or object storage like HDFS, S3, Azure Blob Storage, etc.\nRegular approach An obvious solution is of course to use some side library. For example, we can use boto3 for working with S3, pyarrow for working with HDFS, or built-in Pathlib for Local One. But there are some problems:\nSometimes it is a bad idea to take a huge library and add it to the project as a dependency especially if all that we need is just read or write some bytes from/to storage; All of these libraries has own abstractions and interfaces. So each user should learn one more API; Sometimes we need to be able to write into Local Files System when running unit tests but into some cloud storage from production. Of course one can use unittest.mock.patch (or pytest fixtures) but it can make writing tests not a trivial task. At the same moment, we know that PySpark can read and write data quite effectively into any file system. Moreover, spark understands which system is it by path prefix. For example in this code we shouldn\u0026#39;t specify the file system, all we need is just write the right prefix:\nspark_data_frame.write.parquet(\u0026#34;s3a://my-prefix/table\u0026#34;) # this will write to S3 bucket spark_data_frame.write.parquet(\u0026#34;file://my-home-dir/table\u0026#34;) # and this one will save data locally So why not use such built-in `PySpark` features?\nJava way Spark is written in Scala, a language from the JVM family. And under the hood Spark steel heavily uses org.apache.hadoop so this jar is accessible out-of-the-box in almost each Spark setup. We can make a look into the documentation of org.apache.hadoop.fs.FileSystem: a main class for making i/o operations. There are implementations for S3, HDFS, Local and Azure file storage. So we can use a single interface and all the advantages of Java classes hierarchy and do not care about which implementation to use where. Imagine we have a SparkSession in some Java program. In this case, we can write code like this:\nimport org.apache.spark.sql.SparkSession; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; public class Main { public static void main(String[] args) { SparkSession spark = SparkSession.builder.getOrCreate(); FileSystem fs = FileSystem.get(spark.sparkContext.hadoopConfiguration); Path oldFile = new Path(\u0026#34;hdfs://some-place/old-file\u0026#34;); Path newFile = new Path(\u0026#34;hdfs://some-place/new-file\u0026#34;); fs.rename(oldFile, newFile); } } Here we used, for example, method of FileSystem. Generally speaking, we can do any file operation (move, read, write, rename, glob, etc.) with such a class. For Scala users there is also a nice scala library with a simple, functional interface that hide FileSystem under the hood and provides clean public interfaces.\nBut how can we use this solution from PySpark?\nPySpark solution Working with py4j and JVM Interestingly, all the PySpark is built on the shoulders of py4j: a library with 1100 stars in GitHub. Just for comparison spark has 35300 stars on GitHub.\nUnder the hood PySpark just wraps Scala calls into py4j. In spark runtime you have access to JVM:\nfrom pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() spark_jvm = spark.sparkContext._jvm Let\u0026#39;s create some Java objects and try to interact with them from Python.\njava_int = spark_jvm.java.lang.Integer(1) java_another_int = spark_jvm.java.lang.Integer(2) print(java_int + java_another_int) 3 Under the hood py4j implicitly make the conversion from simple Python types into simple Java types. In the example above we pass python int into java.lang.Integer as is. We can do the same things with strings, numbers, and sometimes with lists. But often we should explicitly covert types from python to Java and back.\nCreate a FileSystem instance Let\u0026#39;s create a FileSystem instance. From the documentation, we can see that there is a constructor (constructor in Java is like __init__(self, **kwargs) in Python) but it is protected which means it is accessible only from FileSystem class but not from outside. But there are few static methods that allows us to initialize an instance of FileSystem:\nget(Configuration conf) Returns the configured FileSystem implementation. get(URI uri, Configuration conf) Get a FileSystem for this URI\u0026#39;s scheme and authority. get(URI uri, Configuration conf, String user) Get a FileSystem instance based on the uri, the passed in configuration and the user. At first, we need to get Configuration conf instance which contains all the information about FileSystem. The good news is that we can get it from our SparkSession object directly from python:\nhadoop_conf = spark._jsc.hadoopConfiguration() Here we are using another object: jsc which is the same SparkContext but accessible not via pyspark wrapper but as JavaObject.\nTo allow spark to choose the right implementation of FileSystem (for example, NativeS3FileSystem for S3 or RawLocalFileSystem for local files) we should pass into get method also URI. To get a URI from a simple path we can use org.apache.hadoop.fs.Path.toUri method:\ndef _get_hdfs( spark: SparkSession, pattern: str ) -\u0026gt; Tuple[JavaObject, JavaObject]: # Java is accessible in runtime only and it is impossible to infer types here hadoop = spark.sparkContext._jvm.org.apache.hadoop # type: ignore hadoop_conf = spark._jsc.hadoopConfiguration() # type: ignore uri = hadoop.fs.Path(pattern).toUri() # type: ignore hdfs = hadoop.fs.FileSystem.get(uri, hadoop_conf) # type: ignore return (hadoop, hdfs) # type: ignore This function gets a spark session and a pattern (or path) and returns us hadoop and FileSystem instance based on the given SparkSession. So if you, for example, already configure your spark session to work with S3 such a function will use this configuration.\nList files The simplest operation we can do with such an instance of FileSystem is to list files in a distributed or local file system. It is sometimes very useful for example if we check if some path exists or to find some directories based on a pattern. There is a method public FileStatus[] globStatus(Path pathPattern) which takes a pattern and returns Java array of FileStatus objects. Let\u0026#39;s see how it works:\nhadoop = spark.sparkContext._jvm.org.apache.hadoop # syntax sugar for simplifying the code path = hadoop.fs.Path(\u0026#34;file:///home/sem/*\u0026#34;) hdfs = hadoop.fs.FileSystem.get(path.toUri(), spark._jsc.hadoopConfiguration()) statuses = file_system.globStatus(path) print(len(statuses)) 105 What happens if we pass a wrong path? hadoop = spark.sparkContext._jvm.org.apache.hadoop # syntax sugar for simplifying the code path = hadoop.fs.Path(\u0026#34;file://home/sem/*\u0026#34;) hdfs = hadoop.fs.FileSystem.get(path.toUri(), spark._jsc.hadoopConfiguration()) statuses = file_system.globStatus(path) print(len(statuses)) pyspark.sql.utils.IllegalArgumentException: Wrong FS: file://home/sem, expected: file:/// Working with FileStatus To provide a top-level python API we should convert results of globStatus from Java FileStatus[] into python list. To do it lets create a data container for storing information about files:\n@dataclass class HDFSFile: name: str path: str mod_time: int is_dir: bool After that we can loop through statuses and extract information from Java objects to store it inside dataclasses:\nres = [] for file_status in statuses: res.append( HDFSFile( name=file_status.getPath().getName(), path=file_status.getPath().toString(), mod_time=file_status.getModificationTime(), is_dir=file_status.isDirectory(), ) ) Working with strings The next thing we want to have here is the ability to write and read strings. Using just simple strings we can serialize a lot of objects into, for example, json and yaml format. But here we are facing some problems. If we make a look into the documentation of FileSystem we find that the main way to write information is a FSDataOutputStream (link to the documentation). It implements a DataOutputStream abstraction which provides two methods that look interesting from the first view:\npublic final void writeUTF(String str) public final void writeChars(String s) Unfortunately both of them have very bad compatibility with Python UTF-8 strings. The first one uses modified UTF-8 which is useful if you need to have C compatibility but such strings are unreadable from python side (you can read them only as bytes and after that manually decode them). The second one uses UTF-16BE encoding which is some kind of standard in Java but also cannot be simply read as string from Python.\npath = hadoop.fs.Path(\u0026#34;file:///home/sem/test_file.txt\u0026#34;) output_stream = file_system.create(path) output_stream.writeChars(\u0026#34;some testing data with utf-8 symbols: абвгдеж😊\u0026#34;) output_stream.flush() output_stream.close() with open(\u0026#34;/home/sem/test_file.txt\u0026#34;, \u0026#34;r\u0026#34;) as test_file: print(test_file.read()) (result, consumed) = self._buffer_decode(data, self.errors, final) UnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0xd8 in position 90: invalid continuation byte Of course, you are still able to read the data as bytes and decode it manually:\nwith open(\u0026#34;/home/sem/test_file.txt\u0026#34;, \u0026#34;br\u0026#34;) as byte_file: print(byte_file.read().decode(\u0026#34;utf-16be\u0026#34;)) some testing data with utf-8 symbols: абвгдеж😊 But it is not the better option. A better way is to write data as bytes on the Java side but read it as regular a string on python side:\ndef write_utf8( hdfs, hadoop, path: str, data: str, mode: Literal[\u0026#34;a\u0026#34;, \u0026#34;w\u0026#34;] ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Write a given string in UTF-16BE to the given path. Do not use this method to write the data! It is fantastically slow compared to `spark.write`. :param path: Path of file :param data: String to write :param mode: Mode. `w` means overwrite but `a` means append. \u0026#34;\u0026#34;\u0026#34; if mode == \u0026#34;w\u0026#34;: # org.apache.hadoop.fs.FileSystem.create(Path f, boolean overwrite) output_stream = hdfs.create(hadoop.fs.Path(path), True) # type: ignore elif mode == \u0026#34;a\u0026#34;: # org.apache.hadoop.fs.FileSystem.append(Path f) output_stream = hdfs.append(hadoop.fs.Path(path)) # type: ignore # org.apache.hadoop.fs.FSDataOutputStream try: for b in data.encode(\u0026#34;utf-8\u0026#34;): output_stream.write(b) output_stream.flush() output_stream.close() except Exception as e: output_stream.close() raise e Combining all together Finally, we are ready to combine it all together and create a class for working with File Systems when all these py4 things are hidden under the hood.\nimport enum import re from dataclasses import dataclass from typing import List, Literal, Tuple from py4j.java_gateway import JavaObject from pyspark.sql import SparkSession _FS_PATTERN = r\u0026#34;(s3\\w*://|hdfs://|dbfs://|file://|file:/).(.*)\u0026#34; class FS_TYPES(enum.Enum): DBFS = \u0026#34;DBFS\u0026#34; HDFS = \u0026#34;HDFS\u0026#34; S3 = \u0026#34;S3\u0026#34; LOCAL = \u0026#34;LOCAL\u0026#34; UNKNOWN = \u0026#34;UNKNOWN\u0026#34; @classmethod def _from_pattern(cls, pattern: str): return { \u0026#34;s3://\u0026#34;: cls.S3, \u0026#34;s3a://\u0026#34;: cls.S3, \u0026#34;dbfs://\u0026#34;: cls.DBFS, \u0026#34;hdfs://\u0026#34;: cls.HDFS, \u0026#34;file://\u0026#34;: cls.LOCAL, }.get(pattern, cls.UNKNOWN) @dataclass class HDFSFile: name: str path: str mod_time: int is_dir: bool fs_type: FS_TYPES def _get_hdfs( spark: SparkSession, pattern: str ) -\u0026gt; Tuple[JavaObject, JavaObject, FS_TYPES]: match = re.match(_FS_PATTERN, pattern) if match is None: raise ValueError( f\u0026#34;Bad pattern or path. Got {pattern} but should be\u0026#34; \u0026#34; one of `s3://`, `s3a://`, `dbfs://`, `hdfs://`, `file://`\u0026#34; ) fs_type = FS_TYPES._from_pattern(match.groups()[0]) # Java is accessible in runtime only and it is impossible to infer types here hadoop = spark.sparkContext._jvm.org.apache.hadoop # type: ignore hadoop_conf = spark._jsc.hadoopConfiguration() # type: ignore uri = hadoop.fs.Path(pattern).toUri() # type: ignore hdfs = hadoop.fs.FileSystem.get(uri, hadoop_conf) # type: ignore return (hadoop, hdfs, fs_type) # type: ignore class HadoopFileSystem(object): def __init__(self: \u0026#34;HadoopFileSystem\u0026#34;, spark: SparkSession, pattern: str) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Helper class for working with FileSystem. :param spark: SparkSession object :param pattern: Any pattern related to FileSystem. We should provide it to choose the right implementation of org.apache.hadoop.fs.FileSystem under the hood. Pattern here should have a form of URI-like string like `s3a:///my-bucket/my-prefix` or `file:///home/user/`. \u0026#34;\u0026#34;\u0026#34; hadoop, hdfs, fs_type = _get_hdfs(spark, pattern) self._hdfs = hdfs self._fs_type = fs_type self._hadoop = hadoop self._jvm = spark.sparkContext._jvm def write_utf8( self: \u0026#34;HadoopFileSystem\u0026#34;, path: str, data: str, mode: Literal[\u0026#34;a\u0026#34;, \u0026#34;w\u0026#34;] ) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Write a given string in UTF-16BE to the given path. Do not use this method to write the data! It is fantastically slow compared to `spark.write`. :param path: Path of file :param data: String to write :param mode: Mode. `w` means overwrite but `a` means append. \u0026#34;\u0026#34;\u0026#34; if mode == \u0026#34;w\u0026#34;: # org.apache.hadoop.fs.FileSystem.create(Path f, boolean overwrite) output_stream = self._hdfs.create(self._hadoop.fs.Path(path), True) # type: ignore elif mode == \u0026#34;a\u0026#34;: # org.apache.hadoop.fs.FileSystem.append(Path f) output_stream = self._hdfs.append(self._hadoop.fs.Path(path)) # type: ignore # org.apache.hadoop.fs.FSDataOutputStream try: for b in data.encode(\u0026#34;utf-8\u0026#34;): output_stream.write(b) output_stream.flush() output_stream.close() except Exception as e: output_stream.close() raise e def read_utf8(self: \u0026#34;HadoopFileSystem\u0026#34;, path: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Read string from given path. Do not use this method to read the data! It is fantastically slow compared to `spark.read`. :param path: Path of file :return: Decoded from UTF-8 string :rtype: str \u0026#34;\u0026#34;\u0026#34; res = [] # org.apache.hadoop.fs.FileSystem.open in_stream = self._hdfs.open(self._hadoop.fs.Path(path)) # type: ignore # open returns us org.apache.hadoop.fs.FSDataInputStream try: while True: if in_stream.available() \u0026gt; 0: res.append(in_stream.readByte()) else: in_stream.close() break except Exception as e: in_stream.close() raise e return bytes(res).decode(\u0026#34;utf-8\u0026#34;) def glob(self, pattern: str) -\u0026gt; List[HDFSFile]: statuses = self._hdfs.globStatus(self._hadoop.fs.Path(pattern)) res = [] for file_status in statuses: # org.apache.hadoop.fs.FileStatus res.append( HDFSFile( name=file_status.getPath().getName(), path=file_status.getPath().toString(), mod_time=file_status.getModificationTime(), is_dir=file_status.isDirectory(), fs_type=self._fs_type, ) ) return res Conclusion There is a nice lightweight Python library with zero additional dependencies: Eren. This library contains a lot of useful routines for working with Hive and Hadoop. I pushed the code above into this library so you are free to use it. All that you need is just to write:\nfrom eren import fs hdfs = fs.HadoopFileSystem(spark_session, \u0026#34;hdfs://some-place\u0026#34;) s3fs = fs.HadoopFileSystem(spark_session, \u0026#34;s3a://prefix/bucket\u0026#34;) local_fs = fs.HadoopFileSystem(spark_session, \u0026#34;file://my-home-folder\u0026#34;) ","permalink":"http://localhost:1313/ssinchenko/post/working-with-fs-pyspark/","summary":"Working with File System from PySpark Motivation Any of us is working with File System in our work. Almost every pipeline or application has some kind of file-based configuration. Typically json or yaml files are used. Also for data pipelines, it is sometimes important to be able to write results or state them in a human-readable format. Or serialize some artifacts, like matplotlib plot, into bytes and write them to the disk.","title":"Working With File System from PySpark"},{"content":"CV About Name: Sem Sinchenko Title: Senior Data Engineer Company: Raiffeisenbank International AG Location: Belgrade, Serbia Key Skills: Python, Apache Spark, ETL, Scala, MLOps Languages: English Russian Contacts ssinchenko@apache.org\nCareer Senior Data Engineer dates: July 2022 - onwards company: Raiffeisenbank International AG division: Advanced Analytics, Retail Tribe skills: Python, Databricks, PySpark, Apache Spark, MLFlow, SQL Achievements:\nDesigned and implemented ML Production Pipelines in Databricks with MLFlow and Apache Spark Created a lot of ETL-pipelines for various Data Marts with PySpark and Databricks Designed and implemented Feature Store for both Production and Development of ML Models Data Engineer dates: October 2020 - July 2022 company: Raiffeisenbank Russia division: HR Department skills: Python, Java, Scala, Apache Spark, Hadoop, Hive, Apache Airflow, SQL Achievements:*\nOrganized data of the whole department in Data Lake Organized data ingestion into Data Lake from various sources Created a lot of ELT-pipelines and Data Marts for BI reporting Organized migration of legacy Excel-based reports into Data Lake and Power BI ML Engineer dates: January 2018 - October 2020 company: Raiffeisenbank Russia division: Retail Marketing skills: Python, NumPy, SciPy, Tensorflow, Scikit-Learn, Pandas, PySpark, SQL Achievements:\nDesigner and implementer of machine learning models and processes Co-architect of the inner model-deployment Python library Creator of the massive-parallel implementations of few data science algorithms on top of the Apache Spark Open Source Activities Apache GraphAr (incubating) PMC Achievements:\nImplemented Python API for the library for working with Graph Data in DataLakes Education MS, Solid State Physics dates: 2020 - 2022 place: Moscow State University Thesis: Probing a critical states of Heisenberg model with Artificial Deep Neural Network.\nBS, Solid State Physics dates: 2011 - 2016 place: Moscow Engineering Physics Institute Thesis: Numeric modeling of X-Ray Powder Diffraction.\nSelf-education Datacamp Data Scientist with Python Skills Soft Agile Kanban Common Computer Science Algorithms and Data Structures Programming Languages Python Scala Java Data Engineering Apache Airflow Apache Spark Pandas SQL DBT Apache Ni-Fi Deltalake Machine Learning NumPy SciPy Scikit-learn Tensorflow Matplotlib Achievements Codewars Hackathons Hackathon \u0026ldquo;Leaders of Digital 2020\u0026rdquo;, Russian Ministry of Energy use-case. Our team was placed 1st. The task was to create a end2end data process for prediction of an energy consumption. I was responsible for Machine Learning Back-end and data processing pipeline. ","permalink":"http://localhost:1313/ssinchenko/page/cv/","summary":"CV About Name: Sem Sinchenko Title: Senior Data Engineer Company: Raiffeisenbank International AG Location: Belgrade, Serbia Key Skills: Python, Apache Spark, ETL, Scala, MLOps Languages: English Russian Contacts ssinchenko@apache.org\nCareer Senior Data Engineer dates: July 2022 - onwards company: Raiffeisenbank International AG division: Advanced Analytics, Retail Tribe skills: Python, Databricks, PySpark, Apache Spark, MLFlow, SQL Achievements:\nDesigned and implemented ML Production Pipelines in Databricks with MLFlow and Apache Spark Created a lot of ETL-pipelines for various Data Marts with PySpark and Databricks Designed and implemented Feature Store for both Production and Development of ML Models Data Engineer dates: October 2020 - July 2022 company: Raiffeisenbank Russia division: HR Department skills: Python, Java, Scala, Apache Spark, Hadoop, Hive, Apache Airflow, SQL Achievements:*","title":""},{"content":"My name is Semyon Sinchenko but you can call me just Sem.\nI\u0026rsquo;m working as a Data Engineer in retail banking. I\u0026rsquo;m a big fan of open source software, quantum physics and old-school science fiction.\n","permalink":"http://localhost:1313/ssinchenko/page/about/","summary":"My name is Semyon Sinchenko but you can call me just Sem.\nI\u0026rsquo;m working as a Data Engineer in retail banking. I\u0026rsquo;m a big fan of open source software, quantum physics and old-school science fiction.","title":"About me"}]