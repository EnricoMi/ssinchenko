<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility | Sem Sinchenko</title>
<meta name="keywords" content="spark, pyspark">
<meta name="description" content="In this post, I want to discuss the groundbreaking changes in the latest LTS release of the Databricks runtime. This release introduced Spark Connect as the default way to work with shared clusters. I will give a brief introduction to the topic of internal JVM calls and Spark Connect, provide examples of 3d-party OSS projects broken in 14.3, and try to understand the reasons for such a move by Databricks.">
<meta name="author" content="Sem Sinchenko">
<link rel="canonical" href="https://semyonsinchenko.gihub.io/ssinchenko/post/how-databricks-14x-breaks-3dparty-compatibility/">
<link crossorigin="anonymous" href="/ssinchenko/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://semyonsinchenko.gihub.io/ssinchenko/images/fav/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://semyonsinchenko.gihub.io/ssinchenko/images/fav/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://semyonsinchenko.gihub.io/ssinchenko/images/fav/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://semyonsinchenko.gihub.io/ssinchenko/images/fav/apple-touch-icon.png">
<link rel="mask-icon" href="https://semyonsinchenko.gihub.io/ssinchenko/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://semyonsinchenko.gihub.io/ssinchenko/post/how-databricks-14x-breaks-3dparty-compatibility/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility" />
<meta property="og:description" content="In this post, I want to discuss the groundbreaking changes in the latest LTS release of the Databricks runtime. This release introduced Spark Connect as the default way to work with shared clusters. I will give a brief introduction to the topic of internal JVM calls and Spark Connect, provide examples of 3d-party OSS projects broken in 14.3, and try to understand the reasons for such a move by Databricks." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://semyonsinchenko.gihub.io/ssinchenko/post/how-databricks-14x-breaks-3dparty-compatibility/" />
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-02-22T14:00:05+02:00" />
<meta property="article:modified_time" content="2024-02-22T14:00:05+02:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png" />
<meta name="twitter:title" content="How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility"/>
<meta name="twitter:description" content="In this post, I want to discuss the groundbreaking changes in the latest LTS release of the Databricks runtime. This release introduced Spark Connect as the default way to work with shared clusters. I will give a brief introduction to the topic of internal JVM calls and Spark Connect, provide examples of 3d-party OSS projects broken in 14.3, and try to understand the reasons for such a move by Databricks."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://semyonsinchenko.gihub.io/ssinchenko/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility",
      "item": "https://semyonsinchenko.gihub.io/ssinchenko/post/how-databricks-14x-breaks-3dparty-compatibility/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility",
  "name": "How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility",
  "description": "In this post, I want to discuss the groundbreaking changes in the latest LTS release of the Databricks runtime. This release introduced Spark Connect as the default way to work with shared clusters. I will give a brief introduction to the topic of internal JVM calls and Spark Connect, provide examples of 3d-party OSS projects broken in 14.3, and try to understand the reasons for such a move by Databricks.",
  "keywords": [
    "spark", "pyspark"
  ],
  "articleBody": " Preface I work for a company that is a customer of the Databricks platform. However, this blog post represents only my personal view and opinion and is not related to any official position of my employer or my employer's dissatisfaction with the service provided by Databricks.\nIntroduction: Spark-Connect When I saw the announcement of Spark Connect for the first time, I had very controversial feelings. From one side, developers of PySpark applications are finally getting the ability to use the latest minor version of Python and not depend on the long list of PySpark dependencies and JVM. But from the other side, I was thinking about how 3d-party PySpark libraries should work with it?\nA common pattern of 3d-party PySpark packages We all know that Python is not the fastest programming language, but it works very well as a glue for compiled languages like the JVM family. And this is exactly how PySpark worked before Spark Connect: we have a JVM when all the Spark routines are running and we have Py4j acting as a bridge. If you take a look at the SparkSession object, you can see the following private attributes:\nself._sc = sparkContext self._jsc = self._sc._jsc self._jvm = self._sc._jvm So, we are having an access to the underlying JVM and underlying JavaSparkContext object. Another important thing is an __init__ method of pyspark.sql.DataFrame object:\ndef __init__( self, jdf: JavaObject, sql_ctx: Union[\"SQLContext\", \"SparkSession\"], ): It allows developers to take a py4j.java_gateway.JavaObject that represents an org.apache.spark.sql.DataFrame JVM object and create from it a PySpark DataFrame. Back converting is even easier: pyspark.sql.DataFrame has an attribute _jdf that is a reference to the underlying org.apache.spark.sql.DataFrame object:\nself._jdf: JavaObject = jdf One may already understand the obvious pattern of creating a PySpark package:\nWe are writing a core part of the package in Java/Scala with access to all Apache Spark APIs in fast compilable language We are creating a python library that just provide bindings to underlying JAR-package via py4j. For example, recently I was happy to create such a PySpark wrapper for a new huge Graph storage format library: my PR with PySpark bindings to GraphAr scala package.\nWhat was changed with SparkConnect? The answer is quite obvious: because Spark Connect decouples the driver from the user's code, the user no longer has access to the underlying JVM. Of course, the topic of importance of py4j for 3d-party libs is very specific and many users of PySpark did not even notice the importance of this change. But for me it was the first question.\nI will be honest if I say that I tried to find an alternative way for 3d-parties to interact with the underlying JVM with Spark Connect. But there is almost nothing about it in all the promotional material from Databricks and in the Apache Spark documentation. The only theoretically possible workaround I could find is this tiny Markdown page, hidden very deep in the source code of Apache Spark. Based on this, I can imagine that the Spark Connect protocol was designed to be extensible, but again: there is no documentation on how to do it!\nWhat was changed in Databricks 14.x release and why it is an absolutely breaking change Databricks 14.x introduced the following changes:\nThere is no longer a dependency on the JVM when querying Apache Spark and as a consequence, internal APIs related to the JVM, such as _jsc, _jconf, _jvm, _jsparkSession, _jreader, _jc, _jseq, _jdf, _jmap, and _jcols are no longer supported. Of course, almost no one (including myself) checks the Release Notes of non-LTS releases of Databricks. And finally, at the beginning of February it happens: Databricks Runtime 14.3 LTS was released. Everyone, including me go and check the changes and see the same thing like in 14.0: _jvm, _jsc, _jsparkSession and _jdf are no longer available in Databricks Notebooks.\nSuch a change absolutely destroyed described above pattern of creating PySpark 3d-party packages…\nWhy is it important? Of course, one may say: Ok, they break something, but no-one except you care about it because everything you need is inside Databricks and Apache Spark itself. Ok, lets see which libraries will be broken.\nMicrosoft Synapse ML (ex MMLSpark) Synapse ML is a well know (4.9k stars) spark extension, focused on applying ML/DL on Apache Spark clusters. One may know it as MMLSpark. There core part of the library is written in Scala, but APIs for R, Python, #NET and Java are provided. If one make a look how a Python API is organized under the hood they would see the described above \"py4j-pattern\":\nclass DiscreteHyperParam(object): \"\"\" Specifies a discrete list of values. \"\"\" def __init__(self, values, seed=0): ctx = SparkContext.getOrCreate() self.jvm = ctx.getOrCreate()._jvm self.hyperParam = self.jvm.com.microsoft.azure.synapse.ml.automl.HyperParamUtils.getDiscreteHyperParam( values, seed, ) link to the code above\nDue to the popularity of that library they already faced issues from Databricks users: [BUG] Databricks 14.3 LTS usage of internal _jvm variable is no longer supported #2167. And I have zero ideas how they are going to fix it because to make it work with Spark Connect they need to rewrite all the logic in pure Python/PySpark.\nAmazon Deequ/PyDeequ PyDeequ is a popular (625 stars) Data Quality library that is native to Apache Spark because its core is written in Scala. Again, if one make a look on how is it implemented under the hood they will see \"py4j-pattern\" again:\nclass _AnalyzerObject: \"\"\" Analyzer base object to pass and accumulate the analyzers of the run with respect to the JVM \"\"\" def _set_jvm(self, jvm): self._jvm = jvm return self @property def _deequAnalyzers(self): if self._jvm: return self._jvm.com.amazon.deequ.analyzers raise AttributeError( \"JVM not set, please run _set_jvm() method first.\" ) # TODO: Test that this exception gets raised link to the code above\nSpark-NLP Spark-NLP is one of the most popular (3.6k stars) way to run LLMs on Apache Spark clusters. Let's again go the source code and see how it works. Oops, looks like we found using of _jdf / _jvm again:\nclass RecursiveEstimator(JavaEstimator, ABC): def _fit_java(self, dataset, pipeline=None): self._transfer_params_to_java() if pipeline: return self._java_obj.recursiveFit(dataset._jdf, pipeline._to_java()) else: return self._java_obj.fit(dataset._jdf) link to the code above\nSpark-extensions spark-extensions is relative popular (155 stars) and actively maintained library, that contains a lot of small helpers and extensions of Apache Spark/PySpark. Under the hood its PySpark part is based on the \"py4j-pattern\" (yes, again):\nfunc = sc._jvm.uk.co.gresearch.spark.__getattr__(\"package$\").__getattr__(\"MODULE$\").dotNetTicksToTimestamp link to the code above\nH2O Sparkling Water Sparkling Water is an official way to run H2O models on Apache Spark cluster. Repository has 955 stars and is actively maintained. Under the hood one may again find \"py4j-pattern\" that is based on _jvm / _jdf:\nclass H2OTargetEncoderModel(H2OTargetEncoderMOJOParams, JavaModel, JavaMLWritable): def transform(self, dataset): callerFrame = inspect.stack()[1] inTrainingMode = (callerFrame[3] == '_fit') \u0026 callerFrame[1].endswith('pyspark/ml/pipeline.py') if inTrainingMode: return self.transformTrainingDataset(dataset) else: return super(H2OTargetEncoderModel, self).transform(dataset) def transformTrainingDataset(self, dataset): self._transfer_params_to_java() return DataFrame(self._java_obj.transformTrainingDataset(dataset._jdf), dataset.sql_ctx) link to the code above\nPayPal gimel gimel is a quite popular (239 stars) framework that is built on top of Apache Spark. In the documentation they directly recommend to use \"py4j-pattern\":\n# import DataFrame and SparkSession from pyspark.sql import DataFrame, SparkSession, SQLContext # fetch reference to the class in JVM ScalaDataSet = sc._jvm.com.paypal.gimel.DataSet # fetch reference to java SparkSession jspark = spark._jsparkSession # initiate dataset dataset = ScalaDataSet.apply(jspark) # Read Data | kafka semantics abstracted for user df = dataset.read(\"kafka_dataset\") # Apply transformations (business logic | abstracted for Gimel) transformed_df = df(...transformations...) # Write Data | Elastic semantics abstracted for user dataset.write(\"elastic_dataset\",df) link to the code above\nHNSWlib-spark HNSWlib is a quite popular (240 stars) and modern JVM library for an Approximate Nearest Neighbors Search. hnswlib-spark is an Apache Spark/PySpark wrapper on top of the main library. And under the hood PySpark part is partially based on a \"py4j-pattern\" by using SparkContext constructor:\ndef __init__(self): spark_conf = SparkConf() spark_conf.setAppName(spark_nlp_config.app_name) spark_conf.setMaster(spark_nlp_config.master) spark_conf.set(\"spark.driver.memory\", memory) spark_conf.set(\"spark.serializer\", spark_nlp_config.serializer) spark_conf.set(\"spark.kryo.registrator\", spark_nlp_config.registrator) spark_conf.set(\"spark.jars.packages\", spark_nlp_config.maven_spark) spark_conf.set(\"spark.hnswlib.settings.index.cache_folder\", cache_folder) # Make the py4j JVM stdout and stderr available without buffering popen_kwargs = { 'stdout': subprocess.PIPE, 'stderr': subprocess.PIPE, 'bufsize': 0 } # Launch the gateway with our custom settings self.gateway = launch_gateway(conf=spark_conf, popen_kwargs=popen_kwargs) self.process = self.gateway.proc # Use the gateway we launched spark_context = SparkContext(gateway=self.gateway) self.spark_session = SparkSession(spark_context) self.out_thread = threading.Thread(target=self.output_reader) self.error_thread = threading.Thread(target=self.error_reader) self.std_background_listeners() link to the code above\nThe Archives Unleashed Toolkit AUT is a tool and a library to analyze Web Archives on Apache Spark clusters. Its PySpark part uses the same \"py4j-pattern\":\nclass WebArchive: def __init__(self, sc, sqlContext, path): self.sc = sc self.sqlContext = sqlContext self.loader = sc._jvm.io.archivesunleashed.df.DataFrameLoader(sc._jsc.sc()) self.path = path link to the code above\nApache Linkis Linkis is a top-level Apache project (3.2k stars). It's PySpark part is heavily based on the same \"py4j-pattern\":\njsc = intp.getJavaSparkContext() jconf = intp.getSparkConf() conf = SparkConf(_jvm = gateway.jvm, _jconf = jconf) sc = SparkContext(jsc=jsc, gateway=gateway, conf=conf) sqlc = HiveContext(sc, intp.sqlContext()) sqlContext = sqlc spark = SparkSession(sc, intp.getSparkSession()) link to the code above\nSpark-dgraph-connector spark-dgraph-connector is an another project from G-Research. It's PySpark part uses the same \"py4j-pattern\":\nclass DgraphReader: def __init__(self, reader: DataFrameReader): super().__init__() self._jvm = reader._spark._jvm self._spark = reader._spark self._reader = self._jvm.uk.co.gresearch.spark.dgraph.connector.DgraphReader(reader._jreader) link to the code above\nGraphAr And finally a project where I'm a contributor and maintainer of PySpark part: GraphAr. GraphAr is a novel way to store huge Graph data in DataLake or LakeHouse solutions. The whole PySpark part is based on \"py4j-pattern\". Mostly because Synapse ML and PyDeequ were main sources of inspiration for me when I worked on the implementation…\nAn endless amount of in-house solutions and libraries I'm more than sure that many companies using Databricks have their own in-house helpers, libraries, etc. And I'm more than sure that a lot of these in-house projects rely on the same \"py4j-pattern\".\nDiscussion The main question for me here is why is Databricks pushing Spark Connect so hard? I have always seen Databricks as a company founded by computer science rock stars and open source enthusiasts. I hope that the new policy of breaking 3d party libs in Databricks runtime and notebooks is just an incident and there will be an explanation soon. And I really hope that with such an action Databricks is not trying to force people to use only the built-in proprietary tools of the platform (like the recently announced data quality solution instead of PyDeequ). We all love Databricks because it is based on open source tools and because the company is so open to collaboration and integration.\nI love the whole idea of Spark Connect. Many benefits of using it are obvious:\nRelax dependencies and requirements on user code; The ability to expose the Spark API to more programming languages (Golang, Rust, etc.); An ability to simplify integration with IDEs (JetBrains, VSCode, Vim, Emacs, etc.); A lot of other benefits… The only problem is the speed with which Spark Connect is pushed by Databricks. In my opinion, in this case, Databricks should not just say something like \"Guys, you used private stuff, there was no guarantee that it would work, so it is your and only your problem\" to all 3d party project developers.\n",
  "wordCount" : "1825",
  "inLanguage": "en",
  "image":"https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png","datePublished": "2024-02-22T14:00:05+02:00",
  "dateModified": "2024-02-22T14:00:05+02:00",
  "author":[{
    "@type": "Person",
    "name": "Sem Sinchenko"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://semyonsinchenko.gihub.io/ssinchenko/post/how-databricks-14x-breaks-3dparty-compatibility/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sem Sinchenko",
    "logo": {
      "@type": "ImageObject",
      "url": "https://semyonsinchenko.gihub.io/ssinchenko/images/fav/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://semyonsinchenko.gihub.io/ssinchenko/" accesskey="h" title="Sem Sinchenko (Alt + H)">Sem Sinchenko</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/page/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/page/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://semyonsinchenko.gihub.io/ssinchenko/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility
    </h1>
    <div class="post-meta"><span title='2024-02-22 14:00:05 +0200 +0200'>February 22, 2024</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Sem Sinchenko

</div>
  </header> 
<figure class="entry-cover"><img loading="eager" src="https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png" alt="">
        
</figure>
  <div class="post-content">
<div id="outline-container-headline-1" class="outline-3">
<h3 id="headline-1">
Preface
</h3>
<div id="outline-text-headline-1" class="outline-text-3">
<p>
I work for a company that is a customer of the Databricks platform. However, this blog post represents only my personal view and opinion and is not related to any official position of my employer or my employer&#39;s dissatisfaction with the service provided by Databricks.</p>
</div>
</div>
<div id="outline-container-headline-2" class="outline-3">
<h3 id="headline-2">
Introduction: Spark-Connect
</h3>
<div id="outline-text-headline-2" class="outline-text-3">
<p>
When I saw the announcement of <a href="https://www.databricks.com/blog/2022/07/07/introducing-spark-connect-the-power-of-apache-spark-everywhere.html">Spark Connect</a> for the first time, I had very controversial feelings. From one side, developers of PySpark applications are finally getting the ability to use the latest minor version of Python and not depend on the long list of PySpark dependencies and JVM. But from the other side, I was thinking about how 3d-party PySpark libraries should work with it?</p>
<div id="outline-container-headline-3" class="outline-4">
<h4 id="headline-3">
A common pattern of 3d-party PySpark packages
</h4>
<div id="outline-text-headline-3" class="outline-text-4">
<p>
We all know that Python is not the fastest programming language, but it works very well as a glue for compiled languages like the JVM family. And this is exactly how PySpark worked before Spark Connect: we have a JVM when all the Spark routines are running and we have <a href="https://github.com/py4j/py4j">Py4j</a> acting as a bridge. If you take a look at the <code class="verbatim">SparkSession</code> object, you <a href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/session.html#SparkSession">can see</a> the following private attributes:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_sc</span> <span class="o">=</span> <span class="n">sparkContext</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_jsc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_jsc</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_jvm</span></span></span></code></pre></div>
</div>
<p>
So, we are having an access to the underlying JVM and underlying <code class="verbatim">JavaSparkContext</code> object. Another important thing is an <code class="verbatim">__init__</code> method of <code class="verbatim">pyspark.sql.DataFrame</code> object:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">jdf</span><span class="p">:</span> <span class="n">JavaObject</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">sql_ctx</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&#34;SQLContext&#34;</span><span class="p">,</span> <span class="s2">&#34;SparkSession&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span></span></span></code></pre></div>
</div>
<p>
It allows developers to take a <code class="verbatim">py4j.java_gateway.JavaObject</code> that represents an <code class="verbatim">org.apache.spark.sql.DataFrame</code> JVM object and create from it a PySpark <code class="verbatim">DataFrame</code>. Back converting is even easier: <code class="verbatim">pyspark.sql.DataFrame</code> has an attribute <code class="verbatim">_jdf</code> that is a reference to the underlying <code class="verbatim">org.apache.spark.sql.DataFrame</code> object:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_jdf</span><span class="p">:</span> <span class="n">JavaObject</span> <span class="o">=</span> <span class="n">jdf</span></span></span></code></pre></div>
</div>
<p>
One may already understand the obvious pattern of creating a PySpark package:</p>
<ol>
<li>We are writing a core part of the package in Java/Scala with access to all Apache Spark APIs in fast compilable language</li>
<li>We are creating a python library that just provide bindings to underlying JAR-package via <code class="verbatim">py4j</code>.</li>
</ol>
<p>
For example, recently I was happy to create such a PySpark wrapper for a new huge Graph storage format library: <a href="https://github.com/alibaba/GraphAr/pull/300/files">my PR with PySpark bindings to GraphAr scala package</a>.</p>
</div>
</div>
<div id="outline-container-headline-4" class="outline-4">
<h4 id="headline-4">
What was changed with SparkConnect?
</h4>
<div id="outline-text-headline-4" class="outline-text-4">
<p>
The answer is quite obvious: because Spark Connect decouples the driver from the user&#39;s code, the user no longer has access to the underlying JVM. Of course, the topic of importance of <code class="verbatim">py4j</code> for 3d-party libs is very specific and many users of PySpark did not even notice the importance of this change. But for me it was the first question.</p>
<p>
I will be honest if I say that I tried to find an alternative way for 3d-parties to interact with the underlying JVM with Spark Connect. But there is almost nothing about it in all the promotional material from Databricks and in the Apache Spark documentation. The only theoretically possible workaround I could find is this <a href="https://github.com/apache/spark/blob/master/connector/connect/docs/adding-proto-messages.md">tiny Markdown page</a>, hidden very deep in the source code of Apache Spark. Based on this, I can imagine that the Spark Connect protocol was designed to be extensible, but again: there is no documentation on how to do it!</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-5" class="outline-3">
<h3 id="headline-5">
What was changed in Databricks 14.x release and why it is an absolutely breaking change
</h3>
<div id="outline-text-headline-5" class="outline-text-3">
<p>
Databricks 14.x introduced <a href="https://docs.databricks.com/en/release-notes/runtime/14.0.html#introducing-spark-connect-in-shared-cluster-architecture">the following changes</a>:</p>
<ul>
<li><em>There is no longer a dependency on the JVM when querying Apache Spark and as a consequence, internal APIs related to the JVM, such as _jsc, _jconf, _jvm, _jsparkSession, _jreader, _jc, _jseq, _jdf, _jmap, and _jcols are no longer supported.</em></li>
</ul>
<p>
Of course, almost no one (including myself) checks the Release Notes of non-LTS releases of Databricks. And finally, at the beginning of February it happens: <a href="https://docs.databricks.com/en/release-notes/runtime/14.3lts.html">Databricks Runtime 14.3 LTS</a> was released. Everyone, including me go and check the changes and see the same thing like in 14.0: <code class="verbatim">_jvm</code>, <code class="verbatim">_jsc</code>, <code class="verbatim">_jsparkSession</code> and <code class="verbatim">_jdf</code> are no longer available in Databricks Notebooks.</p>
<p>
Such a change absolutely destroyed described above pattern of creating PySpark 3d-party packages…</p>
</div>
</div>
<div id="outline-container-headline-6" class="outline-3">
<h3 id="headline-6">
Why is it important?
</h3>
<div id="outline-text-headline-6" class="outline-text-3">
<p>
Of course, one may say: Ok, they break something, but no-one except you care about it because everything you need is inside Databricks and Apache Spark itself. Ok, lets see which libraries will be broken.</p>
<div id="outline-container-headline-7" class="outline-4">
<h4 id="headline-7">
Microsoft Synapse ML (ex MMLSpark)
</h4>
<div id="outline-text-headline-7" class="outline-text-4">
<p>
<a href="https://github.com/microsoft/SynapseML">Synapse ML</a> is a well know (4.9k stars) spark extension, focused on applying ML/DL on Apache Spark clusters. One may know it as <code class="verbatim">MMLSpark</code>. There core part of the library is written in <code class="verbatim">Scala</code>, but APIs for <code class="verbatim">R</code>, <code class="verbatim">Python</code>, <code class="verbatim">#NET</code> and <code class="verbatim">Java</code> are provided. If one make a look how a <code class="verbatim">Python</code> API is organized under the hood they would see the described above &#34;py4j-pattern&#34;:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DiscreteHyperParam</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Specifies a discrete list of values.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">ctx</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span><span class="o">.</span><span class="n">_jvm</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">hyperParam</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">microsoft</span><span class="o">.</span><span class="n">azure</span><span class="o">.</span><span class="n">synapse</span><span class="o">.</span><span class="n">ml</span><span class="o">.</span><span class="n">automl</span><span class="o">.</span><span class="n">HyperParamUtils</span><span class="o">.</span><span class="n">getDiscreteHyperParam</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">values</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">seed</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/microsoft/SynapseML/blob/master/core/src/main/python/synapse/ml/automl/HyperparamBuilder.py#L53">link to the code above</a></p>
<p>
Due to the popularity of that library they already faced issues from Databricks users: <a href="https://github.com/microsoft/SynapseML/issues/2167"> [BUG] Databricks 14.3 LTS usage of internal _jvm variable is no longer supported #2167</a>. And I have zero ideas how they are going to fix it because to make it work with Spark Connect they need to <strong>rewrite all the logic</strong> in pure Python/PySpark.</p>
</div>
</div>
<div id="outline-container-headline-8" class="outline-4">
<h4 id="headline-8">
Amazon Deequ/PyDeequ
</h4>
<div id="outline-text-headline-8" class="outline-text-4">
<p>
<a href="https://github.com/awslabs/python-deequ">PyDeequ</a> is a popular (625 stars) Data Quality library that is native to Apache Spark because its core is <a href="https://github.com/awslabs/deequ">written in Scala</a>. Again, if one make a look on how is it implemented under the hood they will see &#34;py4j-pattern&#34; again:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">_AnalyzerObject</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Analyzer base object to pass and accumulate the analyzers of the run with respect to the JVM
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_set_jvm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">jvm</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span> <span class="o">=</span> <span class="n">jvm</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_deequAnalyzers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">amazon</span><span class="o">.</span><span class="n">deequ</span><span class="o">.</span><span class="n">analyzers</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;JVM not set, please run _set_jvm() method first.&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>  <span class="c1"># TODO: Test that this exception gets raised</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/awslabs/python-deequ/blob/master/pydeequ/analyzers.py#L27">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-9" class="outline-4">
<h4 id="headline-9">
Spark-NLP
</h4>
<div id="outline-text-headline-9" class="outline-text-4">
<p>
<a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master">Spark-NLP</a> is one of the most popular (3.6k stars) way to run LLMs on Apache Spark clusters. Let&#39;s again go the source code and see how it works. Oops, looks like we found using of <code class="verbatim">_jdf</code> / <code class="verbatim">_jvm</code> again:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RecursiveEstimator</span><span class="p">(</span><span class="n">JavaEstimator</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_fit_java</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_transfer_params_to_java</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">pipeline</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span><span class="o">.</span><span class="n">recursiveFit</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">_to_java</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">_jdf</span><span class="p">)</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/python/sparknlp/internal/recursive.py#L27">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-10" class="outline-4">
<h4 id="headline-10">
Spark-extensions
</h4>
<div id="outline-text-headline-10" class="outline-text-4">
<p>
<a href="https://github.com/G-Research/spark-extension">spark-extensions</a> is relative popular (155 stars) and actively maintained library, that contains a lot of small helpers and extensions of Apache Spark/PySpark. Under the hood its PySpark part is based on the &#34;py4j-pattern&#34; (yes, again):</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">uk</span><span class="o">.</span><span class="n">co</span><span class="o">.</span><span class="n">gresearch</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="s2">&#34;package$&#34;</span><span class="p">)</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="s2">&#34;MODULE$&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">dotNetTicksToTimestamp</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/G-Research/spark-extension/blob/master/python/gresearch/spark/__init__.py#L112">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-11" class="outline-4">
<h4 id="headline-11">
H2O Sparkling Water
</h4>
<div id="outline-text-headline-11" class="outline-text-4">
<p>
<a href="https://github.com/h2oai/sparkling-water">Sparkling Water</a> is an official way to run H2O models on Apache Spark cluster. Repository has 955 stars and is actively maintained. Under the hood one may again find &#34;py4j-pattern&#34; that is based on <code class="verbatim">_jvm</code> / <code class="verbatim">_jdf</code>:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">H2OTargetEncoderModel</span><span class="p">(</span><span class="n">H2OTargetEncoderMOJOParams</span><span class="p">,</span> <span class="n">JavaModel</span><span class="p">,</span> <span class="n">JavaMLWritable</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">callerFrame</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">stack</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">inTrainingMode</span> <span class="o">=</span> <span class="p">(</span><span class="n">callerFrame</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;_fit&#39;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">callerFrame</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;pyspark/ml/pipeline.py&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">inTrainingMode</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformTrainingDataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">H2OTargetEncoderModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">transformTrainingDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_transfer_params_to_java</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span><span class="o">.</span><span class="n">transformTrainingDataset</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">_jdf</span><span class="p">),</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sql_ctx</span><span class="p">)</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/h2oai/sparkling-water/blob/master/py-scoring/src/ai/h2o/sparkling/ml/models/H2OTargetEncoderModel.py#L25">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-12" class="outline-4">
<h4 id="headline-12">
PayPal gimel
</h4>
<div id="outline-text-headline-12" class="outline-text-4">
<p>
<a href="https://github.com/paypal/gimel">gimel</a> is a quite popular (239 stars) framework that is built on top of Apache Spark. In the documentation they directly recommend to use &#34;py4j-pattern&#34;:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># import DataFrame and SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">SQLContext</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># fetch reference to the class in JVM</span>
</span></span><span class="line"><span class="cl"><span class="n">ScalaDataSet</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">paypal</span><span class="o">.</span><span class="n">gimel</span><span class="o">.</span><span class="n">DataSet</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># fetch reference to java SparkSession</span>
</span></span><span class="line"><span class="cl"><span class="n">jspark</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_jsparkSession</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># initiate dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">ScalaDataSet</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">jspark</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Read Data | kafka semantics abstracted for user</span>
</span></span><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="s2">&#34;kafka_dataset&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Apply transformations (business logic | abstracted for Gimel)</span>
</span></span><span class="line"><span class="cl"><span class="n">transformed_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">(</span><span class="o">...</span><span class="n">transformations</span><span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Write Data | Elastic semantics abstracted for user</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&#34;elastic_dataset&#34;</span><span class="p">,</span><span class="n">df</span><span class="p">)</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/paypal/gimel/blob/master/docs/index.md?plain=1#L60">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-13" class="outline-4">
<h4 id="headline-13">
HNSWlib-spark
</h4>
<div id="outline-text-headline-13" class="outline-text-4">
<p>
<a href="https://github.com/jelmerk/hnswlib">HNSWlib</a> is a quite popular (240 stars) and modern JVM library for an Approximate Nearest Neighbors Search. <a href="https://github.com/jelmerk/hnswlib-spark">hnswlib-spark</a> is an Apache Spark/PySpark wrapper on top of the main library. And under the hood PySpark part is partially based on a &#34;py4j-pattern&#34; by using <code class="verbatim">SparkContext</code> constructor:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_conf</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">spark_nlp_config</span><span class="o">.</span><span class="n">app_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_conf</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="n">spark_nlp_config</span><span class="o">.</span><span class="n">master</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&#34;spark.driver.memory&#34;</span><span class="p">,</span> <span class="n">memory</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&#34;spark.serializer&#34;</span><span class="p">,</span> <span class="n">spark_nlp_config</span><span class="o">.</span><span class="n">serializer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&#34;spark.kryo.registrator&#34;</span><span class="p">,</span> <span class="n">spark_nlp_config</span><span class="o">.</span><span class="n">registrator</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&#34;spark.jars.packages&#34;</span><span class="p">,</span> <span class="n">spark_nlp_config</span><span class="o">.</span><span class="n">maven_spark</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&#34;spark.hnswlib.settings.index.cache_folder&#34;</span><span class="p">,</span> <span class="n">cache_folder</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Make the py4j JVM stdout and stderr available without buffering</span>
</span></span><span class="line"><span class="cl">    <span class="n">popen_kwargs</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;stdout&#39;</span><span class="p">:</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;stderr&#39;</span><span class="p">:</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;bufsize&#39;</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Launch the gateway with our custom settings</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">gateway</span> <span class="o">=</span> <span class="n">launch_gateway</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">spark_conf</span><span class="p">,</span> <span class="n">popen_kwargs</span><span class="o">=</span><span class="n">popen_kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">process</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gateway</span><span class="o">.</span><span class="n">proc</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Use the gateway we launched</span>
</span></span><span class="line"><span class="cl">    <span class="n">spark_context</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">gateway</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gateway</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">(</span><span class="n">spark_context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">out_thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_reader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">error_thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">error_reader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">std_background_listeners</span><span class="p">()</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/jelmerk/hnswlib-spark/blob/master/hnswlib-spark/src/main/python/pyspark_hnsw/__init__.py#L102">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-14" class="outline-4">
<h4 id="headline-14">
The Archives Unleashed Toolkit
</h4>
<div id="outline-text-headline-14" class="outline-text-4">
<p>
<a href="https://github.com/archivesunleashed/aut">AUT</a> is a tool and a library to analyze Web Archives on Apache Spark clusters. Its PySpark part uses the same &#34;py4j-pattern&#34;:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">WebArchive</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sc</span><span class="p">,</span> <span class="n">sqlContext</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">sc</span> <span class="o">=</span> <span class="n">sc</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">sqlContext</span> <span class="o">=</span> <span class="n">sqlContext</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">loader</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">archivesunleashed</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">DataFrameLoader</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">_jsc</span><span class="o">.</span><span class="n">sc</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/archivesunleashed/aut/blob/main/src/main/python/aut/common.py#L8">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-15" class="outline-4">
<h4 id="headline-15">
Apache Linkis
</h4>
<div id="outline-text-headline-15" class="outline-text-4">
<p>
<a href="https://github.com/apache/linkis">Linkis</a> is a top-level Apache project (3.2k stars). It&#39;s PySpark part is heavily based on the same &#34;py4j-pattern&#34;:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">jsc</span> <span class="o">=</span> <span class="n">intp</span><span class="o">.</span><span class="n">getJavaSparkContext</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">jconf</span> <span class="o">=</span> <span class="n">intp</span><span class="o">.</span><span class="n">getSparkConf</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">(</span><span class="n">_jvm</span> <span class="o">=</span> <span class="n">gateway</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="n">_jconf</span> <span class="o">=</span> <span class="n">jconf</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">jsc</span><span class="o">=</span><span class="n">jsc</span><span class="p">,</span> <span class="n">gateway</span><span class="o">=</span><span class="n">gateway</span><span class="p">,</span> <span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sqlc</span> <span class="o">=</span> <span class="n">HiveContext</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">intp</span><span class="o">.</span><span class="n">sqlContext</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">sqlContext</span> <span class="o">=</span> <span class="n">sqlc</span>
</span></span><span class="line"><span class="cl"><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">intp</span><span class="o">.</span><span class="n">getSparkSession</span><span class="p">())</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/apache/linkis/blob/master/linkis-engineconn-plugins/spark/src/main/resources/python/mix_pyspark.py#L203">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-16" class="outline-4">
<h4 id="headline-16">
Spark-dgraph-connector
</h4>
<div id="outline-text-headline-16" class="outline-text-4">
<p>
<a href="https://github.com/G-Research/spark-dgraph-connector">spark-dgraph-connector</a> is an another project from <span style="text-decoration: underline;">G-Research</span>. It&#39;s PySpark part uses the same &#34;py4j-pattern&#34;:</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DgraphReader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reader</span><span class="p">:</span> <span class="n">DataFrameReader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">_spark</span><span class="o">.</span><span class="n">_jvm</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_spark</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">_spark</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_reader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">uk</span><span class="o">.</span><span class="n">co</span><span class="o">.</span><span class="n">gresearch</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">dgraph</span><span class="o">.</span><span class="n">connector</span><span class="o">.</span><span class="n">DgraphReader</span><span class="p">(</span><span class="n">reader</span><span class="o">.</span><span class="n">_jreader</span><span class="p">)</span></span></span></code></pre></div>
</div>
<p><a href="https://github.com/G-Research/spark-dgraph-connector/blob/main/python/gresearch/spark/dgraph/connector/__init__.py#L62">link to the code above</a></p>
</div>
</div>
<div id="outline-container-headline-17" class="outline-4">
<h4 id="headline-17">
GraphAr
</h4>
<div id="outline-text-headline-17" class="outline-text-4">
<p>
And finally a project where I&#39;m a contributor and maintainer of PySpark part: <code class="verbatim">GraphAr</code>. <a href="https://github.com/alibaba/GraphAr">GraphAr</a> is a novel way to store huge Graph data in DataLake or LakeHouse solutions. The whole PySpark part is based on &#34;py4j-pattern&#34;. Mostly because <code class="verbatim">Synapse ML</code> and <code class="verbatim">PyDeequ</code> were main sources of inspiration for me when I worked on the implementation…</p>
</div>
</div>
<div id="outline-container-headline-18" class="outline-4">
<h4 id="headline-18">
An endless amount of in-house solutions and libraries
</h4>
<div id="outline-text-headline-18" class="outline-text-4">
<p>
I&#39;m more than sure that many companies using Databricks have their own in-house helpers, libraries, etc. And I&#39;m more than sure that a lot of these in-house projects rely on the same &#34;py4j-pattern&#34;.</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-19" class="outline-3">
<h3 id="headline-19">
Discussion
</h3>
<div id="outline-text-headline-19" class="outline-text-3">
<p>
The main question for me here is why is Databricks pushing Spark Connect so hard? I have always seen Databricks as a company founded by computer science rock stars and open source enthusiasts. I hope that the new policy of breaking 3d party libs in Databricks runtime and notebooks is just an incident and there will be an explanation soon. And I really hope that with such an action Databricks is not trying to force people to use only the built-in proprietary tools of the platform (like the recently announced data quality solution instead of PyDeequ). We all love Databricks because it is based on open source tools and because the company is so open to collaboration and integration.</p>
<p>
I love the whole idea of <code class="verbatim">Spark Connect</code>. Many benefits of using it are obvious:</p>
<ol>
<li>Relax dependencies and requirements on user code;</li>
<li>The ability to expose the Spark API to more programming languages (Golang, Rust, etc.);</li>
<li>An ability to simplify integration with IDEs (JetBrains, VSCode, Vim, Emacs, etc.);</li>
<li>A lot of other benefits…</li>
</ol>
<p>
The only problem is the speed with which <code class="verbatim">Spark Connect</code> is pushed by Databricks. In my opinion, in this case, Databricks should not just say something like &#34;Guys, you used private stuff, there was no guarantee that it would work, so it is your and only your problem&#34; to all 3d party project developers.</p>
</div>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://semyonsinchenko.gihub.io/ssinchenko/tags/spark/">Spark</a></li>
      <li><a href="https://semyonsinchenko.gihub.io/ssinchenko/tags/pyspark/">Pyspark</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://semyonsinchenko.gihub.io/ssinchenko/post/multiple-spark-versions-with-maven/">
    <span class="title">« Prev</span>
    <br>
    <span>Supporting multiple Apache Spark versions with Maven</span>
  </a>
  <a class="next" href="https://semyonsinchenko.gihub.io/ssinchenko/post/pyspark-column-lineage/">
    <span class="title">Next »</span>
    <br>
    <span>PySpark column lineage</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility on x"
            href="https://x.com/intent/tweet/?text=How%20Databricks%20Runtime%2014.x%20destroyed%203d-party%20PySpark%20packages%20compatibility&amp;url=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fhow-databricks-14x-breaks-3dparty-compatibility%2f&amp;hashtags=spark%2cpyspark">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fhow-databricks-14x-breaks-3dparty-compatibility%2f&amp;title=How%20Databricks%20Runtime%2014.x%20destroyed%203d-party%20PySpark%20packages%20compatibility&amp;summary=How%20Databricks%20Runtime%2014.x%20destroyed%203d-party%20PySpark%20packages%20compatibility&amp;source=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fhow-databricks-14x-breaks-3dparty-compatibility%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fhow-databricks-14x-breaks-3dparty-compatibility%2f&title=How%20Databricks%20Runtime%2014.x%20destroyed%203d-party%20PySpark%20packages%20compatibility">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fhow-databricks-14x-breaks-3dparty-compatibility%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility on whatsapp"
            href="https://api.whatsapp.com/send?text=How%20Databricks%20Runtime%2014.x%20destroyed%203d-party%20PySpark%20packages%20compatibility%20-%20https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fhow-databricks-14x-breaks-3dparty-compatibility%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility on telegram"
            href="https://telegram.me/share/url?text=How%20Databricks%20Runtime%2014.x%20destroyed%203d-party%20PySpark%20packages%20compatibility&amp;url=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fhow-databricks-14x-breaks-3dparty-compatibility%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Databricks Runtime 14.x destroyed 3d-party PySpark packages compatibility on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=How%20Databricks%20Runtime%2014.x%20destroyed%203d-party%20PySpark%20packages%20compatibility&u=https%3a%2f%2fsemyonsinchenko.gihub.io%2fssinchenko%2fpost%2fhow-databricks-14x-breaks-3dparty-compatibility%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://semyonsinchenko.gihub.io/ssinchenko/">Sem Sinchenko</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
